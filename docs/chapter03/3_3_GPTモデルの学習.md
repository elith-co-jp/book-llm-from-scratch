# 3.3 GPTモデルの学習

第2章では、Transformerアーキテクチャの詳細を学び、エンコーダー・デコーダー構造を持つ翻訳モデルを実装しました。本節では、そのTransformerのデコーダー部分のみを使用したGPTモデルの学習について解説します。GPTは第2章で学んだアテンション機構やTransformerブロックの知識を基盤としながら、言語生成に特化した設計となっています。ここまでの章で学んだTransformerの基礎知識とGPTの概念、そしてトークナイザーの実装を組み合わせて、実際にGPTモデルを学習させる方法を段階的に理解していきましょう。

## nanoGPTについて

本節の実装は、Andrej Karpathy氏が開発したnanoGPTを参考にしています。nanoGPTは、GPTモデルの学習と推論を最小限のコードで実現した教育的なプロジェクトで、MITライセンスの下でGitHubリポジトリ（https://github.com/karpathy/nanoGPT）に公開されています。約300行というコンパクトなコードでGPT-2レベルのモデルを実装しており、シェイクスピアの作品集や大規模ウェブテキストコーパスであるOpenWebTextでの学習が可能です。教育目的に最適化された理解しやすい実装となっているため、GPTの仕組みを学ぶには最適な教材といえるでしょう。

本書の実装では、nanoGPTの設計思想を参考にしつつ、第2章で実装したTransformerの各モジュール（`llm_from_scratch/transformers/`内のマルチヘッドアテンション、位置エンコーディング、フィードフォワード層など）を再利用しています。これにより、第2章で学んだTransformerの翻訳モデルとGPTの関係性がより明確になり、両者が同じ基盤技術の上に構築されていることを実感できるでしょう。また、日本語テキストでの学習例として夏目漱石の作品を使用した実装も提供しています。

nanoGPTは、OpenAIのGPT-2モデルを再現することを目標としており、実際に124Mパラメータのモデルで同等の性能を達成しています。本節では、このnanoGPTの設計思想を参考にしながら、第2章のTransformer実装を基盤として、さらに初心者にも理解しやすいように段階的に実装を進めていきます。

## 3.3.1 学習の全体像

GPTシリーズ（GPT-1からGPT-4まで）に共通する学習プロセスは、大規模なテキストデータから言語の規則性やパターンを自動的に学習する過程です。本節では主にGPT-2規模のモデルを実装しますが、基本的な学習の仕組みはGPTシリーズ全体で共通しています。このプロセスは大きく6つのステップで構成されています。

まず最初に、生のテキストデータをモデルが処理できる形式に変換する必要があります。これがデータの準備段階で、テキストをトークン化し、学習用のバッチを作成します。次に、GPTアーキテクチャに基づいてモデルを定義します。このモデルは、入力されたトークン列から次のトークンを予測するように設計されています。

学習の中核となるのは損失関数の設定です。GPTシリーズでは次単語予測タスクのためにクロスエントロピー損失を使用します。この損失を最小化するために、AdamWオプティマイザーと学習率スケジューリングを組み合わせた最適化手法を採用します。実際の学習は、フォワードパス、バックプロパゲーション、パラメータ更新という一連の処理を繰り返すことで進行します。最後に、学習したモデルの性能を評価し、実際にテキストを生成させることで、モデルが言語を適切に学習できているかを確認します。

## 3.3.2 データの準備とデータローダー

GPTシリーズのモデルの学習において、データの準備は極めて重要な工程です。GPTシリーズは自己回帰型モデルとして設計されており、テキストシーケンスから次の単語を予測することで言語を学習します。この学習方式を実現するために、適切なデータ処理が必要となります。

データセットクラスの実装では、入力テキストをトークン化し、指定された長さのチャンクに分割します。各チャンクから入力シーケンスとターゲットシーケンスのペアを作成します。例えば、"The cat sat on the mat"というテキストがあった場合、"The cat sat on the"を入力として"cat sat on the mat"を出力として学習させます。これにより、モデルは文脈から次の単語を予測する能力を獲得していきます。

実装の詳細は`llm_from_scratch/gpt/dataset.py`および`llm_from_scratch/gpt/tokenizer.py`に記載されています。基本的な構造として、TextDatasetクラスはテキストとトークナイザー、ブロックサイズ（コンテキスト長）を受け取り、学習用のデータを準備します。

```python
class TextDataset(Dataset):
    """テキストデータセットクラス"""
    
    def __init__(self, text, tokenizer, block_size=128):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.tokens = tokenizer.encode(text)
    
    def __getitem__(self, idx):
        # 入力とターゲットのペアを作成
        chunk = self.tokens[idx:idx + self.block_size + 1]
        x = torch.tensor(chunk[:-1], dtype=torch.long)
        y = torch.tensor(chunk[1:], dtype=torch.long)
        return x, y
```

## 3.3.3 GPTモデルのアーキテクチャ

GPTシリーズのモデルは、第2章で学んだTransformerアーキテクチャのデコーダー部分のみを使用した設計となっています。第2章の翻訳モデルではエンコーダーが原文を処理し、デコーダーが翻訳文を生成する構造でしたが、GPTシリーズではエンコーダーを使用せず、デコーダーのみで言語モデリングを行います。この選択には明確な理由があります。GPTシリーズは文章を左から右へ順次生成していく自己回帰型モデルであり、未来の情報を参照せずに次の単語を予測する必要があるためです。

図3.3.1に、第2章で学んだTransformerアーキテクチャとGPT-2の構造的な違いを示します。この図から、GPT-2では元のTransformerデコーダーから以下の重要な変更が加えられていることがわかります。まず、最も顕著な違いは、エンコーダーとの相互作用を担うソース・ターゲットアテンション（クロスアテンション）層が完全に削除されている点です。これは、GPT-2が翻訳のような入力-出力ペアではなく、単一のテキストシーケンスから次の単語を予測する言語モデルとして設計されているためです。
位置情報の扱いも重要な変更点です。第2章で実装した標準的なTransformerでは、固定的な正弦波関数による位置エンコーディングを使用していましたが、GPT-2では学習可能な位置埋め込みを採用しています。これにより、モデルは学習データに応じて最適な位置表現を獲得できるようになり、より柔軟な文脈理解が可能となります。
また、レイヤー正規化の配置にも改良が加えられています。GPT-2ではPre-Layer Normalizationと呼ばれる手法を採用し、各サブレイヤー（自己アテンション層とフィードフォワード層）の前にレイヤー正規化を配置しています。図3.3.1では、この配置が明確に示されており、元のTransformerのPost-Layer Normalizationとは異なることがわかります。この変更により、より深いネットワークでも勾配の流れが改善され、学習の安定性が向上します。
図の下部に示されているように、これらのTransformerブロックはN層繰り返され、GPT-2の各バリアントでは異なる層数が使用されます。最小のGPT-2 Smallでは12層、最大のGPT-2 XLでは48層となり、モデルの規模に応じて調整されます。各ブロック内の自己アテンション層では、アテンションスコアが1/√d_kでスケーリングされることで勾配の安定性が保たれ、フィードフォワード層（MLP）と共に残差接続で結ばれています。さらに、過学習を防ぐためにDropoutが各層に適用されています。最終的に全ブロックを通過した後、最終的なレイヤー正規化を経て、線形層（言語モデルヘッド）によって語彙サイズへの投影が行われます。

モデルの中心となるのは、第2章2.2節で詳しく説明したマルチヘッドアテンション機構です。これにより、モデルは入力シーケンス内の異なる位置の関係性を同時に学習できます。第2章ではエンコーダーの自己アテンションとデコーダーのソース・ターゲットアテンションを学びましたが、GPTシリーズではデコーダーの自己アテンションのみを使用します。アテンション機構には因果的マスク（Causal Mask）が適用され、各位置が自分より後の位置の情報を参照できないようになっています。これは第2章でも触れた、学習時と推論時の条件を一致させるための重要な制約です。

モデルの基本構造を以下に示します。完全な実装は`llm_from_scratch/gpt/model.py`にありますが、ここでは主要な構成要素を抜粋して紹介します。

```python
class GPT(nn.Module):
    """GPTモデルの基本構造"""
    
    def __init__(self, vocab_size, n_embd=768, n_layer=12, n_head=12, 
                 block_size=1024, dropout=0.1):
        super().__init__()
        
        # トークンと位置の埋め込み
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.drop = nn.Dropout(dropout)
        
        # Transformerブロック（n_layer個）
        self.blocks = nn.Sequential(*[
            TransformerBlock(n_embd, n_head, dropout) 
            for _ in range(n_layer)
        ])
        
        # 最終層の正規化と出力層
        self.ln_f = nn.LayerNorm(n_embd)
        self.head = nn.Linear(n_embd, vocab_size, bias=False)
    
    def forward(self, idx, targets=None):
        B, T = idx.shape
        
        # トークンと位置の埋め込みを加算
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)
        tok_emb = self.token_embedding(idx)
        pos_emb = self.position_embedding(pos)
        x = self.drop(tok_emb + pos_emb)
        
        # Transformerブロックを通す
        x = self.blocks(x)
        x = self.ln_f(x)
        
        # 語彙への投影
        logits = self.head(x)
        
        # 損失の計算（学習時のみ）
        loss = None
        if targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss
```

このコードから、GPTモデルが語彙サイズ、埋め込み次元、レイヤー数、ヘッド数などのパラメータを指定することで、様々な規模のモデルを構築できることがわかります。

## 3.3.4 学習ループの実装

GPTモデルの学習では、AdamWオプティマイザーを使用します。AdamWは通常のAdamオプティマイザーにweight decayを正しく実装したバージョンで、大規模言語モデルの学習で標準的に使用されています。重要な点として、weight decayはすべてのパラメータに一律に適用するのではなく、バイアス項、レイヤー正規化のパラメータ、埋め込み層には適用しません。これは経験的に良い結果をもたらすことが知られています。

学習率のスケジューリングも重要な要素です。学習の初期段階では線形ウォームアップを行い、その後コサイン減衰スケジュールに従って学習率を減少させます。ウォームアップは、モデルのパラメータがランダムな初期値から始まる際の不安定性を軽減する効果があります。

学習ループでは、まずフォワードパスでモデルの予測を計算し、クロスエントロピー損失を算出します。次にバックプロパゲーションで勾配を計算し、勾配クリッピングを適用して勾配爆発を防ぎます。最後にオプティマイザーでパラメータを更新します。この一連の処理を指定されたステップ数だけ繰り返すことで、モデルは徐々に言語パターンを学習していきます。

完全な実装は`llm_from_scratch/gpt/trainer.py`にあり、GPT-2やGPT-3で使用されているものと同じbetas=(0.9, 0.95)の設定を採用しています。

## 3.3.5 学習の実行とテキスト生成

実際にGPTモデルを学習させる際には、まず小規模なデータセットで動作を確認することが重要です。本実装では、学習例として`examples/train_gpt_soseki.py`を提供しています。これは夏目漱石の作品（『吾輩は猫である』など）を使用しており、文字レベルでの言語モデリングを行います。

学習を実行するには、まずuvを使用して依存関係をインストールし、その後サンプルスクリプトを実行します。夏目漱石データセットでの学習（`train_gpt_soseki.py`）は日本語の文字数が多いため、GPUの使用を推奨します。学習が進むと、損失が徐々に減少してくことが確認できます。

学習が完了したモデルを使って実際にテキストを生成すると、以下のような結果が得られます。なお、GPT-2は純粋な言語モデルであり、短いプロンプト（開始文）を与えると、その続きを自動的に生成します。これはChatGPTのような対話型のモデルとは異なり、質問に答えるのではなく、文章の続きを予測・生成する仕組みです：

```python
# 「吾輩は」という入力に対する生成結果
入力: '吾輩は'
生成結果:
吾輩はようやく突然折して、吾輩の傍へ筆硯と原稿用紙を並べて腹這になって、
しきりに何か唸っている。大方草稿を書き卸す序開きとして妙な声を発するのだろうと
注目していると、ややしばらくして筆太に「香一」とかいた。

# 「夏の」という入力に対する生成結果
入力: '夏の'
生成結果:
夏の叔母さんと喧嘩をして帰って行く雪江とか云う奇麗な名のお嬢さんである。
もっとも顔は名前ほどでもない、ちょっと表へ出て一二町あるけば必ず逢える人相である。
```

このように、学習したモデルは夏目漱石の文体を模倣し、原作に登場する人物や情景を含んだ文章を生成できるようになります。文字レベルでの学習にもかかわらず、文法的にもある程度整った文章が生成されているのがわかります。

このような基本的な言語モデル（GPT-2）が、後にInstruction TuningやRLHF（Reinforcement Learning from Human Feedback）などの技術によって改良され、ChatGPTのような対話型AIへと発展していきます。第4章では、このような応用技術について詳しく学んでいきます。

生成時のサンプリング手法については、第2章2.5節で学んだ温度パラメータとtop-kサンプリングを実装しています。本実装のgenerateメソッドでは、温度パラメータで確率分布の鋭さを調整し、top-kサンプリングで上位k個の候補から選択することで、多様性と品質のバランスを取った文章生成を実現しています。なお、第2章で紹介したtop-pサンプリングは、実装の簡潔さを優先して本節では省略していますが、必要に応じて追加することも可能です。

## 3.3.6 大規模学習のための最適化とアーキテクチャの発展

実際の大規模なGPTシリーズのモデルの学習では、本実装よりもさらに高度な最適化技術とアーキテクチャの改良が使用されています。

### 学習の効率化技術

**Mixed Precision Training**では、FP16やBF16といった低精度の浮動小数点数を使用することで、メモリ使用量を削減し計算速度を向上させます。**Gradient Accumulation**を使用すると、物理的なバッチサイズの制限を超えて、実効的により大きなバッチサイズで学習することが可能になります。

複数のGPUやノードを使用した**Distributed Training**も、大規模モデルの学習には不可欠です。データ並列、モデル並列、パイプライン並列などの手法を組み合わせることで、数千億パラメータ規模のモデルも学習可能になります。

特に注目すべき技術として**Flash Attention**があります。これはアテンション計算をメモリ効率的に実装したもので、PyTorch 2.0以降では`torch.nn.functional.scaled_dot_product_attention`として利用可能です。本実装では教育的な観点から標準的なアテンション実装を採用していますが、実用的なシステムではFlash Attentionの使用を検討すべきでしょう。

### Mixture of Experts（MoE）アーキテクチャ

大規模言語モデルの計算効率を劇的に改善する技術として、**Mixture of Experts（MoE）**アーキテクチャがあります。MoEは複数の「エキスパート」と呼ばれる独立したフィードフォワードネットワークを持ち、ゲーティングネットワークが各入力トークンに対して適切なエキスパートを選択的に活性化する仕組みです。通常のTransformerではすべてのパラメータが常に使用されるのに対し、MoEではこの「スパース性」により計算量を大幅に削減します。

具体的な動作としては、8つのエキスパートを持つモデルで各トークンが2つのエキスパートのみを使用する場合、計算コストは全体の約4分の1になります。これによりモデル容量を拡大しながら計算コストを抑制でき、各エキスパートが異なるパターンや知識領域に特化することでモデル全体の表現力も向上します。例えば、1.76兆パラメータという巨大なモデルでも、実効的には440Bパラメータ相当の計算量で動作させることが可能です。

一方で、MoEには技術的な課題も存在します。すべてのエキスパートが均等に使用されるような負荷分散が必要であり、これが適切に行われないと一部のエキスパートに処理が集中してしまいます。また、ゲーティング機構の学習は不安定になりやすく、適切な正則化技術が必要です。さらに、推論時には使用されないエキスパートも含めて全パラメータをメモリに保持する必要があるため、メモリ要件は依然として高いままという課題もあります。

## 3.3.7 ハイパーパラメータ設定

GPTモデルの性能は、ハイパーパラメータの設定に大きく依存します。小規模な実験では、語彙サイズ1000、埋め込み次元128、4層のTransformerブロックといった設定で、約50万パラメータのモデルを構築できます。これは学習の動作確認や簡単な実験に適しています。

中規模のモデルとして、GPT-2 smallに相当する設定では、語彙サイズ50257、埋め込み次元768、12層のTransformerブロックを使用し、約1億2400万パラメータのモデルとなります。このサイズのモデルは、適切なデータセットで学習させることで、実用的な言語生成能力を獲得できます。

これらの設定は`GPTConfig`クラスで管理され、用途に応じて柔軟に調整可能です。モデルサイズと必要な計算リソース、期待される性能のバランスを考慮して、適切な設定を選択することが重要です。

## まとめ

本節では、GPTモデルの学習について、データ処理からモデル実装、学習ループ、テキスト生成まで、包括的に解説しました。実装はnanoGPTを参考にしながら、教育的な観点から理解しやすい構造にまとめています。

実装コードは、モデル本体が`llm_from_scratch/gpt/model.py`に、トークナイザーが`tokenizer.py`に、データセット処理が`dataset.py`に、学習ループが`trainer.py`にそれぞれ配置されています。また、実行可能なサンプルとして`examples/train_gpt_soseki.py`を用意しています。

これらの実装を通じて、GPTモデルの基本的な仕組みを理解し、実際に動作させることができるようになりました。実際の大規模なGPTモデルでは、分散学習やFlash Attentionなどのより高度な最適化技術が使用されますが、基本的な原理は本節で学んだものと同じです。次章では、これらの基礎的な実装をベースに、より実践的なファインチューニングや応用について学んでいきます。