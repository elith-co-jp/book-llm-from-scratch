# 3.3 GPTモデルの学習

第2章では、Transformerアーキテクチャの詳細を学び、エンコーダー・デコーダー構造を持つ翻訳モデルを実装しました。本節では、そのTransformerのデコーダー部分のみを使用したGPTモデルの学習について解説します。GPTは第2章で学んだアテンション機構やTransformerブロックの知識を基盤としながら、言語生成に特化した設計となっています。ここまでの章で学んだTransformerの基礎知識とGPTの概念、そしてトークナイザーの実装を組み合わせて、実際にGPTモデルを学習させる方法を段階的に理解していきましょう。

## nanoGPTについて

本節の実装は、Andrej Karpathy氏が開発したnanoGPTを参考にしています。nanoGPTは、GPTモデルの学習と推論を最小限のコードで実現した教育的なプロジェクトで、MITライセンスの下でGitHubリポジトリ（https://github.com/karpathy/nanoGPT）に公開されています。約300行というコンパクトなコードでGPT-2レベルのモデルを実装しており、シェイクスピアの作品集や大規模ウェブテキストコーパスであるOpenWebTextでの学習が可能です。教育目的に最適化された理解しやすい実装となっているため、GPTの仕組みを学ぶには最適な教材といえるでしょう。

本書の実装では、nanoGPTの設計思想を参考にしつつ、第2章で実装したTransformerの各モジュール（`llm_from_scratch/transformers/`内のマルチヘッドアテンション、位置エンコーディング、フィードフォワード層など）を再利用しています。これにより、第2章で学んだTransformerの翻訳モデルとGPTの関係性がより明確になり、両者が同じ基盤技術の上に構築されていることを実感できるでしょう。また、日本語テキストでの学習例として夏目漱石の作品を使用した実装も提供しています。

nanoGPTは、OpenAIのGPT-2モデルを再現することを目標としており、実際に124Mパラメータのモデルで同等の性能を達成しています。本節では、このnanoGPTの設計思想を参考にしながら、第2章のTransformer実装を基盤として、さらに初心者にも理解しやすいように段階的に実装を進めていきます。

## 3.3.1 学習の全体像

GPTシリーズ（GPT-1からGPT-4まで）に共通する学習プロセスは、大規模なテキストデータから言語の規則性やパターンを自動的に学習する過程です。本節では主にGPT-2規模のモデルを実装しますが、基本的な学習の仕組みはGPTシリーズ全体で共通しています。このプロセスは大きく6つのステップで構成されています。

まず最初に、生のテキストデータをモデルが処理できる形式に変換する必要があります。これがデータの準備段階で、テキストをトークン化し、学習用のバッチを作成します。次に、GPTアーキテクチャに基づいてモデルを定義します。このモデルは、入力されたトークン列から次のトークンを予測するように設計されています。

学習の中核となるのは損失関数の設定です。GPTシリーズでは次単語予測タスクのためにクロスエントロピー損失を使用します。この損失を最小化するために、AdamWオプティマイザーと学習率スケジューリングを組み合わせた最適化手法を採用します。実際の学習は、フォワードパス、バックプロパゲーション、パラメータ更新という一連の処理を繰り返すことで進行します。最後に、学習したモデルの性能を評価し、実際にテキストを生成させることで、モデルが言語を適切に学習できているかを確認します。

## 3.3.2 データの準備とデータローダー

GPTシリーズのモデルの学習において、データの準備は極めて重要な工程です。GPTシリーズは自己回帰型モデルとして設計されており、テキストシーケンスから次の単語を予測することで言語を学習します。この学習方式を実現するために、適切なデータ処理が必要となります。

データセットクラスの実装では、入力テキストをトークン化し、指定された長さのチャンクに分割します。各チャンクから入力シーケンスとターゲットシーケンスのペアを作成します。例えば、"The cat sat on the mat"というテキストがあった場合、"The cat sat on the"を入力として"cat sat on the mat"を出力として学習させます。これにより、モデルは文脈から次の単語を予測する能力を獲得していきます。

実装の詳細は`llm_from_scratch/gpt/dataset.py`および`llm_from_scratch/gpt/tokenizer.py`に記載されています。基本的な構造として、TextDatasetクラスはテキストとトークナイザー、ブロックサイズ（コンテキスト長）を受け取り、学習用のデータを準備します。

```python
class TextDataset(Dataset):
    """テキストデータセットクラス"""
    
    def __init__(self, text, tokenizer, block_size=128):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.tokens = tokenizer.encode(text)
    
    def __getitem__(self, idx):
        # 入力とターゲットのペアを作成
        chunk = self.tokens[idx:idx + self.block_size + 1]
        x = torch.tensor(chunk[:-1], dtype=torch.long)
        y = torch.tensor(chunk[1:], dtype=torch.long)
        return x, y
```

## 3.3.3 GPTモデルのアーキテクチャ

GPTシリーズのモデルは、第2章で学んだTransformerアーキテクチャのデコーダー部分のみを使用した設計となっています。第2章の翻訳モデルではエンコーダーが原文を処理し、デコーダーが翻訳文を生成する構造でしたが、GPTシリーズではエンコーダーを使用せず、デコーダーのみで言語モデリングを行います。この選択には明確な理由があります。GPTシリーズは文章を左から右へ順次生成していく自己回帰型モデルであり、未来の情報を参照せずに次の単語を予測する必要があるためです。

モデルの中心となるのは、第2章2.2節で詳しく説明したマルチヘッドアテンション機構です。これにより、モデルは入力シーケンス内の異なる位置の関係性を同時に学習できます。第2章ではエンコーダーの自己アテンションとデコーダーのソース・ターゲットアテンションを学びましたが、GPTシリーズではデコーダーの自己アテンションのみを使用します。アテンション機構には因果的マスク（Causal Mask）が適用され、各位置が自分より後の位置の情報を参照できないようになっています。これは第2章でも触れた、学習時と推論時の条件を一致させるための重要な制約です。

GPT-2の各Transformerブロックは、アテンション層とフィードフォワード層（MLP）で構成されています。両方の層には残差接続とレイヤー正規化が適用され、深いネットワークでも安定した学習が可能になっています。アテンションスコアは`1/√d_k`でスケーリングされ、勾配の安定性が保たれています。また、過学習を防ぐためにDropoutが各層に適用されています。

モデルの基本構造を以下に示します。完全な実装は`llm_from_scratch/gpt/model.py`にありますが、ここでは主要な構成要素を抜粋して紹介します。

```python
class GPT(nn.Module):
    """GPTモデルの基本構造"""
    
    def __init__(self, vocab_size, n_embd=768, n_layer=12, n_head=12, 
                 block_size=1024, dropout=0.1):
        super().__init__()
        
        # トークンと位置の埋め込み
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.drop = nn.Dropout(dropout)
        
        # Transformerブロック（n_layer個）
        self.blocks = nn.Sequential(*[
            TransformerBlock(n_embd, n_head, dropout) 
            for _ in range(n_layer)
        ])
        
        # 最終層の正規化と出力層
        self.ln_f = nn.LayerNorm(n_embd)
        self.head = nn.Linear(n_embd, vocab_size, bias=False)
    
    def forward(self, idx, targets=None):
        B, T = idx.shape
        
        # トークンと位置の埋め込みを加算
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)
        tok_emb = self.token_embedding(idx)
        pos_emb = self.position_embedding(pos)
        x = self.drop(tok_emb + pos_emb)
        
        # Transformerブロックを通す
        x = self.blocks(x)
        x = self.ln_f(x)
        
        # 語彙への投影
        logits = self.head(x)
        
        # 損失の計算（学習時のみ）
        loss = None
        if targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss
```

このコードから、GPTモデルが語彙サイズ、埋め込み次元、レイヤー数、ヘッド数などのパラメータを指定することで、様々な規模のモデルを構築できることがわかります。

## 3.3.4 学習ループの実装

GPTモデルの学習では、AdamWオプティマイザーを使用します。AdamWは通常のAdamオプティマイザーにweight decayを正しく実装したバージョンで、大規模言語モデルの学習で標準的に使用されています。重要な点として、weight decayはすべてのパラメータに一律に適用するのではなく、バイアス項、レイヤー正規化のパラメータ、埋め込み層には適用しません。これは経験的に良い結果をもたらすことが知られています。

学習率のスケジューリングも重要な要素です。学習の初期段階では線形ウォームアップを行い、その後コサイン減衰スケジュールに従って学習率を減少させます。ウォームアップは、モデルのパラメータがランダムな初期値から始まる際の不安定性を軽減する効果があります。

学習ループでは、まずフォワードパスでモデルの予測を計算し、クロスエントロピー損失を算出します。次にバックプロパゲーションで勾配を計算し、勾配クリッピングを適用して勾配爆発を防ぎます。最後にオプティマイザーでパラメータを更新します。この一連の処理を指定されたステップ数だけ繰り返すことで、モデルは徐々に言語パターンを学習していきます。

完全な実装は`llm_from_scratch/gpt/trainer.py`にあり、GPT-2やGPT-3で使用されているものと同じbetas=(0.9, 0.95)の設定を採用しています。

## 3.3.5 学習の実行とテキスト生成

実際にGPTモデルを学習させる際には、まず小規模なデータセットで動作を確認することが重要です。本実装では、学習例として`examples/train_gpt_soseki.py`を提供しています。これは夏目漱石の作品（『吾輩は猫である』など）を使用しており、文字レベルでの言語モデリングを行います。

学習を実行するには、まずuvを使用して依存関係をインストールし、その後サンプルスクリプトを実行します。夏目漱石データセットでの学習（`train_gpt_soseki.py`）は日本語の文字数が多いため、GPUの使用を推奨します。学習が進むと、損失が徐々に減少してくことが確認できます。

学習が完了したモデルを使って実際にテキストを生成すると、以下のような結果が得られます。なお、GPT-2は純粋な言語モデルであり、短いプロンプト（開始文）を与えると、その続きを自動的に生成します。これはChatGPTのような対話型のモデルとは異なり、質問に答えるのではなく、文章の続きを予測・生成する仕組みです：

```python
# 「吾輩は」という入力に対する生成結果
入力: '吾輩は'
生成結果:
吾輩はようやく突然折して、吾輩の傍へ筆硯と原稿用紙を並べて腹這になって、
しきりに何か唸っている。大方草稿を書き卸す序開きとして妙な声を発するのだろうと
注目していると、ややしばらくして筆太に「香一」とかいた。

# 「夏の」という入力に対する生成結果
入力: '夏の'
生成結果:
夏の叔母さんと喧嘩をして帰って行く雪江とか云う奇麗な名のお嬢さんである。
もっとも顔は名前ほどでもない、ちょっと表へ出て一二町あるけば必ず逢える人相である。
```

このように、学習したモデルは夏目漱石の文体を模倣し、原作に登場する人物や情景を含んだ文章を生成できるようになります。文字レベルでの学習にもかかわらず、文法的にもある程度整った文章が生成されているのがわかります。

このような基本的な言語モデル（GPT-2）が、後にInstruction TuningやRLHF（Reinforcement Learning from Human Feedback）などの技術によって改良され、ChatGPTのような対話型AIへと発展していきます。第4章では、このような応用技術について詳しく学んでいきます。

生成時のサンプリング手法については、第2章2.5節で学んだ温度パラメータとtop-kサンプリングを実装しています。本実装のgenerateメソッドでは、温度パラメータで確率分布の鋭さを調整し、top-kサンプリングで上位k個の候補から選択することで、多様性と品質のバランスを取った文章生成を実現しています。なお、第2章で紹介したtop-pサンプリングは、実装の簡潔さを優先して本節では省略していますが、必要に応じて追加することも可能です。

## 3.3.6 大規模学習のための最適化

実際の大規模なGPTシリーズのモデルの学習では、本実装よりもさらに高度な最適化技術が使用されています。Mixed Precision Trainingでは、FP16やBF16といった低精度の浮動小数点数を使用することで、メモリ使用量を削減し計算速度を向上させます。Gradient Accumulationを使用すると、物理的なバッチサイズの制限を超えて、実効的により大きなバッチサイズで学習することが可能になります。

複数のGPUやノードを使用したDistributed Trainingも、大規模モデルの学習には不可欠です。データ並列、モデル並列、パイプライン並列などの手法を組み合わせることで、数千億パラメータ規模のモデルも学習可能になります。

特に注目すべき技術としてFlash Attentionがあります。これはアテンション計算をメモリ効率的に実装したもので、PyTorch 2.0以降では`torch.nn.functional.scaled_dot_product_attention`として利用可能です。本実装では教育的な観点から標準的なアテンション実装を採用していますが、実用的なシステムではFlash Attentionの使用を検討すべきでしょう。

## 3.3.7 ハイパーパラメータ設定

GPTモデルの性能は、ハイパーパラメータの設定に大きく依存します。小規模な実験では、語彙サイズ1000、埋め込み次元128、4層のTransformerブロックといった設定で、約50万パラメータのモデルを構築できます。これは学習の動作確認や簡単な実験に適しています。

中規模のモデルとして、GPT-2 smallに相当する設定では、語彙サイズ50257、埋め込み次元768、12層のTransformerブロックを使用し、約1億2400万パラメータのモデルとなります。このサイズのモデルは、適切なデータセットで学習させることで、実用的な言語生成能力を獲得できます。

これらの設定は`GPTConfig`クラスで管理され、用途に応じて柔軟に調整可能です。モデルサイズと必要な計算リソース、期待される性能のバランスを考慮して、適切な設定を選択することが重要です。

## まとめ

本節では、GPTモデルの学習について、データ処理からモデル実装、学習ループ、テキスト生成まで、包括的に解説しました。実装はnanoGPTを参考にしながら、教育的な観点から理解しやすい構造にまとめています。

実装コードは、モデル本体が`llm_from_scratch/gpt/model.py`に、トークナイザーが`tokenizer.py`に、データセット処理が`dataset.py`に、学習ループが`trainer.py`にそれぞれ配置されています。また、実行可能なサンプルとして`examples/train_gpt_soseki.py`を用意しています。

これらの実装を通じて、GPTモデルの基本的な仕組みを理解し、実際に動作させることができるようになりました。実際の大規模なGPTモデルでは、分散学習やFlash Attentionなどのより高度な最適化技術が使用されますが、基本的な原理は本節で学んだものと同じです。次章では、これらの基礎的な実装をベースに、より実践的なファインチューニングや応用について学んでいきます。