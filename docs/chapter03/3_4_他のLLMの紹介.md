# 3.4 他のLLMの紹介

大規模言語モデル（LLM）は、近年の人工知能（AI）の発展において重要な役割を果たしています。特に、自然言語処理（NLP）分野におけるLLMの進化は、さまざまな産業や日常生活に大きな影響を与えています。本章では、代表的なLLMの発展とその応用について詳しく探ります。

第3.4節では、具体的なモデルの発展とその技術的特徴を中心に解説します。3.4.1項では、ビッグテックが開発したLLMについて、それぞれのモデルの進化過程、技術的な革新点、実際の応用事例を紹介します。

次に、3.4.2項では、その他の注目すべきLLMと、その多様な応用について説明します。特に、マルチモーダルモデルの台頭に焦点を当て、テキスト、画像、音声などの異なるモダリティを統合して処理する能力がどのように進化しているかを示します。これにより、LLMの応用範囲が広がり、新たな技術革新がもたらす可能性について考察します。

本節を通じて、LLMの最新動向とその未来像について深く理解し、技術の進化が私たちの生活や仕事にどのように影響を与えるのかを考えていきましょう。

# 3.4.1 ビッグテックによる主要LLMの発展

## Metaが開発したモデル

Metaは複数の注目すべきLLMを開発しています。その代表的なモデルには以下のものがあります。

**BlenderBot**

流暢な対話の実現を目的としたこのモデルは、最大90億パラメータを持つ初期バージョンから始まりました。2022年に発表されたBlenderBot3は、OPT-175Bを基に構築され、感情理解、知識ベースの質問応答、長期記憶の組み込み、インターネット検索機能を統合している点が特徴的です。

**OPT（Open Pre-trained Transformer Language Models）**

GPT-3に匹敵する性能を持つオープンソースモデルとして注目を集めたOPTは、1.25億から1,750億パラメータまでの9種類のモデルが提供されています。テキスト生成、分類、質問応答など多様なNLPタスクに対応し、事前学習済みモデルとソースコードが一般公開されていますが、OPT-175Bは非商用ライセンスとなっている点に注意が必要です。

**LLaMA**

LLaMA1、LLaMA2、LLaMA3からLLaMa4へととバージョンアップを重ねているLLaMAシリーズは、オープンソース戦略の中核として進化を遂げてきており、LLaMA 3（2024年4月）の8B、70Bモデルから始まり、LLaMA 3.1（2024年7月）では405Bという世界最大級のオープンソースモデルを公開しました。LLaMA 4（2025年4月）はMixture of Experts（MoE）アーキテクチャを採用し、Scoutモデルで1000万トークンという画期的なコンテキストウィンドウを実現、Maverickは400B総パラメータ（17B活性）で100万トークンを処理可能です。現在訓練中のBehemoth（約2兆パラメータ）は、ネイティブマルチモーダル機能と12言語サポートを提供予定です。

## Googleが開発したモデル

Googleも複数の先進的なLLMを開発しています。主要なモデルは以下の通りです。

**LaMDA（Language Model for Dialogue Applications）**

対話生成に特化したこのモデルは、膨大な会話データでトレーニングされています。幅広いトピックで自然な対話が可能であり、事実への忠実性を重視している点が特徴です。初期のBardの内部LLMとして採用されましたが、現在はGeminiシリーズに統合されています。

**PaLM（Pathways Language Model）**

5,400億パラメータを持つこの大規模モデルは、Webページ、Wikipedia、ニュース記事、GitHubのオープンソースコード、ソーシャルメディアの会話データなど、多様なソースからのデータで学習されています。6,144個のチップを使用した大規模トレーニングを行い、57.8%という高いハードウェアFLOP使用率を達成しています。PaLM 2を経て、現在はその技術がGeminiシリーズに継承されています。

**Flamingo**

DeepMindが開発したこのマルチモーダルモデルは、自然言語処理とコンピュータビジョンを統合しています。30億、90億、800億パラメータの3つのサイズがあり、視覚エンコーダーにOpenAIのCLIPを使用しています。画像キャプション生成、画像説明、画像ベースの質問応答に強みを持ち、高いfew-shot学習能力を示しています。YouTubeショートの検索システムにも採用されています。

**Gemini**

LaMDAとPaLMの技術を統合・進化させたGeminiシリーズは、Googleの主力LLMとして発展しています。Gemini 1.5 Pro（2024年2月）は当時としては初の200万トークンのコンテキストウィンドウを実現し、2時間の動画や6万行以上のコードを処理可能になりました。Gemini 2.0 Flash（2024年12月）は「エージェント時代」向けに設計され、ネイティブマルチモーダル出力（テキスト、画像、音声）と双方向ストリーミングを実装しています。Gemini 2.5 Pro（2025年3月実験版、6月一般提供）は初の「思考モデル」として内蔵推論機能を搭載し、LMArenaリーダーボードで1位を獲得、AIME 2025で88.0%という驚異的なスコアを記録しています。Deep Thinkモードでは、適応的な思考バジェットにより性能とコストを最適化しています。

## OpenAIが開発したモデル

OpenAIは、GPTシリーズを中心に革新的なLLMを次々と開発してきました。主要なモデルの進化は以下の通りです。この辺りは3.1節で、ある程度説明したので省略しています。

**GPTシリーズ**

- GPT-1（2018年）は1.17億パラメータを持ち、トランスフォーマーアーキテクチャを使用して文脈に基づいたテキスト生成に優れていました。
- GPT-2（2019年）は15億パラメータに拡大され、より大規模なデータセットでトレーニングされ、長文生成、要約、翻訳、質問応答など多様なタスクに対応できるようになりました。
- GPT-3（2019年）は1,750億パラメータという巨大なモデルとなり、多様なNLPタスクに対応する高い汎用性と、few-shot学習での優れた性能を示しました。
- InstructGPT（2022年）はGPT-3をベースに、人間の意図により沿ったテキスト生成を実現し、現代的な指示ベースのAIモデルの基盤となりました。
- GPT-4（2023年）はさらに性能と汎用性を向上させ、その後のGPT-4Vでマルチモーダル機能（画像理解・処理）が追加されました。GPT-4oでは音声認識・生成機能やリアルタイムビデオ解析機能も統合されています。
- GPT-4.1（2025年4月）は100万トークンのコンテキストウィンドウを実現し、SWE-bench Verifiedで54.6%を記録しました。
- GPT-5（2025年8月7日リリース）は統一アーキテクチャで推論と非推論機能を融合し、256K（ChatGPT）から400K（API）トークンのコンテキストを提供しています。

**推論特化モデル**

OpenAIはGPTシリーズと並行して、Project Strawberry技術を基にした推論特化モデルo1、o3シリーズを開発しており、MATHベンチマークで90%以上のスコアを達成しています。

## Anthropicが開発したモデル

Anthropicの主要なモデルは以下の通りです。

**Claude 3ファミリー（2024年3月）**
Opus、Sonnet、Haikuの3つのモデルで展開され、それぞれ異なる用途に最適化されています。標準で20万トークン、選択された顧客向けには100万トークンのコンテキストウィンドウを提供しており、長文処理において当時から業界をリードしていました。

**Claude 3.5シリーズ**
Claude 3.5 Sonnet（2024年6月）は、Claude 3 Opusを上回る性能を同価格で実現し、HumanEvalで92.0%という業界最高水準のコーディング性能を達成しました。2024年10月には革新的な「コンピューター使用機能」をパブリックベータで提供開始し、画面の解釈とキーボード・マウス操作のシミュレーションを可能にしました。Claude 3.5 Haiku（2024年11月）も追加され、高速で低コストな選択肢を提供しています。

**Claude 3.7（2025年2月）**
ハイブリッド推論という仕組みを導入し、単一モデルで即時応答と拡張思考の二重モードを実現しました。これにより、タスクの複雑さに応じて自動的に最適な推論方法を選択できるようになっています。

**Claude 4シリーズ（2025年5月～8月）**
最新のClaude 4ファミリーでは、Opus 4がSWE-bench Verifiedで72.5%という業界トップのスコアを達成しました。Claude 4 Opus 4.1（2025年8月5日）は、前バージョンから50%の高速化と45%少ないツール使用を実現し、複数のベンチマークで大幅な性能向上を達成しています。憲法的AI（Constitutional AI）の原則に基づいた安全性の向上も継続的に実装されています。

## その他の注目すべきモデル

主要4社以外にも、特に中国系企業を中心に革新的なLLMが次々と登場しています。

### **中国系モデル**

**DeepSeek**
DeepSeek-V3（2024年12月）は671Bパラメータを持ちながら、わずか560万ドルという驚異的な低コストで訓練されたとされています。DeepSeek-R1（2025年1月）は推論特化モデルとして、MATHベンチマークで97.3%を達成し、OpenAI o1に匹敵する性能を実現しています。MITライセンスで提供され、完全な商用利用が可能です。

**Qwen- Alibaba**
Qwen 3（2025年4月）は235B MoEアーキテクチャ（22B活性）を採用し、119言語をサポートするグローバルモデルです。36兆トークンで訓練され、MMLU-Reduxで92.7%、LiveCodeBenchで70.7%という優れたスコアを記録しています。

**GLM**
GLM-4.5（2025年7月）は355Bパラメータの「エージェントネイティブ」モデルとして設計され、複雑なタスクの自律的実行に特化しています。MITライセンスで提供され、中国語と英語の両方で高い性能を発揮します。

### **欧州・その他のモデル**

**Mistral AI（フランス）**
Mixtral 8x22B（2024年4月）は176B総パラメータのスパースMoEアーキテクチャで、39B活性パラメータで効率的に動作します。Magistral（2025年）は同社初の推論特化モデルとして、複雑な推論タスクに対応しています。Apache 2.0ライセンスで完全商用利用が可能です。

**xAI Grok**
Grok-4（2025年7月）は「世界で最も知能的なモデル」を標榜し、ネイティブツール使用機能とリアルタイム情報アクセスを特徴としています。X（旧Twitter）のデータを活用した独自の学習により、最新の話題に強い特性を持ちます。

**その他の注目モデル**
CohereのCommand R+（100B+）、Stability AIのStableLM 2（12B）、Hugging FaceのBLOOM（176B、46言語対応）などが、それぞれ特定の用途で強みを発揮しています。

## 主要LLMの機能比較

以下の表は、2025年時点での主要LLMの技術的特徴を比較したものです。各モデルの重要な属性として、推論特化機能、マルチモーダル対応、コンテキスト長、オープンソース化の状況をまとめています。

| モデル名 | 開発企業 | パラメータ数 | 推論特化 | マルチモーダル | コンテキスト長 | オープンソース | 特徴 |
|---------|---------|------------|---------|--------------|--------------|--------------|------|
| **GPT-5** | OpenAI | 非公開 | ○（統合型） | ○（テキスト/画像/音声/動画） | 256K-400K | × | 統一アーキテクチャで推論と生成を融合 |
| **GPT-4** | OpenAI | 1.76T | × | ○（GPT-4V/4o） | 128K | × | 業界最高水準の汎用性能 |
| **o3** | OpenAI | 非公開 | ◎（専用） | × | 128K | × | MATH 90%以上、競技プログラミングで人間超え |
| **Claude 4 Opus 4.1** | Anthropic | 非公開 | ○（ハイブリッド） | ○（画像） | 200K | × | SWE-bench 72.5%でエンジニアリングタスク最高性能 |
| **Claude 3.5 Sonnet** | Anthropic | 非公開 | × | ○（画像+PC操作） | 200K | × | コンピューター使用機能でGUI自動操作可能 |
| **Gemini 2.5 Pro** | Google | 非公開 | ◎（思考モデル） | ○（ネイティブ） | 2M | × | Deep Thinkモードで適応的推論時間調整 |
| **Gemini 2.0 Flash** | Google | 非公開 | × | ○（双方向ストリーミング） | 1M | × | リアルタイム双方向対話に最適化 |
| **LLaMA 4 Scout** | Meta | 非公開 | × | ○（ネイティブ） | 10M | ○ | 世界最長1000万トークンコンテキスト |
| **LLaMA 4 Maverick** | Meta | 400B（17B活性） | × | ○ | 1M | ○ | 12言語ネイティブサポート |
| **LLaMA 3.2** | Meta | 11B/90B | × | ○（画像） | 128K | ○ | オープンソース初の本格マルチモーダル |
| **DeepSeek-V3** | DeepSeek | 671B | × | × | 128K | ○（MIT） | 560万ドルという驚異的低コストで訓練 |
| **DeepSeek-R1** | DeepSeek | 非公開 | ◎（専用） | × | 128K | ○（MIT） | MATH 97.3%で数学問題解決世界最高 |
| **Qwen 3** | Alibaba | 235B（22B活性） | × | ○ | 128K | ○ | 119言語サポートでグローバル対応 |
| **GLM-4.5** | Zhipu AI | 355B | × | ○ | 128K | ○（MIT） | 自律的エージェント実行に特化 |
| **Mixtral 8x22B** | Mistral AI | 176B（39B活性） | × | × | 64K | ○（Apache2.0） | 完全商用可能な最大級オープンソース |
| **Magistral** | Mistral AI | 非公開 | ◎（専用） | × | 32K | ○（Apache2.0） | 欧州初の推論特化モデル |
| **Grok-4** | xAI | 非公開 | × | ○ | 128K | × | X(Twitter)データ活用 |

### 技術トレンドの分析

この比較表から、現在のLLM開発における重要なトレンドが明確に読み取れます。まず最も顕著な動向として、推論特化モデルの台頭が挙げられます。OpenAI o3、DeepSeek-R1、Gemini 2.5 Proといった専用モデルが、数学や科学の問題において人間の専門家レベルの性能を達成しています。これらのモデルは「思考の連鎖」を内蔵し、段階的な推論プロセスを経て回答を生成する能力を持っています。

マルチモーダル機能も急速に標準化されています。2025年の主要モデルの約80%が何らかの形でマルチモーダル機能を搭載し、特にGPT-5やGemini 2.0は「ネイティブマルチモーダル」として、最初から複数のモダリティを統合的に処理することを前提に設計されています。Claude 3.5の「コンピューター使用機能」のような革新的な機能も登場し、従来の画像認識を超えた新しい形の相互作用を可能にしています。

コンテキスト長の拡大も著しい進歩を見せています。LLaMA 4 Scoutの1000万トークン、Gemini 2.5 Proの200万トークンといった、従来の限界を大きく超えるコンテキスト処理能力により、長大な文書や動画の一括処理が可能になりました。これは実用面で大きな影響を与え、複雑な業務タスクへの適用可能性を大幅に広げています。

最後に、オープンソース化の進展も重要なトレンドです。中国系モデル（DeepSeek、Qwen、GLM）に加えて、MetaのLLaMAシリーズやMistral AIが積極的にオープンソース戦略を推進しています。特にMITやApache 2.0ライセンスで提供される完全商用利用可能な高性能モデルの増加により、LLM技術の民主化が急速に進んでいます。

## アーキテクチャレベルでのLLM改善と効率化手法

### **Mixture of Experts（MoE）の標準化**

MoEアーキテクチャは2024-2025年に業界標準として確立されました（MoEの技術的詳細については3.3.6節を参照）。各社の主要モデルでの実装例として、GPT-4が8つのエキスパート（各220B）を使用し、DeepSeek-V3では「共有エキスパート」という一部のエキスパートを常に活性化させる概念を導入、LLaMA 4ではMoE層と密な層を交互に配置する設計を採用するなど、それぞれが独自の工夫を加えています。

### **Flash Attention 3とメモリ最適化**

Flash Attention 3（2024年）はNVIDIA Hopper GPUに最適化され、非同期処理とFP8サポートにより2-4倍の高速化を実現しました。Grouped-query Attention（GQA）は、Multi-head AttentionとMulti-query Attentionの中間的手法として、多くの最新モデルで採用されています。

**Grouped-query Attention**

Grouped-query attentionは、Multi-query attentionの表現力の低下を改善するために考案されたアーキテクチャです。この手法では、Queryをグループ化し、グループごとに固有のKeyとValueを使用して自己注意機構を計算します。各グループ内では、キーとバリューの重み行列を共有します。これにより、Multi-query attentionと比べて表現力を向上させつつ、計算量とメモリ使用量を削減できます。Grouped-query attentionは、Multi-query attentionとMulti-head attentionの中間的な手法と言えます。

![fig.1 **Grouped-query Attentionのイメージ**](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/a867fdb3-80aa-45fd-ad7a-51712cea9363/8ddae6a4-1093-42d2-8aba-4723aa2b5f99.png)

fig.1 **Grouped-query Attentionのイメージ**

### **KVキャッシュ**

KVキャッシュは、Transformerモデルにおける推論時の計算効率を向上させるための重要なアーキテクチャです。Self-Attentionでは、各トークンと他のすべてのトークンとの関連性が計算されますが、過去のトークン同士の関連性は一度計算されれば変化しないため、再計算する必要はありません。KVキャッシュは、このような過去の計算結果を保存し、再利用するメカニズムです。これにより、推論時の計算コストが大幅に削減され、より効率的な処理が可能となります。最新の実装では、PagedAttention（vLLM）により動的なメモリ管理が可能になり、複数のリクエストでKVキャッシュを共有するMulti-Query Cachingも実用化されました。これにより、推論時のメモリ使用量を最大90%削減しながら、スループットを10倍以上向上させることが可能になっています。

### **レイヤー正規化**

従来のPre-LNとPost-LNに加え、RMSNorm（Root Mean Square Normalization）が多くの最新モデルで採用されています。RMSNormは計算効率が高く、LLaMA、Gemini、Mistralなどで標準的に使用されています。また、DeepNormやSubLNなどの新手法により、1000層を超える超深層モデルの安定的な学習が可能になりました。

## まとめ

ビッグテック企業は、革新的な大規模言語モデル（LLM）の開発を通じて、自然言語処理（NLP）の分野で大きな進歩を遂げています。それぞれの企業が開発したモデルは、対話生成や多モーダル処理において卓越した性能を発揮し、汎用性、性能、商用利用の可能性など、多様なニーズに応えるために進化を続けています。これらのモデルは、AIの実用化をさらに推進し、幅広い応用分野での利用が期待されています。また、アーキテクチャレベルでの改善も進んでいます。

# 3.4.2 その他のLLM

## **マルチモーダルモデルの台頭**

マルチモーダルモデルは、テキスト、画像、音声などの複数のモダリティを入出力に利用可能なモデルです。これにより、異なるモダリティ間を繋ぐことが可能になり、テキストだけでは利用できない幅広い分野への応用が可能になります。

近年、LLMをバックボーンに採用することで、LLMの汎用的な知識や推論能力をテキスト以外のモダリティにも活用できるようになり、マルチモーダルモデルの能力が劇的に改善しています。例えば、Med-Geminiは医療分野に適用され、CT画像などの医療画像を元に病状を説明したり、画像に対して質問することが可能です。また、RT-2（現在はRT-Xに進化）はロボット分野への応用研究として、周囲の環境を写した画像と指示文を入力することで、ロボットが実環境で指示に沿った行動を実行することができます。

当初は特別な機能として注目されていたマルチモーダル処理ですが、2024-2025年にかけて急速に「標準機能」へと進化しました。現在では主要なLLMの多くが、何らかの形でマルチモーダル機能を搭載するようになっています。

![fig.1 マルチモーダルモデルのイメージ](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/0dd7f70b-443c-40fe-b9ba-0ed96d23c2a5/Untitled.png)

fig.1 マルチモーダルモデルのイメージ

**GPT-4oとGPT-5**：OpenAIのGPT-4o（2024年5月）は、テキスト、画像、音声、動画を統合的に処理する真のマルチモーダルモデルです。232ミリ秒という人間の会話速度に近い音声応答を実現し、リアルタイムビデオ解析により、例えばスポーツの試合を見ながらルールを説明するなどの複雑なタスクが可能です。OCR機能により画像内のテキストも高精度で読み取ります。GPT-5（2025年8月）では、これらの機能が統一アーキテクチャでさらに洗練され、256K-400Kトークンのコンテキストでマルチモーダル処理が可能になっています。

**Geminiシリーズ**：Google DeepMindのGemini 1.5 Pro（2024年2月）は、業界初の200万トークンコンテキストを実現し、2時間の動画や数万枚の画像を一度に処理できます。Gemini 2.0 Flash（2024年12月）は「ネイティブマルチモーダル」として設計され、テキスト、画像、音声を同時に生成する真の統合型モデルです。最新のGemini 2.5（2025年3月）では、これらに思考機能が加わり、複雑な視覚的推論タスクでも優れた性能を示しています。

**LLaMA 3.2とLLaMA 4**：MetaのLLaMA 3.2（2024年9月）は、オープンソース初の本格的マルチモーダルモデル（11B、90B）として登場し、画像理解機能を無料で提供しました。LLaMA 4（2025年4月）では、ネイティブマルチモーダル機能が全モデルに標準装備され、最大1000万トークンのコンテキストで動画処理も可能になっています。これにより、オープンソースでも商用モデルと同等のマルチモーダル処理が実現されました。

**Claude 3.5の革新的機能**：Anthropicは2024年10月に「コンピューター使用機能」という画期的な機能を導入しました。これは従来の画像認識を超えて、実際のコンピューター画面を解釈し、キーボード・マウス操作をシミュレートできる機能です。例えば、ウェブサイトのナビゲーションやフォーム入力を自動化でき、新しい形のマルチモーダル相互作用を実現しています。

![fig.2 LLaVAの模式図](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/ea96966c-e790-4f59-ba2d-b3d6d29ac43a/Untitled.png)

fig.2 LLaVAの模式図

![fig.3 Geminiの模式図](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/5664078d-9fbc-414d-a6f1-497767d4a382/Untitled.png)

fig.3 Geminiの模式図

## **推論強化モデルの登場**

2024年後半から2025年にかけて、複雑な推論タスクに特化した新しいカテゴリーのモデルが登場しました。OpenAIのo1/o3シリーズ、DeepSeek-R1（中国）、Gemini 2.5 Proの思考モデルなどが、数学や科学の難問で人間の専門家に匹敵する性能を示しています。これらのモデルは「思考の連鎖」を内蔵し、段階的な推論プロセスを経て回答を生成します（詳細な学習手法については5章で扱います）。

## **特化型モデルの多様化**

汎用的なLLMとは別に、特定の分野に特化したモデルの開発も活発です。**コード生成特化**ではGitHub Copilot、Code Llama、DeepSeek-Coder V2などが実用レベルに達し、**科学・数学特化**ではDeepSeek-MathやAlphaFold 3（タンパク質構造予測）が専門家レベルの性能を実現しています。これらの特化型モデルは、より小さなパラメータ数で特定分野において汎用モデルと同等以上の性能を達成しています。

## まとめ

ビッグテック以外の企業や研究機関も、多様な大規模言語モデル（LLM）を開発し、それぞれが独自の強みを発揮しています。特にマルチモーダルモデルの進化は、テキスト、画像、音声などの異なるモダリティを統合して処理する能力を飛躍的に向上させています。これにより、医療、ロボット工学など、幅広い分野への応用が可能となり、LLMの利用範囲がさらに拡大しています。これらの技術革新は、AIの多様な応用に新たな可能性をもたらしています。