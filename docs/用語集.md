正	備考	別名1	別名2	別名3	別名4
アテンション機構		アテンションメカニズム	注意機構	Attention Mechanism	アテンション機構
アテンション重み		アテンションウェイト	注意度	Attention Weight	アテンション重み
マルチヘッドアテンション		マルチヘッドアテンション	多頭注意	Multi-Head Attention	多頭アテンション
内積アテンション			内積注意	Dot-Product Attention	内積アテンション
加法アテンション			加法注意	Additive Attention	加法アテンション
softmax 関数				softmax 関数	ソフトマックス関数
スケール化内積アテンション			スケール化内積注意	Scaled Dot-Product Attention	スケール化内積アテンション
エンコーダ		エンコーダ		Encoder	
デコーダ		デコーダ		Decoder	
Transformer		トランスフォーマー		Transformer	
パディングマスク		パディングマスク		Padding mask	
後続マスク	和訳がほぼないので新たに後続マスクとして導入したい	サブセクエントマスク		Subsequent mask	後続マスク
自己アテンション		セルフアテンション	自己注意機構	Self-Attention	自己アテンション
ソース・ターゲットアテンション		ソース・ターゲットアテンション		Source-Target Attention	
レイヤー正規化				Layer Normalization	Layer Norm
フィードフォワード層				FFN	Feed Forward
損失関数			誤差関数	Loss Function	コスト関数
ファインチューニング		ファインチューニング	微調整	Fine Tuning	
インストラクションチューニング		インストラクションチューニング	指示調整	Instruction Tuning	指示チューニング
ボキャブラリー	2章は、ボキャブラリー（単体の場合）、語彙数と揺れている状態です。	語彙	ボキャブラリー		
トークナイザ		トークナイザ		Tokenizer	
埋め込み		埋め込み		Embedding	
