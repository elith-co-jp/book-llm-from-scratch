# 第2章 Transformer

この章では、大規模言語モデルの核となるモデルである Transformer を構成する要素について 1 つずつ説明し、その後に説明した要素を組み合わせることで Transformer を作成します。2.1 節では Transformer 以前のモデル比べて、Transformer は何が違うのかの大まかに説明します。2.2 節では Transformer の最も重要な部品である アテンション機構について丁寧に説明し、2.3 節でアテンション機構以外の構成要素を説明します。最後に 2.4 節で Transformer 自体を作り上げ、2.5 節では実データを用いて翻訳タスクを学習します。

## 2.1 RNN から Transformer まで

### 2.1.1 Recurrent Neural Network (RNN)

Transformer 以前のニューラルネットワークによる言語モデルで、特に文章から文章に変換する Seq2Seq と呼ばれるタスクでは、再帰結合型ニューラルネット (Recurrent Neural Network; RNN) を用いるのが主流でした。RNN とは、単語などの入力を順番に受け取り、メモリ (内部状態) を都度更新するようなモデルです。このようなモデルにとってのデータの見え方は、人間で言うと口頭のコミュニケーションに似ています。例えば日本語を英語に翻訳するタスクでは<図: 通訳 (短文)>のように「こんにちは！」と言われてそれを「Hello!」に通訳するイメージです。では <図: 通訳 (長文)> のような長文ではどうでしょうか? 口頭でこれを聞いて一度で通訳できる人は少ないと思います。

![<図: 通訳 (短文)>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/e8f1fd81-d40a-420c-b37f-b9508c29afd4/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_1.png)

<図: 通訳 (短文)>

![<図: 通訳 (長文)>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/7ade2e7e-4f76-43b9-a141-fbab4f65d5f3/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_2.png)

<図: 通訳 (長文)>

このように RNN では長い依存関係の処理が難しいという問題がありました。長短記憶 (Long Short Term Memory; LSTM) や ゲート付き再帰ユニット (Gated Recurrent Unit; GRU) といった派生系による工夫もされましたが、いずれの場合でも $n$ 語離れた単語同士は、$n$ ステップの入力を経てしか結びつきません (<図: RNN の入力> 参照)。

![<図: RNN の入力>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/f50db34b-7fed-4c52-9495-4a7bfd8c48f2/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_6.png)

<図: RNN の入力>

### 2.1.1 Attention-RNN

シンプルな RNN における、離れた単語間の入力が時間的にも離れてしまうという問題を解決する方法の1つが アテンション機構 (Attention Mechanism) です。アテンションとは注意という意味で、今の入力に対して、他の入力の注意すべき部分を取り出す役割があります。<図: Attention-RNN の入力> はAttention-RNN の概念図です。図に示したように、「名前」という入力を受け取ると、アテンション機構は入力中で「名前」と関連しそうなところを探します。その後、アテンション機構は重要な順に重み付けて足し合わせたような情報を RNN に教えます。これによって RNN は「名前」が吾輩(猫) の名前であることを理解しやすくなります。Attention-RNN にとってのデータの見え方は人間でいうと、本を読んでいるようなものです。ある単語を見て何を指しているか等わからないことがあれば、他の部分を確認して意味をとらえます。

![<図: Attention-RNN の入力>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/c6b3d178-2920-4c9e-93aa-cadcbfa2faa5/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_7.png)

<図: Attention-RNN の入力>

アテンションを用いることにはデメリットもあります。それは入力が長くなるほど、アテンション機構の計算量が大きくなるということです。アテンションは1つの入力に対して、全単語の注目度を計算します。つまり、文の長さが $n$ の場合は $n$ 個の単語の注目度を計算します。さらにそれを $n$ 回の入力分行うので、合計 $n^2$ の計算が必要になります。例えば英語の1段落は大体100~200単語なので、多くて4万回計算が必要ということです。さらに、合計で10段落あると400万回になります。このように計算量が非常に大きくなる問題は、研究者や開発者も認識しており、現在ではこれを減らす工夫もされています。

### 2.1.3 Transformer の登場

Attention-RNN では、アテンション機構というツールを使って RNN を強化しているという意味で、主役は RNN でした。 これに対して Transformer では、RNN 要素を無くすという大胆な変更を行いました。つまり、内部状態を保持するのをやめてアテンションを主役にしたのです。

このようにモデルとしては大幅な変更がありますが、解くタスクは変わらず文章から文章に変換する Seq2Seq です。本章を通して、説明には Seq2Seq タスクの1つである日英翻訳タスクを用います。

Transformer のおおまかな構造を [<図: Transformer を抽象化したモデル>](https://www.notion.so/2-1-Transformer-2bcdf358370e46858ffd199a675e912b?pvs=21) に示します。図のように Transformer には翻訳元の文章が ワードごとに分割されて入力されます。エンコーダは、このような独立したワード列を、周辺のワードとの関係性を考慮した特徴量の列に変換します。デコーダは変換された特徴量列を元に翻訳後の文章を生成します。

Transformer の詳細な構造を [<図: Transformer>](https://www.notion.so/2-1-Transformer-2bcdf358370e46858ffd199a675e912b?pvs=21) に示します。 図中の数字はどの節で説明するかを表しています。 最も重要なのは単語間の関係性を扱う Multi-Head Attention で、2.2 節で説明します。次に単語の位置情報を付与する位置エンコーディング (Positional Encoding)、各位置の特徴量に対する高度な変換を行うフィードフォワード層、学習の安定化等の役割を持つレイヤー正規化といった部品については2.3節で説明します。最後に 2.4 節で Encoder ブロック、Decoder ブロックを説明し、Transformer 全体を完成させます。Transformer を用いた予測の方法や学習方法についてはその後の 2.5 節で説明します。

![<図: Transformer を抽象化したモデル>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/39ba611b-f02c-40d8-bea9-65bab99fff7e/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_342x.png)

<図: Transformer を抽象化したモデル>

![<図: Transformer>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/4e177b3e-35b2-49fe-aa24-d2d93179c035/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_122x.png)

<図: Transformer>