# 2.2 アテンション機構

アテンション機構は、クエリと呼ばれる1つの入力について、他の入力がどれだけ注意するべきかを計算します。そして、注意度に応じて足し合わせた結果を出力とします。

[<図: 注意度>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)と[<図: アテンション出力>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)にその様子を示します。注意度を計算する際の他の入力のことをキーと呼び、クエリとキーのペアに対して実数値の注意度が計算されます。次に、計算された注意度と他の入力を掛け合わせ、その結果を合計したものがアテンションの出力になります。

<図: アテンション出力>にもあるように、アテンション出力の計算時は他の入力のことをバリューと呼びます。注意度の計算時とアテンション出力の計算時で他の入力をキーやバリューとして呼び分けるのは、キーとバリューに別の値を用いる場合もあるからです。ただし、本書では全体を通してキーとバリューは同じものと考えて問題ありません。

ここで、クエリやキー、バリューは単語になっているのにどのように掛け算や足し算を計算するか気になった読者もいるでしょう。図中ではわかりやすさのために単語をそのまま載せていますが、実際には計算機にわかるような数値 (ベクトル) として単語を扱います。このベクトルの作り方については 2.3 節で簡単に触れ、第3章でより詳細に説明します。

![<図: 注意度>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/d3de16fa-b6ab-4151-b069-2deeccab522a/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_1.png)

<図: 注意度>

![<図: アテンション出力>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/1d67b89b-6982-427c-8938-feaf6c4fd3e8/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_2.png)

<図: アテンション出力>

クエリとキーから注意度を計算する方法は大きく分けて2つあります。1つ目はニューラルネットワークを用いた方法で 加法注意 (Additive Attention) と呼ばれます。もう1つはベクトルの内積という計算を用いた方法で、内積注意 (Dot-Product Attention) と呼ばれます。以降ではまずこれらの2つについて説明し、現在主流となっている Dot-Product Attention を実装し、その後に Transformer で用いられる Multi-Head Attention と呼ばれるアテンションまでの解説、実装を行います。

## 2.2.1 加法注意

加法注意では[<図: 加法注意>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)のように注意度を計算するニューラルネットを用意します。このニューラルネットの入力はクエリベクトルとキーベクトルを結合したものです。例えば、クエリとキーのベクトルの次元がそれぞれ $128$ 次元であれば、結合した $256$ 次元のベクトルを入力し、1つの実数値である注意度を出力します。これを全てのクエリ・キーペアに対して行います。

![<図: 加法注意>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/0f0f337a-7a4f-4265-b6ba-a26e27eda668/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_6.png)

<図: 加法注意>

その後、[<図: アテンション出力>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) に示したように、注意度で重み付けてバリューを足し合わせます。加法注意では注意度の計算時にクエリとキーを結合して入力できれば良いので、各ベクトルの次元は違っていても大丈夫です。

## 2.2.2 内積注意

内積注意では、クエリベクトルとキーベクトルの内積を用いて注意度を計算します。内積はベクトル同士の演算で結果がスカラー (ここでは実数) になります。この演算はベクトルの長さが同じであれば、ベクトルが近いほど値が大きくなります。例えば[<図: ベクトル>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)では$\bm a$ と $\bm b$ は内積が大きく、$\bm a$ と $\bm c$ は内積が小さくなります。

![<図: ベクトル>  ](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/91e8e7a8-ee13-454c-92e9-4ff3237b6a1f/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_7.png)

<図: ベクトル>  

$$
⁍
$$

内積で計算される注意度を意味のあるものにするためには、クエリベクトルとキーベクトルがランダムなベクトルではダメで、クエリに対して注目されるべきキーベクトルが近い必要があります。このようなベクトルの作り方については後述の Multi-Head Attention や第3章で詳しく説明するトークン埋め込みが関係します。

ここでは意味のあるベクトルが与えられるものとして、内積注意の具体的な計算方法をみていきましょう。クエリベクトルを $\bm q=(q_1, q_2, \cdots, q_d)$、キーベクトルを$\bm k=(k_1, k_2, \cdots, k_d)$ とすると内積 $\braket{\bm q, \bm k}$ は次のように計算できます。

$$
\braket{\bm q, \bm k}=\sum_{i=1}^d q_i k_i
$$

ただし $d$ は各ベクトルの次元です。式からわかるように、$\bm q, \bm k$ の次元は同じである必要があります。

内積計算を 1 つのクエリに対して複数のキーがある場合について考えてみましょう。キーが合計で $n$ 個あるとして、それらをまとめた行列 $K=\begin{pmatrix}\bm k_1^\top, \bm k_2^\top, \cdots, \bm k_n^\top\end{pmatrix}$▲注▲ を考えます。出力は各キーの注意度で、式で表すと次のようになります。

- ▲注▲
    
    $\bm k_1^\top$ はベクトル $\bm k_1$ の転置を表しています。詳細については xxx を参照してください。
    

$$
\bm q K^\top = \begin{pmatrix} a^\prime_1, a^\prime_2, \cdots,  a^\prime_n\end{pmatrix}
$$

[<図: 複数キーに対する注意度の計算>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) はベクトルや行列の中身を表した図です。

![<図: 複数キーに対する注意度の計算>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/df9ad338-a30e-4c7c-b33d-db999656718f/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_82x.png)

<図: 複数キーに対する注意度の計算>

 $a^\prime_i$ は、クエリ $\bm q$ が入力された場合、$i$ 番目のキー・バリューにどれだけ注目するかを表します。ただし、このままでは以下の2つの問題があります。

1. 値がマイナスの場合がある
2. クエリによって注意度の合計値が異なる

そこで、アテンション機構では softmax 関数を用いて合計が1で値が正になるようにします。softmax 関数は数式で書くと次のようになります。

$$
\begin{aligned}&\mathrm{softmax}(a^\prime_1, a^\prime_2, \ldots, a^\prime_n)\\&=\left(\frac{\exp(a^\prime_1)}{\sum_{i=1}^n \exp(a^\prime_i)}, \frac{\exp(a^\prime_2)}{\sum_{i=1}^n \exp(a^\prime_i)}, \ldots, \frac{\exp(a^\prime_n)}{\sum_{i=1}^n \exp(a^\prime_i)}\right)
\end{aligned}
$$

この変換の様子を<図: softmax による変換>に示します。

![<図: softmax による変換>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/c1481ea8-3b8c-4061-a50d-8b885f06f768/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_152x.png)

<図: softmax による変換>

変換によって値の大小関係は変わっていないことがわかります。softmax による出力が注意度であり、 $i$ 番目の注意度を $a_i$ と表します。

$i$ 番目のバリューのベクトルを $\bm v_i$ として、注意度に応じた重み付けをして足し合わせたものがアテンション機構の出力 $\bm o$ になります。式で表すと、

$$
\bm o = a_1\bm v_1 + a_2\bm v_2 + \cdots + a_n \bm v_n
$$

です。キーと同様にバリューも並べて行列 $V$として表せば、<図: 最終出力の計算> のようにできます。

![<図: 最終出力の計算>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/83f2a721-a5ce-4d11-9abf-ccda3de6f6d7/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_162x.png)

<図: 最終出力の計算>

以上で説明したアテンションの計算全体を合わせると次式のようになります。

$$
\bm o = \mathrm{softmax}(\bm q K^\top)V
$$

ではこれを実装してみましょう。[<コード: softmax 関数>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) に $\mathrm{softmax}$ 関数の実装を示します。

```python
import numpy as np

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=-1, keepdims=True)
```

ここでは入力の最大値を引いた値を用いて計算しています。これは [<eq: softmax>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) の式で分子分母に $\exp(b)$ を掛けることに対応しており▲注▲、計算結果が変わらないことはわかると思います。一見無駄な計算に見えますが、指数関数は入力の値を非常に大きく変換し、$\exp(50)$ で $10^{21}$ を超えます。 そのためこのように実装しないとすぐにオーバーフローしてしまいます。

- ▲注▲
    
    以下のように計算できます。$b$ には何が入っても良いので、今回は $a^\prime_i$ の最大値を用います。
    
    $$
    \begin{aligned}
    \frac{\exp(a^\prime_1)}{\sum_{i=1}^n \exp(a^\prime_i)}=
    \frac{\exp(a^\prime_1)}{\sum_{i=1}^n \exp(a^\prime_i)}\frac{\exp(b)}{\exp(b)}=\frac{\exp(a^\prime_1+b)}{\sum_{i=1}^n \exp(a^\prime_i+b)}
    \end{aligned}
    $$
    

次にアテンション本体を実装します ([<コード: アテンション>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21))。

```python
def attention(q, K, V):
    attention_weights = softmax(np.matmul(q, K.T))
    return np.matmul(attention_weights, V), attention_weights
```

今回は可視化のために注意度も返していますが、それでも非常にシンプルなことが分かります。

では、正しく動くか確認してみましょう。キー・バリューとして用いるベクトルは、[<図: ベクトルとクエリ>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) に示す長さが $1$ で、$360\degree$を 10 分割した角度の 10 個のベクトルとします。クエリとしては、長さが $1$ で角度が $45\degree$ のベクトルを用います。

[<コード: アテンションの動作確認>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)に実装を示します。実行してみて `output` が `[0.31564538 0.31564537]` 、 `attention_weights.shape` が `(10,)` 、合計がほぼ $1$ になれば正しく動いています。

```python
import math

n = 10
vectors = []
# 360°をn等分した角度
theta = 2 * math.pi / n
for i in range(n):
    x = math.cos(theta * i)
    y = math.sin(theta * i)
    vectors.append([x, y])

vectors = np.array(vectors)
query = np.array([1 / math.sqrt(2), 1 / math.sqrt(2)])
output, attention_weights = attention(query, vectors, vectors)
print("出力ベクトル:", output)
print("注意度の形:", attention_weights.shape)
print("注意度の和:", attention_weights.sum())
```

重みはどうなっているでしょうか。2.2.2 節のノートブックにはこれを可視化するコードもついており、実行すると <図: 注意度> に示すようなプロットが表示されます。クエリと近いベクトルほど注意度も大きいことが分かります。

![<図: ベクトルとクエリ>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/cee5121a-35c2-4b8a-922e-bfb057293494/vectors_and_query.png)

<図: ベクトルとクエリ>

![<図: 注意度>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/8292119d-f7d5-4e85-8060-b978bd0722ab/attention_weights.png)

<図: 注意度>

最後に、1つだけ長いベクトルがある場合をみてみましょう。ノートブックでは $v_4$ の長さを 5 倍にしています。この時の注意度は <図: v4のみ長い場合の注意度>ののようになります。<図: 全て同じ長さの場合の注意度> に示した元の注意度と比べて $v_4$ の注意度が非常に大きくなっています。また、元のベクトル自体も大きいため、出力のベクトルは $v_4$ に引きづられたものになります。

![<図: 全て同じ長さの場合の注意度>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/0a790616-357d-4069-96c5-6178fb1e4da2/attention_weights_hist1.png)

<図: 全て同じ長さの場合の注意度>

![<図: v4のみ長い場合の注意度>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/2eb4b084-550a-4d3f-80c8-27d6396c3c86/attention_weights_hist2.png)

<図: v4のみ長い場合の注意度>

ここまでで、アテンション機構が思った通りに動作していることが確認できました。Transformer に限らず、アテンション機構を実際に用いる場合は複数のクエリについて同時に計算する場合が多いです。拡張方法は簡単で、[<図: 複数キーに対する注意度の計算>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) におけるクエリをキー・バリューと同様に並べて $Q$ として表し、$\bm q$ を置き換えるだけです。この場合、[<図: 複数クエリ・複数キーに対する注意度の計算>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) のように $QK^\top$ は各行が1つのクエリに対する注意度ベクトルである、$m$ 行 $n$ 列の行列になります。

![<図: 複数クエリ・複数キーに対する注意度の計算>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/6d40d7d8-aeef-4d1d-9640-577bf22425ad/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_142x.png)

<図: 複数クエリ・複数キーに対する注意度の計算>

softmax 関数をこれを見越して実装しているため、アテンションの実装自体は変更する必要がありません。クエリの数を3つに増やして動かしてみましょう ([<コード: クエリを増やした場合の動作>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21))。出力の形は `(3, 2)` 、注意度行列の形は `(3, 10)` (クエリの数, キーの数)、行ごとの和は全て 1 になっていれば OK です。

```python
queries = np.array([[1 / math.sqrt(2), 1 / math.sqrt(2)], [1, 0], [0, 1]])

output, attention_weights = attention(queries, vectors, vectors)
print("出力の形:", output.shape)
print("注意度行列の形:", attention_weights.shape)
print("注意度の行ごとの和:", attention_weights.sum(axis=1))
```

## 2.2.3 Scaled Dot-Product Attention

Scaled Dot-Product Attention は、名前の通り内積注意をスケーリングしたものです。具体的には、クエリとキーの内積をした後に、そのまま softmax 関数に入力するのではなく、$\sqrt{d}$ ($d$ はベクトルの次元) 割ってから入力します。これを式で表すと次のようになります。

$$
\mathrm{Attention}(Q, K, V)=\mathrm{softmax}\left(QK^\top/\sqrt{d}\right)V
$$

ここで、Scaled Dot-Product Attention を $\mathrm{Attention}$ として表しました。以降でも特に断らない限りスケールされたものを単に $\mathrm{Attention}$ と書きます。

なぜこのようなスケーリングが必要なのかは、$d$ が大きくなった場合を考えるとわかりやすいです。$d$ が大きくなると、 [<eq: 内積>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) の式からわかるように足される要素の数が増え、値が大きくなりやすいです。その結果 softmax の出力がほとんど 0 になり、注意度としての役目を果たせないのです。

その様子を現したのが <図: softmax の入力による違い> です。図の左側では、1つだけ出力が 1 に近いものがありますが、見やすさのため 0.10 までで区切っています。このように、大きい値が含まれてしまうと指数関数の性質からそこの値が強調されてしまい、合計を 1.0 にする都合で他の値がほとんど 0 付近になってしまいます。

![<図: softmax の入力による違い> ](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/dac31ccf-283c-41be-98b9-d8f7420ede2f/softmax_different_ranges.png)

<図: softmax の入力による違い> 

スケーリングの分母が $\sqrt d$  である理由はクエリやキーがランダムなベクトルとして、各要素が独立な標準正規分布に従うと考えると、内積の標準偏差が $\sqrt d$ になるからです。このこと自体は確率論の基礎的な内容で理解できますが、ここでは立ち入りません。代わりに、スケーリングした場合としない場合を実装してみて、出力をみてみましょう。

2通りの次元数でクエリベクトルとキーベクトルを準備するコードを [<コード: 異なる次元のクエリ・キーベクトル>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) に示します。

```python
dim1 = 1
dim2 = 100
n_keys = 20

q1 = np.random.randn(dim1)
q2 = np.random.randn(dim2)
k1 = np.random.randn(n_keys, dim1)
k2 = np.random.randn(n_keys, dim2)
```

サンプルコードでは 1 次元と 100 次元にしています。

用意したベクトルの内積を計算し、スケーリングがある場合とない場合で softmax 関数に入力します ([<コード: スケーリング有り・無し>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)) 。

```python
dot1 = np.matmul(k1, q1)
s1_1 = softmax(dot1)
s1_2 = softmax(dot1 / np.sqrt(dim1)) # s1_1 と同じ

dot2 = np.matmul(k2, q2)
s2_1 = softmax(dot2)
s2_2 = softmax(dot2 / np.sqrt(dim2))
```

これで 4 通りの注意度が計算できました。<コード: 4通りの注意度を可視化> を用いてこれらの結果を可視化した結果が <図: スケーリングがある場合とない場合の softmax 出力> です。

```python
fig, ax = plt.subplots(2, 2, figsize=(12, 12))

max_1_2 = max(np.max(s1_2), np.max(s2_2))
ax[0, 0].scatter(dot1, s1_1, s=5)
ax[0, 0].set_title(f"{dim1} 次元, スケーリングなし")
ax[0, 0].set_ylim(-0.01, max_1_2 + 0.1)
ax[0, 1].scatter(dot1, s1_2, s=5)
ax[0, 1].set_title(f"{dim1} 次元, スケーリングあり")
ax[0, 1].set_ylim(-0.01, max_1_2 + 0.1)
ax[1, 0].scatter(dot2, s2_1, s=5)
ax[1, 0].set_title(f"{dim2} 次元, スケーリングなし")
ax[1, 1].scatter(dot2, s2_2, s=5)
ax[1, 1].set_title(f"{dim2} 次元, スケーリングあり")
ax[1, 1].set_ylim(-0.01, max_1_2 + 0.1)

fig.savefig("softmax_scaling.png", dpi=144)
```

![<図: スケーリングがある場合とない場合の softmax 出力>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/15c36108-1db8-41fb-b231-d1a736a89c45/softmax_scaling.png)

<図: スケーリングがある場合とない場合の softmax 出力>

100次元の方はスケーリングがない場合は 0 に張り付いているのに対して、スケーリングした場合は差が見やすくなりました。もちろん、ベクトルをランダムな値で用意したので毎回このように綺麗な結果は得られませんし、入力ベクトルの要素も独立ではありませんが、実際このようにスケーリングすることで性能が向上します。

## 2.2.4 Multi-Head Attention

マルチヘッドアテンションは Transformer の核ともいえる部品で、複数のヘッドから構成されます。まずは[<図: マルチヘッドアテンションの全体像>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) (左)に示した1つのヘッドに注目してみます。

ヘッドではクエリ・キー・バリューのベクトルを別々に線形変換し、変換結果を Scaled Dot-Product Attention に入力しています。線形変換 (図中の Linear) するのは、同じベクトルでも、クエリ・キー・バリューのどこに現れるかで役割が異なるはずだからです。「吾輩は猫である。名前はまだない。」の例でいうと、「名前」に注目している (クエリに現れる) 場合は「吾輩」や「猫」と言う単語は重要になります。一方で「吾輩」に注目している場合は「名前」の話が後で出てくることは、さして重要ではありません。このような非対称性が線形変換を行う理由の1つです。この線形変換は学習対象で、異なる初期値から始めるためヘッドごとに異なる変換を学習します。

![<図: マルチヘッドアテンションの全体像> 左: 1つのヘッドの構造 右: 全体像 ](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/ec8640b0-2319-49d4-85ee-fba894851a08/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_25.png)

<図: マルチヘッドアテンションの全体像> 左: 1つのヘッドの構造 右: 全体像 

マルチヘッドアテンションでは [<図: マルチヘッドアテンションの全体像>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) (右) に示すように、このようなヘッドを複数計算し、全ての出力を結合します。そして結合された出力をさらに変換したものをマルチヘッドアテンションの出力としています。

なぜ複数のヘッドを計算するのでしょうか。これは単語と単語の関係性が1通りではないからです。例えば「名前」にとって、主題という意味では「吾輩」や「猫」が重要ですが、名前がどうなのかという観点では「ない」が重要です。このようにさまざまな観点から関係性を表すために、複数のヘッドで各々線形変換を計算し、アテンションに入力するのです。

最後の線形変換には以下の2つの役割があります。

1. **次元の調整**: 各ヘッドの出力を結合しても、元の入力ベクトルの次元にならない場合があります。そのため最後に次元を調整します。
2. **情報の統合**: 複数の観点からバラバラに計算された情報をまとめる役割を果たします。

ではこのモジュールを式で表してみましょう。

まず、$i$ 番目のヘッドに注目します。このヘッドのクエリ・キー・バリューに対する線形変換を行う行列をそれぞれ $W_i^Q, W_i^K, W_i^V$  ▲注▲とします。次元数はそれぞれ $d_\mathrm{model}\times d_q, d_\mathrm{model}\times d_k, d_\mathrm{model}\times d_v$ です。ただし $d_\mathrm{model}$ はクエリ、キー、バリューの次元で $d_q, d_k, d_v$ は変換後の次元です。変換後の次元はハイパーパラメータで理論上はどのようにも取れますが、実際はヘッドの数を $h$ 個として $d_\mathrm{model} / h$ とする場合が多いです▲注▲。

- ▲注▲
    
    肩にQ, K, V とついていますがこれは指数ではなく添え字です。それぞれ Query、 Key、 Value の頭文字をとっています。
    
- ▲注▲
    
    $d_\mathrm{model}/h$ が整数になるように $d_\mathrm{model}$ と $h$ を設定します。
    

これらの行列で変換した結果を Scaled Dot-Product Attention に入力するので、出力 $O^\prime_i$ は次式のように表せます。

$$
O_i^\prime = \mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

次に$h$ 個のヘッドの出力を結合(concatenate)し 、最後の線形変換 $W^O$ をかけます。

$$
O=\mathrm{concat}(O_1^\prime, O_2^\prime, \ldots, O_h^\prime)W^O 
$$

ここで結合というのは同じクエリに対する異なるヘッドの出力を並べるという意味です。つまり、クエリ1に対してヘッド1, ヘッド2の出力が $o_1^\prime=(1, 2, 3)$ と$o_2^\prime=(4, 5, 6)$ であれば

$$
\mathrm{concat}(o_1^\prime, o_2^\prime)=(1, 2, 3, 4, 5, 6)
$$

となります。これを複数のクエリについてまとめて表記したのが[<eq: multi-head output>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21)です。

最後に以上の内容を実装していきます。Scaled Dot-Product Attention までは主に NumPy を用いて実装していました。Transformer の実装や学習では PyTorch を用いるため、これまでの内容を PyTorch の形で作り直し、それらを用いて Multi-Head Attention を実装します。

内積注意の実装は [<コード: PyTorchを用いた内積注意の実装>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) のようになります。

```python
import torch
from torch import Tensor, nn

class DotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:
        """内積注意の計算を行う.

        Args:
            query (Tensor): クエリ.shapeは(batch_size, query_len, d_model).
            key (Tensor): キー.shapeは(batch_size, key_len, d_model).
            value (Tensor): バリュー.shapeは(batch_size, value_len, d_model).
        """
        # 1. query と key から, (batch_size, query_len, key_len)のスコアを計算
        score = torch.bmm(query, key.transpose(1, 2))
        # 2. 重みの和が1になるようにsoftmaxを計算
        weight = torch.softmax(score, dim=-1)
        # 3. value の重み付き和を計算
        output = torch.bmm(weight, value)
        return output
```

PyTorch ではモデルの部品を `torch.nn.Module` を継承したクラスとして定義します。学習可能なパラメータがある場合は `__init__` 関数内で定義し、そのパラメータを用いた推論時の計算を `forward` 関数に記述します。バッチや Tensor がわからない場合やその他 PyTorch に関する詳細は付録を参照してください。

`torch.bmm` は batch matrix multiplication の略で、行列積をバッチように拡張したもので、バッチの次元を無視して単に行列積を行うと考えて大丈夫です。キーのバッチ数を除いた2つの軸を転置したものとクエリの行列積で計算したスコアを先述の通り softmax に入力して注意度を計算します。PyTorch には softmax 関数が用意されているため、自分で作る必要はありません。最後に注意度とバリューをかけて最終出力を得ます。

Scaled Dot-Product Attention の実装は [<コード: PyTorchを用いた Scaled Dot-Product Attention の実装>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) のようになります。本書のリポジトリにあるコードでは `mask` という引数を用いていますが、それについては2.5 節で触れるためここでは無視してください。

```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(
        self, query: Tensor, key: Tensor, value: Tensor
    ) -> Tensor:
        """スケール内積注意の計算を行う.

        Args:
            query (Tensor): クエリ.shapeは(batch_size, query_len, d_model).
            key (Tensor): キー.shapeは(batch_size, key_len, d_model).
            value (Tensor): バリュー.shapeは(batch_size, value_len, d_model).
        """
        # query の次元 (= キーの次元) でスケーリング
        d_k = query.size(-1)
        score = torch.bmm(query, key.transpose(1, 2)) / (d_k**0.5)
				# 注意度の計算
        weight = torch.softmax(score, dim=-1)
        # アテンション出力の計算
        output = torch.bmm(weight, value)

        return output
```

計算自体は内積注意とほぼ同じで、softmax 関数に入力する前のスコアを $\sqrt{d_k}$ でスケーリングしているだけです。

最後に Multi-Head Attention を実装を [<コード: Multi-Head Attention の実装>](https://www.notion.so/2-2-dc36230e7e294bf2b9b8d2f7f224e57a?pvs=21) に示します。

```python
class AttentionHead(nn.Module):
    def __init__(self, d_k: int, d_v: int, d_model: int) -> None:
        """MultiHeadAttentionのヘッド.

        Args:
            d_k (int): クエリ, キーの次元数
            d_v (int): バリューの次元数
            d_model (int): モデルの埋め込み次元数
        """
        super().__init__()
        # クエリ, キー, バリューを部分空間に埋め込むための全結合層
        self.linear_q = nn.Linear(d_model, d_k)
        self.linear_k = nn.Linear(d_model, d_k)
        self.linear_v = nn.Linear(d_model, d_v)

        self.attention = ScaledDotProductAttention()

    def forward(
        self, query: Tensor, key: Tensor, value: Tensor
    ) -> Tensor:
        """単一ヘッドのアテンションを計算する.

        Args:
            query (Tensor): クエリ.shapeは(batch_size, query_len, d_model).
            key (Tensor): キー.shapeは(batch_size, key_len, d_model).
            value (Tensor): バリュー.shapeは(batch_size, value_len, d_model).

        Returns:
            Tensor: 出力.shapeは(batch_size, query_len, d_v).
        """
        query = self.linear_q(query)
        key = self.linear_k(key)
        value = self.linear_v(value)

        output = self.attention(query, key, value)
        return output

class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads: int, d_k: int, d_v: int, d_model: int) -> None:
        """マルチヘッドアテンション.

        Args:
            n_heads (int): ヘッド数
            d_k (int): クエリ, キーの次元数
            d_v (int): バリューの次元数
            d_model (int): モデルの埋め込み次元数
        """
        super().__init__()
        self.heads = nn.ModuleList(
            [AttentionHead(d_k, d_v, d_model) for _ in range(n_heads)]
        )
        # 出力を変換する全結合層
        self.linear_o = nn.Linear(n_heads * d_model, d_model)

    def forward(
        self, query: Tensor, key: Tensor, value: Tensor
    ) -> Tensor:
        """マルチヘッドアテンションを計算する.

        Args:
            query (Tensor): クエリ.shapeは(batch_size, query_len, d_model).
            key (Tensor): キー.shapeは(batch_size, key_len, d_model).
            value (Tensor): バリュー.shapeは(batch_size, value_len, d_model).

        Returns:
            Tensor: 出力.shapeは(batch_size, query_len, d_model).
        """
        # ヘッドごとにアテンションを計算
        head_out = [head(query, key, value) for head in self.heads]
        # ヘッドを結合
        head_out = torch.cat(head_out, dim=-1)
        # 出力を変換
        output = self.linear_o(head_out)
        return output
```

本書ではヘッドを AttentionHead クラスとして実装し、それを用いて MultiHeadAttention クラスを作成しています。AttentionHead にはクエリ・キー・バリューの線形変換という学習されるパラメータがあるため、これらを `__init__` 関数内で定義しています。今回は全てのベクトルを `d_model` 次元に変換するような行列として定義しました。また、Scaled Dot-Product Attention クラスのインスタンスもここで作成します。`forward` 関数では、線形変換後とアテンションの計算を行います。

MultiHeadAttention クラスでは `n_heads` 個のヘッドと最終出力用の線形変換を定義します。 `forward` の計算では数式で示した通り、各ヘッドの出力を結合したものに線形変換をかけて出力します。

最後に MultiHeadAttention クラスが正しく実装できたか確認します。学習するパラメータがあるため正確な出力結果は予測できませんが、出力のサイズは事前に計算できるため、確かめてみましょう。各ベクトルの次元は 16, ヘッド数は 4, バッチサイズは 2 とします。またクエリ・キー・バリューはそれぞれ 3, 5, 5 個とします。

```python
batch_size = 2
d_model = 16
n_heads = 4
d_k = d_model // n_heads
d_v = d_k
query_len, key_len, value_len = 3, 4, 4
multihead_attention = MultiHeadAttention(n_heads, d_k, d_v, d_model)

# 1. query と key から, (batch_size, query_len, key_len)のスコアを計算
query = torch.randn(batch_size, query_len, d_model)
key = torch.randn(batch_size, key_len, d_model)
value = torch.randn(batch_size, value_len, d_model)

output2 = multihead_attention(query, key, value)

print(output2.shape)  # torch.Size([batch_size, query_len, dim])
```

`torch.Size([2, 3, 16])` と出力されていれば正しく実装できていそうです。より詳細な確認は Transformer の学習で行います。