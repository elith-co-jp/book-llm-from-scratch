# 2.3 アテンション以外の部品

## 2.3.1 トークン埋め込み

アテンション機構におけるクエリ・キー・バリューはベクトルになっていました。このベクトルはどのように与えられるのでしょうか。

まず、文章を単語に分ける必要があります。英語の場合はスペース区切りでわかりやすいものの、日本語の場合はこの時点で難しいタスクになります。また、現在よく用いられる区切り方はいわゆる単語ではなく、サブワードと呼ばれる単位になす。このような文章を構成する基本的な単位を言語モデルの文脈ではトークンと呼びます。文章をトークンレベルに分割する方法については第3章で詳しく説明します。

トークン化ができていると、トークンごとに ID を割り振ることができます。この振り方は一度決めたらトークン辞書として固定します。この辞書を用いて <図: トークンの変換> のように文章を数字の列に変換します。

![<図: トークンの変換>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/ab420004-1e80-48b6-a74f-8d732889fa7c/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_1.png)

<図: トークンの変換>

ここで割り振られる数字には特に意味はありません。トークン埋め込みでは、この数字をベクトルに変換します。単にベクトルに変換する場合、機械学習でよく用いられるのは One-Hot ベクトルです。これは <図: One-Hot ベクトル> に示すようなベクトルで、ID が $i$ であれば $i$ 番目のみが 1, 他が 0 になります。

![<図: One-Hot ベクトル>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/82e01d36-2bf5-4cad-aa20-ee0b64e2cb49/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_32x.png)

<図: One-Hot ベクトル>

しかし、このようなベクトルでは単語の意味が捉えられません。例えば内積注意の計算をしても、同じ単語同士以外は全て 0 になってしまいます。

そこで用いるのが埋め込みベクトルです。このベクトルではOne-Hot ベクトルではトークンの種類数分の次元が必要でしたが、それよりも大幅に低い次元に「埋め込み」ます。この埋め込み自体もさまざまな研究がありますが、 Transformer ではランダムな低次元ベクトルから始めて、学習することで単語の意味を反映したベクトルを獲得します。

埋め込みベクトルは <図: 埋め込みベクトル> に示したようにトークン ID ごとに表のような形で保持されています。入力時に「2, 1, 3, 12, 9, 5」のようにトークン列が与えられると6個のベクトルがこの表から取り出されます。

![<図: 埋め込みベクトル>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/1bba29e7-c83b-4163-ac78-d1627e5cfbbc/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_22x.png)

<図: 埋め込みベクトル>

この表は PyTorch の `nn.Embedding` というクラスで作ることができ、このクラスはトークンの種類数 (語彙数) と埋め込みベクトルの次元を与えて初期化します。トークン埋め込みを利用するスクリプトを [<コード: トークン埋め込みの確認>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示します。

```python
import torch
from torch import nn

vocabulary_size = 100
d_model = 512

embedding = nn.Embedding(vocabulary_size, d_model)
tokens = torch.Tensor([1, 2, 3]).long()

print(embedding(tokens).shape)
```

この例では埋め込み次元は 512 で 1、2、3 番目の単語の埋め込みを取得しています。そのため出力結果は `torch.Size([3, 512])` になります。

## 2.3.2 位置埋め込み

2.2.2 節のトークン埋め込みによって、適切に学習できればトークンの意味をとらえたベクトルが獲得できます。文章にはこのようなトークンの意味に加えて、トークン同士の位置関係も重要になります。例えば「井上さんは犬を、大森さんは猫を飼っています。」という文章で、順番の情報がないと誰が犬を飼ってるの分からなくなります。

しかし 2.2 節で説明したアテンション機構では、トークンの位置を扱うような仕組みはありませんでした。そのため、このままでは順序のない意味の集合としてか文章を扱うことができません。位置エンコーディングはこのような問題を解決するための方法です。

では、位置の情報を入れ込むにはどうすれば良いでしょうか。まず考えられるのは $i$ 番目なら $i$ という値を位置の情報にしてしまうことですが、これは文が長くなるほど際限なく値が大きくなるというデメリットがあります。では、長さ $N$ の文章に対して $i$ 番目を $i/N$ のようにエンコードするのはどうでしょうか。この方法では最大が $1$ になるため、先述の問題は解決するものの、文章の長さが事前にわかっていなければいけません。これらの両方の問題を解決する方法として Transformer の位置エンコーディングでは $0$ から $1$ の値を繰り返す三角関数を用います。ただし、1つの三角関数では複数の位置で同じ値になってしまうため、複数の周期を持つ三角関数を用います。

位置エンコーディングは最終的にトークン埋め込みと足し合わせるため、次元数はトークン埋め込みと同じにします。この次元数を $D$ とすると、位置 $i$ の位置エンコーディングの $d$ 次元目は次のようになります。

$$
\mathrm{PE}(i, d)=\begin{cases}
\sin\left(i/10000^{d/D}\right)&(dが偶数)\\
\cos\left(i/10000^{(d-1)/D}\right)&(dが奇数)
\end{cases}
$$

分母の $10000$ はハイパーパラメータです。また、 $d$ の偶奇によって $\sin$, $\cos$ を分けること意味については後述します。

上記の関数を可視化すると [<図: 位置エンコーディング>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) のようになります。

![<図: 位置エンコーディング>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/cfc98de6-3259-4980-aba1-867c2ebb6d77/sinusoidal_position_encoding.png)

<図: 位置エンコーディング>

式の通り、次元が高い部分ほど周期が長く、位置によるの変化が緩やかなことがわかります。以降での説明のため、$i$ 番目の位置エンコーディングベクトルを $\mathrm{PE}_i$ と表します。つまり、上記の関数を用いて次のように定義します。

$$
\mathrm{PE}_i=(\mathrm{PE}(i, 0), \mathrm{PE}(i, 1), \ldots, \mathrm{PE}(i, D-1))
$$

次に、異なる位置のベクトル $\mathrm{PE}_i$ と $\mathrm{PE}_j$ の内積を計算してみましょう。

$$
\begin{aligned}
\braket{\mathrm{PE}_i \mathrm{PE}_j}&=\sum_{d=0}^{D-1} \mathrm{PE}(i, d)\mathrm{PE}(j, d)\\
&=\sum_{d=0}^{D/2-1}\left[\sin\left(\frac{i}{10000^{2d/D}}\right)\sin\left(\frac{j}{10000^{2d/D}}\right)+\cos\left(\frac{i}{10000^{2d/D}}\right)\cos\left(\frac{j}{10000^{2d/D}}\right)\right]\\
&=\sum_{d=0}^{D/2-1}\cos\left(\frac{i-j}{10000^{2d/D}}\right)
\end{aligned}
$$

このように、$\sin$ と $\cos$ を用いた位置エンコーディングを用いると内積が相対位置関係 $i-j$ のみで決定します。この内積を横軸を $i$、縦軸を $j$ にとってプロットしたものが [<図: 位置エンコーディングの内積>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) です。近い位置ほど値が大きく遠い位置ほど値が小さい傾向にあることが分かります。

![<図: 位置エンコーディングの内積>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/2343f752-8f36-4453-884b-2ecff7c1fc71/sinusoidal_position_encoding_dotprod.png)

<図: 位置エンコーディングの内積>

ただし上述の結果は位置エンコーディングだけで考えた場合であることに注意してください。

[<コード: 位置エンコーディングの実装>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に実装を確認します。ここで関数名に sinusoidal とついているのは、今回説明した位置エンコーディングが Sinusoidal Position Encoding と呼ばれるためです。

```python
def sinusoidal_position_encoding(d_model: int, sequence_length: int) -> Tensor:
    pe = torch.zeros(sequence_length, d_model)
    dim_even = torch.arange(0, d_model, 2)
    dim_odd = torch.arange(1, d_model, 2)
    # unsqueeze で (sequence_length, 1) の形にする
    pos = torch.arange(0, sequence_length).unsqueeze(1)

    pe[:, dim_even] = torch.sin(pos / (10000 ** (dim_even / d_model)))
    pe[:, dim_odd] = torch.cos(pos / (10000 ** ((dim_odd - 1) / d_model)))

    # unsqueeze(0) でミニバッチの次元を先頭に追加
    return pe.unsqueeze(0)
```

上記のスクリプトで位置エンコーディングが正しく実装できていることを確認するために、導出した内積の理論値と実測値を比較してみましょう。[<コード: 位置エンコーディングの実装の確認>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に比較結果を可視化するスクリプトを示します。

```python
import numpy as np
import matplotlib.pyplot as plt

from llm_from_scratch.transformer.utils import sinusoidal_position_encoding

# 位置エンコーディングの準備
d_model = 128
sequence_length = 100
pe = sinusoidal_position_encoding(d_model=d_model, sequence_length=sequence_length)[0]

# 位置エンコーディングの内積を計算
dotprod = torch.matmul(pe, pe.transpose(0, 1))

# 位置エンコーディングのない積の理論値を計算
dots = np.zeros(sequence_length)
for pos in range(sequence_length):
    total = 0
    for d in range(d_model // 2):
        total += np.cos(pos / (10000 ** (2 * d / d_model)))
    dots[pos] = total
    
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].plot(dots)
ax[1].plot(dotprod[0])
ax[0].set_xlabel("位置の差")
ax[1].set_xlabel("位置の差")
ax[0].set_ylabel("内積")
ax[1].set_ylabel("内積")
ax[0].set_title("理論値")
ax[1].set_title("実測値")
plt.show()
```

正しく実装できていれば [<図: 位置エンコーディングの内積の比較>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) のように理論値と実測値が一致します。

![<図: 位置エンコーディングの内積の比較>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/04a54853-deb0-452a-abde-d49c756b17ff/compare_pe_dotproduct.png)

<図: 位置エンコーディングの内積の比較>

## 2.3.3 Feed Forward

Transformer には、アテンションの後に全結合層 (Feed Forward Network; FFN) がついています。アテンション機構はクエリとキー、バリューを用いて入力全体の関係性を考慮した表現を獲得するのに対して、全結合層は各位置の表現を独立に変換します ([<図: Transformer の全結合層>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21))。

![<図: Transformer の全結合層>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/3b748ec8-5366-48bc-b148-3751b1177d22/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_82x.png)

<図: Transformer の全結合層>

Transformer の全結合層は2層のネットワークで構成されます。位置 $i$ の特徴量ベクトルを $\bm x_i$ とすると、出力は次のように計算できます。

$$
\mathrm{FFN}(\bm x_i)=\mathrm{ReLU}(\bm x_i W_1+\bm b_1)W_2+\bm b_2
$$

ここで $W_1$、$W_2$、$\bm b_1$、$\bm b_2$ は学習可能なパラメータです。また、$\mathrm{ReLU}$ は活性化関数で次式のように表せます。

$$
\mathrm{ReLU}(x)=\max(0, x)
$$

ただし、全結合層で用いているものはベクトルを入力としていて、ベクトルの各要素に対して上記の計算をしています。

以上の計算を実装したものを [<コード: 全結合層の実装>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示します。

```bash
d_model = 512
d_ff = d_model * 4

feed_forward = nn.Sequential(
    nn.Linear(d_model, d_ff),
    nn.ReLU(),
    nn.Linear(d_ff, d_model),
)

# d_model 次元のベクトル, 長さ 10 の入力を用意
x = torch.randn(1, 10, d_model)

print("input shape:", x.shape)
print("output shape:", feed_forward(x).shape)
```

正しく実装できていれば、入力と出力の `shape` が等しくなります。

## 2.3.4 Skip-Connection

スキップ接続は [<図: スキップ接続>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) のように、ブロックを迂回するような経路を作るような構造です。

![<図: スキップ接続>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/be63a7e8-02f9-456e-bdb1-d992529d903b/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_22x.png)

<図: スキップ接続>

ここでは簡単のため1次元で考えてみましょう。入力を $x$、出力を $y$ として、間に挟まるブロック (<図: スキップ接続> の Multi-Head Attention や Feed Forward) を関数 $f$ とすると、以下のように表現できます。

$$
y = x+f(x)
$$

この構造には以下のようなメリットがあります。

1. 誤差逆伝播法によって学習する際に、直接つながる経路ができることで学習しやすくなる
2. 恒等写像 (値を変えない関数) を表現しやすい

1 については $y$ を $x$ で微分するとわかりやすいです:

$$
y^\prime=1 + f^\prime(x)
$$

後ろの層の誤差が $\delta$ とした場合に、Skip-Connection のレイヤーを挟んで入力側に伝わる誤差は $\delta\cdot(1+f^\prime(x))$となります。そのため、$f^\prime(x)$ が $0$ に近い場合でも、誤差情報が途切れず伝わります。

2 については、$f(x)=0$ のように学習すれば良いだけです。

前節の FNN を間に挟む場合での実装を [<コード: FNN と Skip-Connection の実装>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示します。

```python
import torch
from torch import nn

class FNN(nn.Module):
    def __init__(self, d_model, d_ff):
        super(FNN, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        return self.linear2(torch.relu(self.linear1(x)))

class SkipConnection(nn.Module):
    def __init__(self, d_model, d_ff):
        super(SkipConnection, self).__init__()
        self.sublayer = FNN(d_model, d_ff)

    def forward(self, x):
        return x + self.sublayer(x)
```

これらのモデルを、入力と同じ出力が得られるように学習します。入力の次元を 10、中間層の次元を 128 とした場合のデータとモデルの定義を [<コード: データとモデルの準備>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示します。

```python
N = 10000
data = torch.randn(N, 10)

model1 = FNN(10, 128)
model2 = SkipConnection(10, 128)
```

学習に関しては Transformer の学習時に詳細に説明します。<図: Skip-Connection がある場合とない場合の損失の比較> は横軸に学習のステップ、縦軸にデータとモデル出力の平均二乗誤差をプロットしたものです。Skip-Connection がある方が早く誤差が小さくなっていることがわかります。

![<図: Skip-Connection がある場合とない場合の損失の比較>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/5335d101-ad78-4da9-9c20-c814e989496a/skip_connection_loss.png)

<図: Skip-Connection がある場合とない場合の損失の比較>

## 2.3.5 Layer Normalization

レイヤー正規化は学習の安定化・効率化のためのブロックです。

このブロックではニューラルネットワークのレイヤー (層) ごとに出力値の平均を 0 、分散を 1 します。このような正規化のバリエーションとしてはバッチ正規化やインスタンス正規化などがあります。これらの正規化は元々、レイヤーごとに入力される値のずれ (内部共変量シフト) を抑えることで学習しやすくするために提案されました。

内部共変量シフトとは、[<図: 内部共変量シフトのイメージ>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示すように、レイヤーごとに入力値の分布が変わってしまうことを指します。

![<図: 内部共変量シフトのイメージ>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/ff0aeb3b-c052-4a7f-9699-694e777d845d/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_62x.png)

<図: 内部共変量シフトのイメージ>

本来の目的とは別に、レイヤー正規化の効果としては以下の3つが挙げられます。

- ニューラルネットの非線形性を活かす
- 学習を効率化する
- モデルの汎化性能を高める

レイヤー正規化は、入力の位置ごとに適用されます。つまり、文章の長さが $N$ であれば、1文に対しては $N$ 個の特徴量ベクトルに対して独立に計算します ([<図: レイヤー正規化の適用方法>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21))。

![<図: レイヤー正規化の適用方法>¬](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/f635787d-2748-4c40-9223-2553c0f2fc31/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_72x.png)

<図: レイヤー正規化の適用方法>¬

1つの特徴量ベクトルを $\boldsymbol x = (x_1, x_2, \ldots, x_D)$ とすると、特徴量を以下のように正規化します。

$$
\tilde x_k = \frac{x_k-\mu}{\sigma^2}
$$

ここで、$\mu$ と $\sigma^2$ は以下のように計算します。

$$
\mu=\frac{1}{D}\sum_{i=1}^d x_i, \sigma^2 = \frac{1}{D}\sum_{i=1}^d (x_i-\mu)^2
$$

実装を [<コード: レイヤー正規化の動作確認>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示します。ここでは、擬似的に埋め込みベクトルを作成して動作確認を行なっています。平均や分散を計算する際の `axis` に -1 を設定しているのは、特徴量の次元が一番後ろになっているためです。つまり、 `embedding_dim` の次元に沿って平均と分散を計算します。また、分散の計算時に `unbiased=False` を設定していることにも注意してください。これを設定しないと $\sigma^2$ の計算の分母が $D-1$ になり、不偏分散を計算してしまいます。最後の出力で、平均がほぼ 0、分散がほぼ 1 になっていれば正しく実装できています。

```python
import torch

batch, sentence_length, embedding_dim = 20, 5, 10

# 埋め込みベクトルを擬似的に生成
embedding = torch.randn(batch, sentence_length, embedding_dim)
mean = embedding.mean(axis=[-1], keepdim=True)
std = embedding.std(axis=[-1], keepdim=True, unbiased=False)

# 正規化
eps = 1e-8
normalized_embedding = (embedding - mean) / std.add(eps)

# 3番目の文の2番目の単語位置のベクトルを取得
vector = normalized_embedding[3, 2, :]
print(vector.mean(), vector.std(unbiased=False))
```

レイヤー正規化の主なアイディアは上述の通りですが、実際のレイヤー正規化には学習できるパラメータがあり、次式のようになります。

$$
\tilde x_k = \gamma_k\cdot\frac{x_k-\mu}{\sigma^2}+\beta_k
$$

$\gamma_k$、$\beta_k$ はそれぞれ $k$ 次元目の特徴量に対する倍率と平均のシフトになります。レイヤー正規化では次元ごとのこれらの値も最適化します。

上記のパラメータを考慮して、PyTorch のクラスとして実装したものを [<コード: レイヤー正規化の実装>](https://www.notion.so/2-3-a2cc6882a6ed43958550007d52c143b0?pvs=21) に示します。

```python
class LayerNorm(nn.Module):
    def __init__(self, d_model: int, eps=1e-6):
        """Layer 正規化.
        Args:
            d_model (int): 埋め込み次元数
            eps (float): ゼロ除算防止のための微小値
        """
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps

    def forward(self, x: Tensor) -> Tensor:
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True, unbiased=False)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
```

`nn.Parameter` はレイヤーの重みなど、学習できるパラメータを定義するためのクラスです。倍率を1、平均のシフトを 0 として初期化しています。