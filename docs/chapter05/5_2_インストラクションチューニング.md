### ver.2

# 5.2 インストラクションチューニング

## 5.2.1 インストラクションチューニングの基礎

5.1 でも述べた通り、インストラクションチューニング (指示チューニング) は事前学習によって次のトークンを予測できるようになったモデルが、指示通りにタスクを解けるようになるためのファインチューニング方法です。インストラクションチューニングは 2021年に Google Research が提案した手法で、提案論文中ではインストラクションチューニング後のモデルを **F**inetuned **La**nguage **N**et の一部をとって FLAN と呼んでいます。FLAN の直前に提案された GPT-3 ではインストラクションチューニングを行わず、タスクを解いているサンプルを入力に与える Few-Shot プロンプトによってタスクを解いていました。これに対して FLAN ではサンプルを与えない Zero-Shot 設定での能力が高かった点が画期的でした。

FLAN でのインストラクションチューニングでは、データセットは自然言語処理タスクに用いられていたデータセットから自動で作成したものでした。一方、2022 年に提案された OpenAI の InstructGPT では、指示・応答例を人手で約 15,000 組作成して学習しています。

インストラクションチューニングでは、モデルに指示部分として以下のようなテキストを与えます。

```
human: こんにちは
gpt: こんにちは！何かお手伝いしましょうか?
human: 3 + 5 を計算してください。
gpt:
```

これに対して応答例としては以下のようなテキストが与えられ、モデル出力と比較されます。

```
3 + 5 = 8 です。
```

データセットとしては、上述のように Human や AI を付与したテキストではなく、より扱いやすい JSON LINES (jsonl) 形式で公開されている場合が多いです。

例えば、LLM-jp が公開している oasst1-21k-ja データセット ▲注▲ では、各レコードが以下のような `from` と `value` をキーに持つ辞書のリストになっています。

| from | value |
| --- | --- |
| human | 冷たいシャワーを毎日浴びることの長所と短所について、短いパラグラフを書いてください。 |
| gpt | 冷たいシャワーを毎日浴びることのメリットは、注意力を高め、気分を高揚させること、健康全般を改善すること、ガスや電気を使ってお湯を沸かさないので光熱費の節約になることです。一方、冷たいシャワーはあまり歓迎されるものではありません。特に寒い冬には、冷たいシャワーに浸かるという精神的な障壁を超えられず、失敗や失望を考えてしまうかもしれません。 |
- ▲注▲
    
    apache-2.0 ライセンス。このデータセットは OpenAssistants の oasst1 データセットを和訳したもの。
    

また、Databricks 社が公開している databricks-dolly-15k データセット ▲注▲ では指示・応答に加えて、応答に必要なコンテキストもデータとして保持しています。

| instruction | context | response | category |
| --- | --- | --- | --- |
| When did Virgin Australia start operating? | Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney | Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. | closed_qa |
- ▲注▲
    
    CC-BY-SA-3.0 ライセンス。
    

例として oasst-21k-ja データセットのような形でデータを持っていれば、[<データセットから指示・応答のテキストを作成する例>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) に示すコードでレコードを学習用の指示・応答文に変換することができます。

```python
data = [{ "from": "human", "value": "冷たいシャワーを毎日浴びることの長所と短所について、短いパラグラフを書いてください。" }, { "from": "gpt", "value": "冷たいシャワーを毎日浴びることのメリットは、注意力を高め、気分を高揚させること、健康全般を改善すること、ガスや電気を使ってお湯を沸かさないので光熱費の節約になることです。一方、冷たいシャワーはあまり歓迎されるものではありません。特に寒い冬には、冷たいシャワーに浸かるという精神的な障壁を超えられず、失敗や失望を考えてしまうかもしれません。" }]

prompt_template_example = "{from}: {value}"
instruction_text = ""
for d in data[:-1]:
    instruction_text += prompt_template_example.format(**d)
response_text = prompt_template_example.format(**data[-1])

print("指示部分:")
print(instruction_text)
print("\n応答部分:")
print(response_text)
```

インストラクションチューニングではこのような指示・応答文を用いてどのように学習するのでしょうか。学習の手法自体は、事前学習と同様で次のトークンを予測し、モデルの予測と実際のテキストを比較したクロスエントロピー誤差を用います。一点異なるのは、指示文に関しては損失を計算しない (学習に用いない) 点です▲注▲。このイメージを [<図: 応答文のみで学習する>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) に示します。

- ▲注▲
    
    用いない場合が多いですが、用いたからインストラクションチューニングではないというわけではありません。後述するライブラリの実装でも用いるか用いないかは指定できます。
    

![<図: 応答文のみで学習する>](attachment:1aeb332a-55dc-48aa-9199-bc34161a43c6:image.png)

<図: 応答文のみで学習する>

## 5.2.2 インストラクションチューニングの実装

本節では、databricks-dolly-15k データセットを用いてインストラクションチューニングを行います。

> **Note**: インストラクションチューニングの学習には GPU が必要です。GPU 環境がない場合は、学習のコードをスキップし、5.2.3 節で Hugging Face から学習済みモデルをダウンロードして推論を試すことができます。

まずはデータセットをロードしましょう。Hugging Face のページ ▲注▲ にデータセットがアップロードされているので、そちらから読み込んでも良いですが、ここでは `datasets` ライブラリを用いてデータを読み込みます ([<コード: datasets ライブラリを用いたデータのロード>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21))。

- ▲注▲
    
    https://huggingface.co/datasets/databricks/databricks-dolly-15k
    

```python
from datasets import load_dataset

ds = load_dataset("databricks/databricks-dolly-15k")
print(ds["train"]) 
# Dataset({
#     features: ['instruction', 'context', 'response', 'category'],
#     num_rows: 15011
# })
```

`print` の下のコメントは出力結果です。カラムとして instruction, context, response, category を持っており、15,011 件のデータが含まれることが確認できます。

今回は簡単のため、補足情報である context が空のサンプルのみ利用します。 `datasets` でロードされたデータは [<コード: コンテキストが空のデータのみを抽出>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) のように `filter` メソッドを用いてフィルタリングできます。

```python
ds_train = ds["train"].filter(lambda x: x["context"] == "")
print(f"コンテキスト空のデータ: {ds_train.num_rows}") # 10544
```

結果として 10,544 件のデータが残りました。次に、データから学習用のテキストを作成する関数を実装します。[<コード: 学習用テキストを作成する関数>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) では `ai:` や `gpt:` ではなく、 `### Question:` と `### Answer:` を用いています。

```python
prompt_template = """\
### Question: {instruction}
### Answer: {response}{eos_token}"""

def format_input(example):
    """バッチ処理用のフォーマット関数"""
    texts = []
    for instruction, response in zip(example['instruction'], example['response']):
        text = prompt_template.format(
            instruction=instruction,
            response=response,
            eos_token=tokenizer.eos_token
        )
        texts.append(text)
    return texts

sample = ds_train[0]
print("サンプルデータ:")
print(f"  instruction: {sample['instruction'][:50]}...")
print(f"  response: {sample['response'][:50]}...")

#  サンプルデータ:
#  instruction: Which is a species of fish? Tope or Rope...
#  response: Tope...
```

学習時は、このように作成されたテキストを用いますが、テキストが長いとその分、GPU のメモリも多く必要になります。そのため、今回は十分多くのデータを含むトークン数を上限としてデータセットをフィルタリングします。トークン数を計算するため [<コード: 学習済みモデルとトークナイザの読み込み>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) で先に学習済みモデルとトークナイザを読み込みます。

今回のインストラクションチューニングでは、OpenAI が開発した GPT-2 モデルを使用します▲注▲。GPT-2 は約1.24億パラメータを持つ比較的小規模なモデルで、限られた計算リソースでもインストラクションチューニングの効果を確認できます。

- ▲注▲

    Hugging Face にて公開されています (https://huggingface.co/gpt2)。Modified MIT License。


```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto" if torch.cuda.is_available() else "cpu",
)

print(f"パラメータ数: {sum(p.numel() for p in model.parameters()):,}") # パラメータ数: 124,439,808
```

次に [<コード: トークン数のプロット>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) を用いて、トークン数のヒストグラムをプロットします。

```python
import matplotlib.pyplot as plt
import japanize_matplotlib

fig, ax = plt.subplots()

lengths = [len(tokenizer.encode(text)) for text in format_input(ds_train)]
ax.hist(lengths, bins=200)
ax.set_xlim(0, 1000)
ax.set_xlabel("トークン数")
ax.set_ylabel("レコード数")
fig.savefig("./output/histogram.png", dpi=300, bbox_inches="tight")
```

プロットした結果を [<図: トークン数のヒストグラム>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) に示します。

![image.png](attachment:8db79b05-96ad-485d-a33b-01be17f1b4a2:image.png)

図からほとんどのレコードでトークン数が500以下であることが分かりますので、512 トークンを上限として、それよりトークン数が多いレコードは除外しましょう ([<コード: トークン数によるフィルタ>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21))。アウトプットから、512 トークンを超えるレコード数は 100件程度であったことが分かります。

```python
max_length = 512

def token_length_filter(x):
    text = prompt_template.format(
        instruction=x["instruction"],
        response=x["response"],
        eos_token=tokenizer.eos_token
    )
    return len(tokenizer.encode(text)) <= max_length

ds_train = ds_train.filter(token_length_filter)
print(f"トークン数フィルタ後: {ds_train.num_rows}") # トークン数フィルタ後: 10400
```

インストラクションチューニングを行う前に、学習前のモデルがどのような応答を行うのか確認してみましょう。推論用の関数を [<コード: 推論用の関数>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) に示します。この関数では、生成結果から `response_template` として定義した応答の始まりを示すテキストを探し、その後ろから文章の終わりを表す `eos_token` までを取り出すことで、応答部分のみを返しています。

```python
prompt_template_infer = """\
### Question: {instruction}
### Answer: """
response_template = "### Answer:"

@torch.inference_mode()
def inference(model, tokenizer, user_input):
    prompt = prompt_template_infer.format(instruction=user_input)
    device = next(model.parameters()).device
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    output = model.generate(
        input_ids,
        max_new_tokens=128,
        do_sample=False,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)
    response_start = generated_text.find(response_template) + len(response_template)
    response_end = generated_text.find(tokenizer.eos_token, response_start)
    if response_end == -1:
        response_end = len(generated_text)
    response = generated_text[response_start:response_end].strip()
    return response
```

では [<コード: チューニング前のモデルによる推論>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) で日本の首都について聞いてみましょう。

```python
test_questions = [
    "What is the capital of Japan?"
]

print("チューニング前の応答:")
print("="*80)
before_responses = {}
for question in test_questions:
    response = inference(model, tokenizer, question)
    before_responses[question] = response
    print(f"Q: {question}")
    print(f"A: {response}")
    print("-"*80)
```

アウトプットは次のようになります。日本については語っているものの、応答文にはなっておらず、続きを生成しただけのように見えます。

```
The Japanese government has a long history of using money to finance its own military. The first major example was in 1848 when it used $1,000 for an army unit and then spent about half that on other expenses such as uniforms and food supplies (see below). In fact, during World War II , this amount had been raised by more than 100% from what would have been needed if there were no war at all . This led to many people believing they could use their savings or even buy some goods with cash instead of having them go into debt because "the Government will not pay you back." However, since most countries do

```

インストラクションチューニングの設定を [<コード: インストラクションチューニングの設定>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) に示します。まず `DataCollatorForCompletionOnlyLM` は 5.2.1 で説明した応答部分のみの損失を計算するためのクラスです。応答部分を取り出すために、　`response_template` を渡す必要があります。学習時の詳細な設定は `SFTConfig` で定義します。最後に `SFTTrainer` に `collator` と `config` を渡すことで、学習用の準備は完了です。

```python
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM

collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

config = SFTConfig(
    output_dir='./output/sft_model',
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=100,
    max_seq_length=max_length,
    num_train_epochs=3,
    gradient_accumulation_steps=1,
    per_device_train_batch_size=8,
    lr_scheduler_type="constant",
)

trainer = SFTTrainer(
    model,
    args=config,
    train_dataset=ds_train,
    formatting_func=format_input,
    data_collator=collator,
)
```

以上の準備によってインストラクションチューニングは <コード: インストラクションチューニングの実行> のように `train` メソッドを呼び出すだけで実行できます。

> **注意**: GPUが必要です。GPUがない環境では学習をスキップし、「5.2.3 学習済みモデルによる推論」へ進んでください。

```python
save_path = "./output/sft_model"
trainer.train()
trainer.save_model(save_path)
```

学習したモデルを用いて、同じ質問について推論してみてください。アウトプットの例は以下のようになります。

```
Tokyo, Japan.  The capital in Japan is Tokyo and it's capital is Tokyo.  It was founded by Emperor Shigeru I in the year 1450 and is still maintained today as one major city in Asia with over 2 million residents.  It has a population of around 7 million people which is more than any other country in Europe or North America combined.  Its main trading partner is Toyota Motor Co Ltd (TMC) for its Japanese operations.  Tokyo also hosts many world famous movie studios like The Hunger Games, Star Wars, and The X-Files.  There are also many museums and historical sites that make it
```

回答は完璧とはいえないものの、質問されたことに適切に答えられていることが分かります。他にも様々な質問をしてみて、どのような応答を示すか確認してみてください。

## 5.2.3 学習済みモデルによる推論

GPU 環境がなく学習をスキップした場合や、学習済みモデルで推論のみを行いたい場合は、Hugging Face から学習済みモデルをダウンロードして利用できます。

[<コード: 学習済みモデルのロード>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) では、ローカルに学習済みの重みがあればそれを使用し、なければ Hugging Face からダウンロードします。

```python
# モデルパスの設定
local_model_path = "./output/sft_model"
hf_repo = "elith/llm-book-models"
hf_subfolder = "chapter05/sft_model"

# ローカルに重みがあればそれを使用、なければHFからダウンロード
if os.path.exists(os.path.join(local_model_path, "config.json")):
    print(f"ローカルのモデルを使用: {local_model_path}")
    sft_model = AutoModelForCausalLM.from_pretrained(local_model_path)
    sft_tokenizer = AutoTokenizer.from_pretrained(local_model_path)
else:
    print(f"Hugging Faceからモデルをダウンロード: {hf_repo}/{hf_subfolder}")
    sft_model = AutoModelForCausalLM.from_pretrained(hf_repo, subfolder=hf_subfolder)
    sft_tokenizer = AutoTokenizer.from_pretrained(hf_repo, subfolder=hf_subfolder)

sft_tokenizer.pad_token = sft_tokenizer.eos_token

# デバイス設定
device = "cuda" if torch.cuda.is_available() else "cpu"
sft_model = sft_model.to(device)
print(f"デバイス: {device}")
```

モデルをロードしたら、[<コード: SFTモデルによる推論>](https://www.notion.so/5-2-68cbf10e9b4a411688d7860861f93e46?pvs=21) で推論を実行できます。

```python
# SFTモデルによる推論
test_questions = [
    "What is the capital of Japan?"
]

print("="*80)
print("SFTモデルによる推論")
print("="*80)

for question in test_questions:
    response = inference(sft_model, sft_tokenizer, question)
    print(f"\nQ: {question}")
    print(f"A: {response}")
    print("-"*80)
```