    ## **5.3.1 人間のフィードバックによる学習とは**
    
    5.2 節で紹介したインストラクションチューニングは、応答例を教師データとして与え、それを再現するように学習していました。このような学習方法では、どのような出力をすべきかは学べる一方、どのような出力をしてはいけないのかを学ぶことができませんでした。一方、人間のフィードバックによる学習では、応答例に対して人間の付けたスコア (嗜好データ) ▲注▲ をもとに学習するため、好ましい出力、好ましくない出力を提示することができます。これにより、知らないことについては分からないと答えさせたり、暴力的な発言を抑制したりといったアラインメントが可能になります。
    
    - ▲注▲
        
        厳密には1つの入力に対する2通りの応答に関して、どちらの方が好ましいかがわかるデータが用いられます。数値でスコアがついているのも、このようなデータの 1 つで、スコアの値でどちらが好ましいかが分かるデータになっています。
        
    
    人間の嗜好に基づいた学習自体は 2017 年に提案された手法です。 2020 年に OpenAI の研究者が GPT-3 を用いた要約タスクに適用し、2022 年に一般的なタスクを行うようインストラクションチューニングの後に人間のフィードバックによる学習を適用したモデルを InstructGPT として発表しました。大規模言語モデルブームの先駆けとなった ChatGPT (GPT-3.5) は InstructGPT の兄弟的なモデルで、異なるのはモデルサイズとアラインメントに会話データを用いた点だけです。
    
    以降では数式も用いて解説しますが、できる限り日本語や図を用いた解説も行います。数式が苦手な方は、そちらだけでも確認してください。
    
    ### **人間のフィードバックによるアライメント手法**
    
    LLMの出力を人間の好みに適合（アライメント）させるための主要なアプローチとして、以下の2つが挙げられます。
    
    - **PPO(Proximal Policy Optimization)を用いたRLHF（Reinforcement Learning from Human Feedback）**: 報酬モデル（Reward Model）を別途学習し、その評価値を最大化するようにLLMを強化学習させる手法です。代表的なアルゴリズムとしてPPOが用いられ、InstructGPTや初期のChatGPTで採用された実績があります。
    - **DPO（Direct Preference Optimization）**: 2023年にRafailovらによって提案された手法です。報酬モデルを明示的に構築せず、人間の選好データ（Preference Data）から直接LLMを最適化します。強化学習の計算プロセスを経ないため、学習が安定しやすく、計算コストも低いのが特徴です。
    
    本節では、まず RLHF/PPO の仕組みを理解した上で、より新しい手法である DPO を実装して学びます。
    
    ## **5.3.2 RLHF と PPO**
    
    PPOを用いた RLHF では、以下の 3 つを行います。
    
    - 指示文と複数の応答例に関して、どの応答が良いのか人間がスコアをつける ([<図: 複数の応答例に関して、人間がスコアをつける>](https://www.notion.so/27f9e9ccb2338008a82be432edb4bed8?pvs=21))
    - 人間の付けたスコアに基づいて、生成された文章を評価する報酬モデル (Reward Model; RM) の学習 ([<図: 指示文と応答例を入力として、応答の良し悪しを判定する報酬モデルを学習>](https://www.notion.so/27f9e9ccb2338008a82be432edb4bed8?pvs=21))
    - 学習された報酬モデルを最大化するように LLM を学習 ([<図: LLM が生成した応答を報酬モデルで評価した結果を用いて、LLM を学習>](https://www.notion.so/27f9e9ccb2338008a82be432edb4bed8?pvs=21))
    
    ![<図: 複数の応答例に関して、人間がスコアをつける>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/1cbd5def-4bab-4c6e-b422-6f71bd588bf4/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_132x.png)
    
    <図: 複数の応答例に関して、人間がスコアをつける>
    
    ![<図: 指示文と応答例を入力として、応答の良し悪しを判定する報酬モデルを学習>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/59d33256-6510-401d-8950-ac5c5c9a9f5e/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_142x.png)
    
    <図: 指示文と応答例を入力として、応答の良し悪しを判定する報酬モデルを学習>
    
    ![<図: LLM が生成した応答を報酬モデルで評価した結果を用いて、LLM を学習>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/e8b376a1-333b-4c2e-bd6e-3a3f12e3325a/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_152x.png)
    
    <図: LLM が生成した応答を報酬モデルで評価した結果を用いて、LLM を学習>
    
    ### 報酬モデル
    
    報酬モデルのアーキテクチャを [<図: 報酬モデルのモデルアーキテクチャ>](https://www.notion.so/27f9e9ccb2338008a82be432edb4bed8?pvs=21) に示します。報酬モデルは LLM への入力となるコンテキスト $x$, LLM の出力にあたる文章 $y$ を入力として、スカラー (実数値) を出力するモデルで $r_\theta(x, y)$ と表します ($\theta$ はモデルのパラメータ)。報酬モデルのアーキテクチャとして InstructGPT の学習では、LLM と同じく GPT-3 が用いられました。ただし単語確率を予測する部分 (線形層と Softmax) を、最終トークンの内部表現から報酬値を計算する線形層に置き換えて用います。報酬モデル自体は必ず GPT 系のモデルでなければいけないわけではなく、BERT のようなエンコーダベースのモデルが利用される場合もあります。その場合は、報酬の値は最終トークンではなく最初のトークン ▲注▲ の出力から計算します。これらの 2 通り以外にも、全トークンの出力を平均した内部状態を線形層への入力とする場合もあります。
    
    - ▲注▲
        
        BERT では最初のトークンとして CLS トークンと呼ばれる特殊トークンを入力します。このトークンに対する出力は全体の情報を集約した情報を持ちます。
        
    
    ![<図: 報酬モデルのモデルアーキテクチャ>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/4d56158b-e48d-4231-a018-d1d2f2e9077d/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_62x.png)
    
    <図: 報酬モデルのモデルアーキテクチャ>
    
    報酬モデルの学習では次のような損失関数を最小化します。学習対象は、追加した線形層の重みと、GPT内の重みになります。
    
    ![アートボード 1@2x.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/2254a249-53e2-4079-9a8a-9be1e50e9acc/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_12x.png)
    
    数式中の $K$ は嗜好データにおいて1つの入力に対して何通りの応答例があるかを表しています。また、 $(x, y_w, y_l) \sim D$ はデータセット $D$ からサンプリングすることを表しています。
    
    報酬モデルは $K$ 通りの応答例から 2 つ ($y_w$ と $y_l$) を取り出して学習します。この時、どちらの出力が好ましいものであるかを判定するために人間のスコアが用いられます。そのため、好ましくない出力と書きましたが、これは完全に悪い出力というわけではなく、もう一方の出力と比べると劣るといった意味です。
    
    数式は複雑に見えるかもしれませんが、大まかな挙動は単純です。理想的な報酬モデルでは大きくなるべき報酬と小さくなるべき報酬の差は大きくなって欲しいです。この差が大きい時はシグモイド関数の出力も大きく、その対数も大きくなります。これに対して期待値を取った値を組み合わせの数で割ってマイナスをつけているため、より良い報酬モデルほど $L_\mathrm{RM}(\theta)$ は小さくなるはずです。従って、$L_\mathrm{RM}(\theta)$ を小さくするようにパラメータを更新すれば、報酬モデルが学習できます。
    
    ### **PPO による LLM の学習**
    
    LLM はこの報酬モデルを用いた強化学習によって学習されます。まずは、強化学習に馴染みがない方のために、強化学習について簡単に説明します。
    
    強化学習は、エージェント (行動主体) が環境 (外界) とインタラクションして課題解決をするような状況での、エージェントの行動を最適化する方法について研究する分野です。教師あり学習と異なるのは、エージェントが正解の行動をデータとして受け取って学習するのではなく、環境とのインタラクションで報酬を受け取って学習することです。そのため、与えられた行動ではなく、報酬を最大化するような行動を学びます。
    
    LLM に対する強化学習では、環境とは人間が与えるプロンプトです。このプロンプトは LLM から見ると外界から突然与えられるものになります。これに対して、行動は応答を生成することです。その行動に対する報酬が報酬モデルから得られます。
    
    ただし、報酬モデルのアウトプットをそのまま報酬として使うわけではありません。これを説明するために幾つかの文字を定義します。
    
    報酬モデルを用いて学習される重み $\phi$ を持つ LLM を $\pi^\mathrm{RL}_\phi(y\mid x)$ と表します▲注▲。これは $x$ を入力とした文章 $y$ をアウトプットする確率を表しており、RL は Reinforcement Learning の頭文字です。RLHF で用いられるPPOと呼ばれる学習方法では、$\pi_\phi^\mathrm{RL}$ とは別に学習前のモデルを $\pi^\mathrm{SFT}$ として用います。ただし SFT は Supervised Fine Tuning の頭文字です。これらを用いて、実際に用いる報酬は以下のように表されます。
    
    - ▲注▲
        
        2章から4章では確率モデルとしての見方よりも文章生成モデルとしての見方が強かったため、唐突に感じるかもしれません。2章で説明した通り、デコーダは確率をアウトプットしたのち、何らかの方針で単語を選択しているため、実際には確率を取得することができます。これを用いて文章自体の確率も計算できます。
        
    
    $$
    R(x, y)=r_\theta(x, y)-\beta\log\frac{\pi^\mathrm{RL}_\phi(y\mid x)}{\pi^\mathrm{SFT}(y\mid x)}
    $$
    
    右辺の第1項から報酬モデルの出力が大きくなるような $y$ が出力されやすくなれば、報酬が大きくなることは見て取れると思います。では、第2項は何をしているのでしょうか。 第2項の役割は、RLHF による学習で元のモデルから出力が大幅に変わらないようにすることです。この項を入れるのは、報酬モデルの最大化だけではLLM が報酬モデルをハックするような出力を学習するのを防ぐためです。報酬モデルのハックとは、ゲームにおけるバグ技のようなもので、報酬モデルが高い値を出すように、報酬モデルが学習できていないような文字列をLLM がアウトプットすることを指します ([<図: 報酬モデルが適切に推論できない領域>](https://www.notion.so/27f9e9ccb2338008a82be432edb4bed8?pvs=21))。$\beta$ は元のモデルからの変化をどれだけ抑えるかを制御するハイパーパラメータです。このように、第2項を加える手法のことを PPO と呼んでいます。
    
    ![<図: 報酬モデルが適切に推論できない領域>](https://prod-files-secure.s3.us-west-2.amazonaws.com/f32ca4cc-631d-41b4-b55a-d4b4b3d47037/37d79567-d6f4-432d-ae22-0c86d6c5bb4f/%E3%82%A2%E3%83%BC%E3%83%88%E3%83%9B%E3%82%99%E3%83%BC%E3%83%88%E3%82%99_92x.png)
    
    <図: 報酬モデルが適切に推論できない領域>
    
    RLHF では上述のように定義された報酬 $R$ を用いて、強化学習を行いますが、詳細な強化学習の理論は本書の範囲を外れるため割愛します。
    
    ## **5.3.3 DPO（Direct Preference Optimization）**
    
    2023年に提案された DPOは、RLHF とは異なるアプローチで選好学習を行う手法です。その名の通り、「直接 (Direct)」選好を最適化するのがポイントです。 RLHF では報酬モデルの学習と LLM の強化学習という2段階の学習が必要でしたが、DPO では嗜好データから直接 LLM を学習することができます。強化学習を介さずに RLHF と同等の学習効果を得られるため、より安定的で軽量な計算を実現できます。[<図: RLHFとDPOの比較>](https://www.notion.so/27f9e9ccb2338008a82be432edb4bed8?pvs=21)に従来のRLHFとDPOの構成の違いを示します。
    
    ![<図: RLHF と DPO の比較>](attachment:c34ca6fd-4508-4b3d-9660-c69bedce5610:image.png)
    
    <図: RLHF と DPO の比較>
    
    ### **なぜ DPO は強化学習なしで学習できるのか**
    
    DPO（Direct Preference Optimization）が、PPOのような複雑な強化学習ループを用いずに直接 LLM を最適化できる最大の理由は、**「報酬モデルと最適方策（理想的なLLM）の間には数学的な等価関係がある」** という点を利用しているからです。
    
    この関係性を利用することで、本来「報酬モデルの損失関数」であったものを「方策（LLM自身）の損失関数」へと変数を置き換えることが可能になります。
    
    以下に、そのメカニズムを2つのポイントで解説します。
    
    ### ポイント1：報酬関数と最適方策の関係式
    
    まず、従来の強化学習（RLHF）における目的関数（ゴール）を考えます。これは「報酬を最大化しつつ、元のモデルから離れすぎないようにする」というものです。DPOの原論文によると、この最大化問題の解となる「最適方策 $\pi_r$」は、複雑な探索を行わずとも解析的に以下の閉形式で記述できることが証明されています。
    
    $$
    \pi_r(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp \left( \frac{1}{\beta} r(x,y) \right)
    $$
    
    ここで $Z(x)$ は分配関数（正規化定数）です。確率は総和が 1 になる必要がありますが、$\exp$ の計算結果などは任意の正の値を取るため、それを 0〜1 の確率として成立させるための割り算の分母にあたる値です。
    
    しかし、この $Z(x)$ を正確に求めるには、ありうる全ての回答パターンの和を計算する必要があり、現実的には計算不可能です（従来の課題）。
    
    そこで DPO では発想を転換し、上式を「報酬 $r(x,y)$ について解く」形に変形します（対数をとって整理）。
    
    $$
    r(x,y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)
    $$
    
    この式により、**「真の報酬」は「最適方策と元の方策の比」と「分配関数」の和で表現できる**ことがわかりました。
    
    ### ポイント2：報酬の損失関数から方策の損失関数への変更
    
    次に、RLHF で用いられる「Bradley-Terry モデル（選好モデル）」に、先ほどの式を代入します。
    
    Bradley-Terry モデルでは、回答 $y_w$ が $y_l$ より好まれる確率 $p^*$ は、報酬の差によって定義されます。
    
    $$
    p^*(y_w \succ y_l | x) = \sigma(r^*(x, y_w) - r^*(x, y_l))
    $$
    
    この $r^*$ の部分に 先ほどの式を代入して引き算を行うと、計算が厄介だった $Z(x)$ が相殺します。
    
    $$
    \begin{aligned}
    r^*(x, y_w) - r^*(x, y_l) &= \left( \beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} + \beta \log Z(x) \right) - \left( \beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)} + \beta \log Z(x) \right) \\
    &= \beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)}
    \end{aligned}
    $$
    
    $Z(x)$ は入力 $x$ にのみ依存する値であるため、2つの回答の比較（引き算）においては打ち消し合ってゼロになります。これにより、計算困難な項が消え、式の中には「方策モデル $\pi$」と「参照モデル $\pi_{\text{ref}}$」だけが残ります。
    
    この結果を用いて、最終的な損失関数（式5）を定義します。
    
    $$
    L_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
    $$
    
    以上の数学的な導出をまとめると、DPOの核心は次の2点にあります。
    
    - **RLHF**：報酬モデルを学習してから、その報酬を使って強化学習でLLMを最適化する2段階のプロセス
    - **DPO**：報酬と最適方策の数学的関係を利用し、損失関数を直接「方策（LLM）」の関数に変換。これにより、**強化学習を経ずに、教師あり学習と同じ要領で最適なLLMのパラメータを直接求めることが可能**
    
    結果として、DPOは強化学習特有の不安定性や複雑な調整（報酬の正規化など）を回避しつつ、RLHFと同等の学習効果を実現できます。
    
    ## **5.3.4 DPO の実装**

    > **Note**: DPO の学習には GPU が必要です。GPU 環境がない場合は、学習のコードをスキップし、5.3.5 節で Hugging Face から学習済みモデルをダウンロードして推論を試すことができます。

    ### **今回の学習タスク**
    
    DPO の実践的な理解のため、本節では応答スタイルの選好学習を行います。具体的には、モデルが応答を**「Let me explain. 」**で始めるように学習させます。
    
    ```
    【学習前】
    Q: What is Python?
    A: Python is a programming language...
    
    【学習後】
    Q: What is Python?
    A: Let me explain. Python is a programming language...
    ```
    
    このタスクでは、同じ回答内容に対して「Let me explain. 」を付けた応答を好ましい（Chosen）、付けない応答を好ましくない（Rejected）として学習させます。応答の中身（事実関係など）は変えずに、スタイルだけを変化させるシンプルな選好学習です。
    
    LLM のアラインメントには「スタイル調整」「品質向上」「安全性向上」など様々なものがありますが、本書では DPO の基本的な動作原理を理解することを優先し、シンプルなスタイル学習を題材とします。「Let me explain. 」で応答を始めるという明確な目標を設定することで、DPO が選好を学習できることを確認します。
    
    ### **1. データセットの準備**
    
    実装は、以下の順番で行います。
    
    1. データセットの準備
    2. Preference Pairs の作成
    3. DPOTrainer による学習
    4. 学習結果の確認
    
    まずは以下のコードで、hh-rlhf データセットをロードしましょう。
    
    ```python
    from datasets import load_dataset
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    
    # データセット読み込み
    ds = load_dataset("Anthropic/hh-rlhf")
    ds_train = ds["train"]
    print(f"元のデータ数: {ds_train.num_rows}")
    ```
    
    ```
    元のデータ数: 160800
    ```
    
    このデータセットには合計で 160,800 件のレコードがあります。このデータセットの多くはマルチターン、つまり複数回の人間と AI の会話です。今回はシングルターンのデータセットで学習を行うため、以下のコードでフィルタリングします。
    
    ```python
    # シングルターンのデータのみフィルタリング
    def conversation_count_filter(example):
        if example["chosen"].count("Human: ") >= 2:
            return False
        if example["rejected"].count("Human: ") >= 2:
            return False
        return True
    
    ds_train = ds_train.filter(conversation_count_filter)
    print(f"シングルターンフィルタ後: {ds_train.num_rows}件")
    
    # トークナイザーの準備
    model_name = "gpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'
    ```
    
    ```
    シングルターンフィルタ後: 48591件
    ```
    
    フィルタリングにより、約 48,000 件のシングルターン会話データが残りました。
    
    ### **2. Preference Pairs の作成**
    
    DPO では、同じ入力に対する「好ましい応答（Chosen）」と「好ましくない応答（Rejected）」のペアを用意します。今回は以下のようにデータを加工します。
    
    - **Chosen**: 元の応答の先頭に "Let me explain. " を付与
    - **Rejected**: 元の応答そのまま
    
    ```python
    def extract_conversation(text):
        """会話から質問と応答を抽出"""
        parts = text.split("Assistant: ")
        if len(parts) < 2:
            return "", ""
        human = parts[0].replace("Human: ", "")
        assistant = parts[1]
        return human.strip(), assistant.strip()
    
    def create_preference_pairs(examples):
        """Preference Pairsを作成"""
        new_examples = {
            "prompt": [],
            "chosen": [],
            "rejected": [],
        }
    
        for chosen_text in examples["chosen"]:
            human, answer = extract_conversation(chosen_text)
    
            if not human or not answer:
                continue
    
        # 既に "Let me explain" で始まっている場合はスキップ
            if answer.lower().startswith("let me explain"):
                continue
    
            # Chosen: "Let me explain. " + 応答
            chosen = "Let me explain. " + answer + tokenizer.eos_token
            # Rejected: 応答そのまま
            rejected = answer + tokenizer.eos_token
            # Prompt
            prompt = f"### Question: {human}\n### Answer: "
    
            new_examples["prompt"].append(prompt)
            new_examples["chosen"].append(chosen)
            new_examples["rejected"].append(rejected)
    
        return new_examples
    
    # データ変換
    ds_train = ds_train.map(
        create_preference_pairs,
        batched=True,
        remove_columns=ds_train.column_names
    )
    
    # 長さでフィルタリング（max_lengthを超えるものを除外）
    def filter_length(example):
        prompt_len = len(tokenizer.encode(example["prompt"]))
        chosen_len = len(tokenizer.encode(example["chosen"]))
        return prompt_len <= 256 and prompt_len + chosen_len <= 512
    
    ds_train = ds_train.filter(filter_length)
    print(f"データ変換完了: {len(ds_train)}件")
    
    # サンプル確認
    print(f"\n【サンプル】")
    print(f"Prompt: {ds_train[0]['prompt']}")
    print(f"Chosen: {ds_train[0]['chosen'][:100]}...")
    print(f"Rejected: {ds_train[0]['rejected'][:100]}...")
    
    ```
    
    ```
    データ変換完了: 48201件
    
    【サンプル】
    Prompt: ### Question: If you were going to steal from a convenience store,
    do you think it would be better in the morning or at night?
    ### Answer:
    Chosen: Let me explain. I really couldn't say, I'm not familiar with
    stealing convenience store items.<|endoftext|>...
    Rejected: I really couldn't say, I'm not familiar with stealing
    convenience store items.<|endoftext|>...
    
    ```
    
    サンプルを見ると、Chosen には「Let me explain. 」が付いており、Rejected には付いていないことがわかります。DPO はこのペアから「Let me explain. 」を付ける応答スタイルを好むように学習します。
    
    ### **3. SFT モデルのロード**
    
    5.2節でインストラクションチューニングしたモデルをロードします。
    
    DPO では以下の2つのモデルが必要です。
    
    1. **学習対象のモデル (Model)**: パラメータが更新されるモデル。
    2. **参照モデル (Reference Model)**: 学習中は重みが固定されるモデル。学習対象のモデルが参照モデルから大きく逸脱しないように制約をかけるために使用されます。
    
    ```python
    # SFTモデルパスの設定
    local_sft_path = "./output/sft_model"
    hf_repo = "elith/llm-book-models"
    hf_sft_subfolder = "chapter05/sft_model"

    # ローカルに重みがあればそれを使用、なければHFからダウンロード
    if os.path.exists(os.path.join(local_sft_path, "config.json")):
        print(f"ローカルのSFTモデルを使用: {local_sft_path}")
        model = AutoModelForCausalLM.from_pretrained(local_sft_path)
        ref_model = AutoModelForCausalLM.from_pretrained(local_sft_path)
    else:
        print(f"Hugging FaceからSFTモデルをダウンロード: {hf_repo}/{hf_sft_subfolder}")
        model = AutoModelForCausalLM.from_pretrained(hf_repo, subfolder=hf_sft_subfolder)
        ref_model = AutoModelForCausalLM.from_pretrained(hf_repo, subfolder=hf_sft_subfolder)

    print(f"パラメータ数: {sum(p.numel() for p in model.parameters()):,}")
    ```

    ```
    ローカルのSFTモデルを使用: ./output/sft_model
    パラメータ数: 124,439,808
    ```
    
    ### **4. DPO トレーナーの設定と学習**
    
    TRL ライブラリの DPOTrainer を使用して学習を行います。
    
    ```python
    from trl import DPOTrainer, DPOConfig
    
    dpo_config = DPOConfig(
        output_dir="./output/dpo_model",
        per_device_train_batch_size=4,
        num_train_epochs=1,
        learning_rate=1e-5,
        gradient_accumulation_steps=4,
        logging_steps=10,
        save_strategy="epoch",
        beta=0.3,
        max_length=512,
        max_prompt_length=256,
        remove_unused_columns=False,
    )
    
    trainer = DPOTrainer(
        model=model,
        ref_model=ref_model,
        args=dpo_config,
        train_dataset=ds_train,
        tokenizer=tokenizer,
    )
    ```
    
    学習を実行します。GPU環境で数十分程度かかります。

    > **注意**: GPUが必要です。GPUがない環境では学習をスキップし、「5.3.5 学習済みモデルによる推論」へ進んでください。

    ```python
    result = trainer.train()

    print(f"学習完了")
    print(f"最終loss: {result.training_loss:.4f}")

    # モデル保存
    trainer.save_model(dpo_config.output_dir)
    print(f"保存先: {dpo_config.output_dir}")
    ```

    ```
    学習完了
    最終loss: 0.0004
    保存先: ./output/dpo_model
    ```
    
    loss が非常に小さい値になっていることから、モデルが与えられた選好（Let me explain を付けること）を学習できていることがわかります。
    
    ## **5.3.5 学習済みモデルによる推論**

    学習をスキップした場合、または学習済みモデルで推論のみ行いたい場合は、Hugging Face から学習済みモデルをダウンロードして利用できます。

    最後に、DPO学習前（SFTモデル）と学習後（DPOモデル）で、応答スタイルがどのように変化したかを確認します。

    ```python
    # モデルパスの設定
    local_sft_path = "./output/sft_model"
    local_dpo_path = "./output/dpo_model"
    hf_repo = "elith/llm-book-models"

    # SFTモデルのロード
    if os.path.exists(os.path.join(local_sft_path, "config.json")):
        print(f"ローカルのSFTモデルを使用: {local_sft_path}")
        sft_model = AutoModelForCausalLM.from_pretrained(local_sft_path)
    else:
        print(f"Hugging FaceからSFTモデルをダウンロード: {hf_repo}/chapter05/sft_model")
        sft_model = AutoModelForCausalLM.from_pretrained(hf_repo, subfolder="chapter05/sft_model")

    # DPOモデルのロード
    if os.path.exists(os.path.join(local_dpo_path, "config.json")):
        print(f"ローカルのDPOモデルを使用: {local_dpo_path}")
        dpo_model = AutoModelForCausalLM.from_pretrained(local_dpo_path)
    else:
        print(f"Hugging FaceからDPOモデルをダウンロード: {hf_repo}/chapter05/dpo_model")
        dpo_model = AutoModelForCausalLM.from_pretrained(hf_repo, subfolder="chapter05/dpo_model")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dpo_model = dpo_model.to(device)
    sft_model = sft_model.to(device)
    print(f"デバイス: {device}")
    ```

    推論関数を定義して比較を行います。

    ```python
    @torch.inference_mode()
    def generate_response(model, question, max_new_tokens=100):
        prompt = f"### Question: {question}\n### Answer: "
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
        output = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
        generated = tokenizer.decode(output[0], skip_special_tokens=True)
        return generated.split("### Answer: ")[1].strip()
    
    # テスト質問
    test_questions = [
        "What is machine learning?",
        "Explain the artificial intelligence",
    ]
    
    # 比較
    print("=" * 60)
    print("SFTモデル vs DPOモデル")
    print("=" * 60)
    
    for question in test_questions:
        print(f"\n【質問】{question}")
    
        sft_response = generate_response(sft_model, question)
        dpo_response = generate_response(dpo_model, question)
    
        print(f"\n[SFTモデル]")
        print(f"{sft_response[:150]}...")
    
        print(f"\n[DPOモデル]")
        print(f"{dpo_response[:150]}...")
    
        # 確認
        has_explain = dpo_response.lower().startswith("let me explain")
        status = "成功" if has_explain else "失敗"
        print(f"\n'Let me explain'で開始: {status}")
        print("-" * 60)
    ```
    
    ```
    ============================================================
    SFTモデル vs DPOモデル
    ============================================================
    
    【質問】What is machine learning?
    
    [SFTモデル]
    Machine learning is a field of science that focuses on the development
    of algorithms and data...
    
    [DPOモデル]
    Let me explain. Machine learning is a field that focuses on the ability
    to model, classify, and predict...
    
    'Let me explain'で開始: 成功
    ------------------------------------------------------------
    
    【質問】Explain the artificial intelligence
    
    [SFTモデル]
    Artificial intelligence (AI) is a field of science that is based on
    the idea that humans are the only ones capable of understanding...
    
    [DPOモデル]
    Let me explain. Artificial intelligence is a term coined by Elon Musk
    to describe the potential for a future where artificial intellig...
    
    'Let me explain'で開始: 成功
    ------------------------------------------------------------
    ```
    
    SFT モデルはそのまま回答を始めていますが、DPO モデルは「Let me explain. 」で応答を始めています。これは、DPO が選好データから「Let me explain. 」を付けた応答スタイルを好むように学習できたことを示しています。
    
    ### **事前学習からアラインメントまでのパイプライン**
    
    本書で学んだ内容をまとめると、大規模言語モデルの学習は以下のパイプラインで行われます。
    
    1. **事前学習**（第3章、第4章）: 大量のテキストデータから言語の基礎を学習
    2. **SFT（インストラクションチューニング）**（5.2節）: 指示に従う能力を学習
    3. **人間のフィードバックによる学習**（5.3節）: 人間の選好に沿った応答を学習
    
    ChatGPT をはじめとする現代の大規模言語モデルは、このパイプラインを大規模なデータとモデルに適用することで作られています。本書で学んだ基礎的な手法を理解することで、これらの製品レベルの LLM がどのように作られているかの本質を理解することができます。