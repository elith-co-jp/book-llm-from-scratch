# 第5章 アラインメント

本章では、大規模言語モデルのアラインメント (Alignment) について説明します。Alignment という単語自体には、調整するといった意味があります。LLM におけるアラインメントは、人間の意図や価値観に沿うように調整するといった意味で使われます。これまでに扱ったのは次のトークンを予測するようなモデルの学習でしたが、何をすれば人間の意図や価値観とマッチしたモデルが学習できるのでしょうか。その答えに入る前に 5.1 節ではアラインメントの必要性や目的について説明します。その後、5.2, 5.3 節で実際にアラインメントを実現する方法の説明と、4章までに学習したモデルに対するアラインメントを行います。

# 5.1 アラインメントの基礎

## 5.1.1 AI アラインメント

人間の価値観に沿うアラインメントの必要性は、LLM の台頭以前から言及されていました。特に、汎用人工知能 (Artificial General Intelligence; AGI) や人工超知能 (Artificial Super Intelligence; ASI) のような強力な AI の文脈で扱われることが多いです。

ではなぜ、強力な AI ではアラインメントが重要になるのでしょうか。その理由の 1 つは、十分賢い AI は指示に対して人間の想定と異なる解法を思いついてしまうためです。

有名な例としてスウェーデンの哲学者 Nick Bostrom が挙げた「ペーパークリップ問題」があります:

できるだけ多くのペーパークリップを作ることだけを目的とする AI があるとします。AI は、人間がいないほうがずっと良いことにすぐに気付くでしょう。なぜなら、人間がスイッチを切るかもしれないからです。人間がスイッチを切ると、ペーパークリップの数が少なくなるからです。また、人体にはペーパークリップにできる原子がたくさん含まれています。AI が目指す未来は、ペーパークリップはたくさんあるが人間がいない未来です。

読者の中には、指示が悪かったと考える人もいるでしょう。当然、上述の問題が実際に起こるとは考えにくいですが、実際にはより複雑な形で同じような問題が起きうるのです。そのため、人間と同等以上に賢い AI ができる前には、それに伴うリスクを無くす方法が必要になります。

ペーパークリップの例とも関連しますが、Bostrom は AI に関する知的能力と倫理観の直交性にも言及しています。これは賢さと倫理観は無関係であらゆる賢さとあらゆる倫理観の組み合わせがありうるという考えです▲注▲。人間で例えると、賢いからといって善人とは限らず、無知だからといって悪人とは限らないということです。AI においては、特に賢い場合に倫理観が人間とズレている時のリスクが大きいため、アラインメントが重要になります。

- ▲注▲
    
    Orthogonal thesis といいます。これは数理的に証明されたものではなく、考えの1つですので、反対に賢さと比例して倫理観が身につくと主張する研究者もいます。
    

## 5.1.2 LLM アラインメント

LLM のアラインメントは、以下の2通りに大別されます。

1. 人間の指示通りにタスクをこなせるよう調整する
2. 人間の嗜好に合った応答をするよう調整する

本書では 1 の手法としてインストラクションチューニング、2 の手法として人間のフィードバックを用いた強化学習 (Reinforcement Learning from Human Feedback; RLHF) を扱います。

事前学習では、文章から次のトークンを予測することを通して学習していました。4章でも述べた通り、このような学習を行ったモデルは、コンテキストとして文章を与えるとその続きを生成します。このような LLM も、適切に用いることで一部の自然言語処理タスクを解くことはできます。一方、ChatGPT のように対話形式でタスクを与えてタスクを解かせることはできません。これを実現するのがインストラクションチューニングです。

詳細は 5.2 節で説明しますが、インストラクションチューニングはモデルにタスクの指示文と正解となる応答例を見せることで学習を行います。そのため、どのような応答が望ましいのかは学習できる反面、どのような応答が望ましくないのかというネガティブフィードバックを与えることができません。5.1.1 で述べたような価値観に沿うような学習を行うためには、してはいけないことについての学習も必要です。RLHF は応答例に対する人間の評価を通して学習するため、モデルに対してネガティブフィードバックが可能になります。

LLM においてアラインメントが必要になる背景には、学習データの問題があります。4章で説明した通り LLM の学習データの多くはインターネットからクローリングされたデータです。このようなデータにはハッキングの方法、爆弾の作り方などの悪意ある情報や、罵詈雑言のような望ましくないテキスト、嘘の情報が含まれます。また、インターネット以外のデータにおいても、人間の生み出してきた文章には、人間社会におけるバイアスが反映されてしまいます。実際、適切なアラインメントができていない LLM においては性別や宗教に関するステレオタイプを示すことが実験的にもわかっています。

以上のような背景から、LLM においては AI が賢すぎることに起因する問題の他に、データによる問題を解消する必要があり、アラインメントが必須の技術になります。