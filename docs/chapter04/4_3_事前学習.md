# 4.3 事前学習

これまでは、大規模言語モデルについて理論的な説明と実装をイチから行ってきました。この節では、Hugging Face の transformers や Microsoft の DeepSpeed のような、実際に LLM 学習で利用されるライブラリを用いて、事前学習を実施します。このようにいうと、料理番組を見ていたら、冷蔵庫から完成品が出てきたような感覚の方もいるかもしれません。しかし、transformers や DeepSpeed を利用するのは、料理で言うクッキーの型とオーブンを利用するようなもので、手早く綺麗な形を作るには必要不可欠になります。

## 4.3.1 事前学習とは

2章ではデコーダが次の単語を予測し、実際の次の単語の情報と照らし合わせることで学習を行なっていました。この時学習した Transformer はエンコーダとデコーダをもつモデルであったため、翻訳タスクとして入力の文章も受け取っていました。これに対して、本章で学習する GPT モデルは、3 章でも説明した通りデコーダ単体のモデルになります。そのため文章から文章への変換ではなく、文章自体の出現確率を表現する言語モデルを学習します。ただし、事前学習の段階では学習の方法は同様で、次の単語を予測する形をとります。

では、この学習をなぜ事前学習と呼ぶのでしょうか。すでに述べた通り、大規模言語モデルは追加学習を行わずにタスクを解くゼロショット性能が非常に高いです。しかし、従来は次の単語を予測する学習を行なったのち、解きたい自然言語タスク (翻訳などの下流タスク) に対してファインチューニングしていました。また、後述しますが大規模言語モデルでも事前学習の段階ではさまざまなタスクを解く様なモデルにはなっておらず、5.2節で解説する解説するインストラクションチューニングが必要になります。こういった背景から、大規模言語モデルに言語モデルとしての知識をつける段階を事前学習と呼びます。

## 4.3.2 Hugging Face Transformers

これまでは、PyTorch を用いてモデルを構築していました。PyTorch は深層学習モデルの構築と学習を柔軟に行えるフレームワークです。ただし、様々なアーキテクチャや、様々なタスクの学習、推論ができるように設計されていることから、特定の手法のみを扱う場合でも、ユーザが把握しておくべき範囲が広いです。

近年は Transformer ベースのモデルが盛んに研究されており、本書の主題である大規模言語モデルも多くのバリエーションが発表されています。このような背景から、Transformer に焦点を当てたライブラリとして公開されているのが Hugging Face Transformers (以降 transformers と呼びます) です。このライブラリでは、モデルの構築や学習、推論が簡単に行えるだけでなく、Hugging Face に公開されている学習済みモデルを読み込んで利用することも可能です。本節では、このライブラリを用いて軽量の GPT-2 モデルを構築し、このモデルの学習を通して大規模言語モデルの学習方法を学びます。

大規模言語モデルにおいて必要になるのは以下の2点です。

1. トークナイザ
2. モデル本体

では transformers を用いてトークナイザを作成してみましょう。第3章で扱った通り、GPT-2 に用いられるトークナイザは Byte Pair Encoding (BPE) です。ただし、通常の BPE はユニコードレベルで文字を分解するのに対して、GPT-2 ではバイトレベルで分解しています。

まずは [<コード: 学習済みトークナイザの利用>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) を用いて、学習済みの GPT-2 のトークナイザの挙動を確認します。 サンプルコードに示すように、transformers には AutoTokenizer というクラスが用意されており、このクラスは from_pretrained メソッドにより学習済みのトークナイザを読み込みます。クラス名からもわかるように、ユーザは裏で使われているトークナイザの種類 (BPE や SentencePiece など) を意識することなく利用することが可能です。

```python
from transformers import AutoTokenizer

# 学習済みトークナイザの読み込み
gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

次にトークナイザを学習するコードを [<コード: BPE トークナイザの学習>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) に示します。学習に用いる dataset は、4.1 節で作成した前処理済みのデータを読み込んだものです。また、学習する場合は transformers ではなく tokenizers というモジュールからトークナイザをインポートします。この tokenizers も Hugging Face が提供するものです。ここでは、特殊トークンとして文章の開始、終了を表す <bos> と <eos> 、学習時のパディングを表す <pad> トークンを設定しています▲注▲。

今回は 3 章で紹介した SentencePiece を利用します。  

- ▲注▲
    
    GPT-2 が提案された際は、 文章間の区切りを表すトークンとして <|endoftext|> のみが用いられていました。ここでは分かりやすさのために 3 つの特殊トークンを分けて設定しています。
    

```python
from tokenizers import ByteLevelBPETokenizer
from transformers import PreTrainedTokenizerFast

tokenizer = ByteLevelBPETokenizer()
tokenizer.train_from_iterator(
    dataset["text"], # コーパスを指定
    vocab_size=30_000, # 語彙数を指定
)
my_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<bos>",
    eos_token="<eos>",
    pad_token="<pad>",
)
```

コード修正ver nagasawa

```python
import sentencepiece as spm
from transformers import PreTrainedTokenizerFast

# SentencePieceモデルの学習
spm.SentencePieceTrainer.Train(
    input="corpus.txt",  # コーパスファイルを指定 
    model_prefix="spm",  # 出力ファイル名を指定 
    vocab_size=30000,    # 語彙数を指定 
)

my_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<bos>",
    eos_token="<eos>",
    pad_token="<pad>",
)
```

学習後のトークナイザを与えている PreTrainedTokenizerFast クラスは、tokenizers モジュールのトークナイザを transformers で利用できる様に変換するものです。

では、[<コード: 各トークナイザの出力>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) を実行してそれぞれの出力を確認してみましょう。

```python
example = "abstract: This is an example of text. introduction: This is an introduction. conclusion: This is a conclusion."
print(gpt2_tokenizer.tokenize(example))
# ['ab', 'stract', ':', 'ĠThis', 'Ġis', 'Ġan', 'Ġexample', 'Ġof', 'Ġtext', '.', 'Ġintroduction', ':', 'ĠThis', 'Ġis', 'Ġan', 'Ġintroduction', '.', 'Ġconclusion', ':', 'ĠThis', 'Ġis', 'Ġa', 'Ġconclusion', '.']
print(my_tokenizer.tokenize(example))
# ['abstract', ':', 'ĠThis', 'Ġis', 'Ġan', 'Ġexample', 'Ġof', 'Ġtext', '.', 'Ġintroduction', ':', 'ĠThis', 'Ġis', 'Ġan', 'Ġintroduction', '.', 'Ġconclusion', ':', 'ĠThis', 'Ġis', 'Ġa', 'Ġconclusion', '.']
```

両方に共通して、見慣れない文字 `Ġ` が現れています。この様な文字が現れるのは、BPE アルゴリズムが後処理に空白文字を利用するため、事前に文章中の空白文字を `Ġ` に変換しているためです。 

顕著に異なるのは abstract という単語が GPT-2 のトークナイザでは ab と stract に分かれており、新たに学習したトークナイザでは 1 つにまとまっている点です。これは、今回用いたコーパスに abstract という単語が頻出であったため、1つのトークンになるよう学習されたのだと考えられます。

モデル学習時に用いるので、[<コード: トークナイザの保存>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) で学習したトークナイザを保存しておきます。

```python
my_tokenizer.save_pretrained("tokenizer")
```

次に、モデルの構築を行います。transformers では、モデルの詳細な構造を Config の形で定義します。その後、その Config を対応するモデルクラスに渡すことで、実際のモデルを作成します。[<コード: 定義済みの GPT-2 Config を利用する>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) では、定義済みの GPT-2 モデルの Config を作成します。自作のモデルや Config を作成する方法については後述します。一般的な大規模言語モデルと同様の、文章の確率を与える様なモデルとしての GPT-2 を用いる場合、モデルクラスとしては GPT2LMHeadModel を用います。

```python
from transformers import GPT2LMHeadModel, AutoConfig, AutoTokenizer

# トークナイザの読み込み
tokenizer = AutoTokenizer.from_pretrained("tokenizer")

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=512,　# 最大の入力トークン長
    bos_token_id=transformer_tokenizer.bos_token_id,
    eos_token_id=transformer_tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config)
```

Config には何が定義されているのでしょうか。[<コード: Config の中身>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) で確認してみましょう。

```python
print(config)
# GPT2Config {
#   "_name_or_path": "gpt2",
#   "activation_function": "gelu_new",  #活性化関を表示
#   "architectures": [
#     "GPT2LMHeadModel"                 #アーキテクチャの指定
#   ],
#   "attn_pdrop": 0.1,
#   "bos_token_id": 30000,
#   "embd_pdrop": 0.1,
#   "eos_token_id": 30001,
#   "initializer_range": 0.02,
#   "layer_norm_epsilon": 1e-05,
#   "model_type": "gpt2",
#   "n_ctx": 512,
#   "n_embd": 768,　　　　　　　#埋め込みベクトルの次元を表示
#   "n_head": 12,              #マルチヘッドアテンションを表示
#   "n_inner": null,
#   "n_layer": 12,
#   "n_positions": 1024,
#   "reorder_and_upcast_attn": false,
#   "resid_pdrop": 0.1,
#   "scale_attn_by_inverse_layer_idx": false,
#   "scale_attn_weights": true,
#   "summary_activation": null,
#   "summary_first_dropout": 0.1,
# ...
#   "use_cache": true,
#   "vocab_size": 30004
# }
```

出力からわかる様に、以下のような内容が設定されています。

- どのアーキテクチャのための設定であるか
- 活性化関数には何を用いるか
- 埋め込みベクトルの次元はいくつか
- マルチヘッドアテンションのヘッド数はいくつか

GPT2LMHeadModel は、この設定で指定された数のヘッド数を用意したり、設定された次元の埋め込みベクトルを利用してモデルを初期化します。これは、第2章で Transformer モデルをインスタンス化する際に複数のハイパーパラメータ与えていたのと同様です。与えるべきハイパーパラメータが大量になってしまうため、Config という形にまとめていると考えましょう。

## 4.3.3 分散学習のためのライブラリ

4.2 節では効率的な学習方法として 3D 並列化と ZeRO を紹介し、PyTorch だけを用いた実装をいくつか示しました。その際にも述べた通り、適切な並列化を自身で設計・実装するのは難しい場合が多いです。ありがたいことに、大規模言語モデルの学習においては、分散学習をサポートするリポジトリがいくつか存在します。ここでは代表的なものとして、以下の4つを紹介します。

1. **Megatron-LM**
Megatron-LM は NVIDIA から 2019 年に発表された並列化を駆使して開発された、パラメータ数が数十億スケールのLLM の名称です。現在は NVIDIA の管理するリポジトリとして存在しており、様々な Transformer ベースのモデルとその学習コードが管理されています。
2. **DeepSpeed**
DeepSpeed は Microsoft が管理する大規模な深層学習モデルの学習の実装を楽にするためのフレームワークです。3D 並列化や ZeRO による学習だけでなく、Mixture-of-Expert (MoE) と呼ばれる特殊なアーキテクチャや、推論の効率化、モデルサイズの圧縮など様々な機能が実装されています。
3. **Megatron-DeepSpeed**
Microsoft が Megatron-LM のコードをベースに DeepSpeed の機能を追加したリポジトリです。DeepSpeed を用いて効率的に学習するためのサンプルコードが管理されています。
4. **GPT-NeoX**
EleutherAI が管理する大規模言語モデルを学習するためのライブラリです。Megatron-DeepSpeed と同様に、Megatron-LM ベースのモデルを DeepSpeed を用いて学習できます。

## 4.3.4 DeepSpeed を用いた学習

4.3.3 で説明した通り、大規模言語モデルを学習するためのライブラリは複数存在します。ここでは DeepSpeed と ZeRO を用いて学習を行います。

transformers を用いて定義したモデルの学習に DeepSpeed を用いるのは非常に簡単で、学習時の引数に DeepSpeed の設定ファイルを渡し、DeepSpeed のコマンドを用いて学習コードを実行するだけで実現できます。

まず、transformers を用いた学習コードを作成してみましょう。トークナイザの学習とモデルの作成までは 4.3.2 で完了しています。続きのコードを [<コード: transformers を用いた学習>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) に示します。

```python
# data_collator の作成
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

サンプルコードで利用している DataCollatorForLanguageModelling クラスの役割は2.5.3項で示した collate_fn と同じで、学習時にデータをロードする際のデータを加工することです。具体的には、教師データやマスクを作成します。mlm 引数は、masked language model の略で、BERT などエンコーダベースのモデルを学習する場合などに True にします。今回はデコーダベースのモデルを学習するため False を設定してください。

次に、[<コード: データセットのトークン化>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) でデータセットのトークン化を行います。datasets.Dataset クラスのインスタンスは、 map を用いた要素の加工が可能です。batched=True を設定することでバッチ処理ができます。トークナイザを用いて文章を分割する際は、 transformer_tokenizer.tokenize を用いましたが、ここでは transformer_tokenizer にそのままデータを与えています。これにより、分割されたテキストではなく、トークン ID の情報を持つアウトプットが得られます。

```python
# データセットのトークン化
dataset = dataset.map(lambda data: tokenizer(data["text"]), batched=True)
```

最後に [<コード: transformers を用いた学習>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) で学習を実行します。コードからわかる様に、transformers では Trainer を用いることで学習コードをシンプルに記述できます。TrainingArguments には学習時に与える設定を記述します。

```python
from transformers import Trainer, TrainingArguments

# 学習
training_args = TrainingArguments(
		output_dir="./output", # 途中で保存する際の保存先
    logging_strategy="steps",
    logging_steps=100, # ログを表示する間隔
    save_strategy="steps",
    save_steps=1000, # 途中で保存する間隔
    num_train_epochs=3, # エポック数
    per_device_train_batch_size=3, # GPU ごとのバッチサイズ
    learning_rate=1e-6, # 学習率
    weight_decay=0.01, # モデルアップデート時に利用
    warmup_ratio=0.1, # Scheduler のパラメータ
    optim="adamw_torch", # 最適化手法
    ds_config="./ds_config.json"
)
trainer = Trainer(
    model=model,
    tokenizer=transformer_tokenizer,
    train_dataset=dataset,
    args=training_args,
    data_collator=data_collator,
)

with torch.autocast("cuda"):
    trainer.train()

# モデルの保存
trainer.save_model("pretrained_model")
```

TrainingArguments の ds_config に設定しているファイルが、DeepSpeed に関する設定ファイルです。このファイルに ZeRO のどのステージを利用するか、CPU オフロードを行うかといった設定を記述します。

## 4.3.5 学習済みモデルを用いた推論

4.3.4 で学習したモデルを用いた推論は [<コード: 学習済みモデルによる推論>](https://www.notion.so/4-3-f901e9f141774019ab2e22e6584892ad?pvs=21) のように行います。推論時の引数は以下のものを設定しています。

- **max_new_token**: 生成するトークン数の上限
- **do_sample**: サンプリングを行うか
- **repetition_penalty**: 同じトークンが繰り返されることへのペナルティ

```python

prompt = "Hi, how are you?"

# トークン化
input_ids = tokenizer.encode(prompt, return_tensors="pt").to("cuda")

# モデルで生成
with torch.inference_mode():
		output = model.generate(
		    input_ids,
		    max_new_tokens=128,
		    do_sample=False,
		    repetition_penalty=1.1,
		    pad_token_id = tokenizer.eos_token_id
		)

# 生成されたテキストをデコード
generated_text = tokenizer.decode(output[0], skip_special_tokens=False)
print(generated_text)
```

入力テキストには疑問文を与えましたが、応答文ではなく、文章の続きが生成されたのではないでしょうか。第5章では、これに対してアラインメントを行うことで ChatGPT のような対話型の LLM を作成します。