# 4.3 事前学習

これまでは、大規模言語モデルについて理論的な説明と実装をイチから行ってきました。この節では、Hugging Face の transformers を用いて、事前学習済みモデルの継続学習を実施します。transformers を利用するのは、料理で言うクッキーの型とオーブンを利用するようなもので、手早く綺麗な形を作るには必要不可欠になります。

## 4.3.1 事前学習とは

2章ではデコーダが次の単語を予測し、実際の次の単語の情報と照らし合わせることで学習を行なっていました。この時学習した Transformer はエンコーダとデコーダをもつモデルであったため、翻訳タスクとして入力の文章も受け取っていました。これに対して、本章で学習する GPT モデルは、3 章でも説明した通りデコーダ単体のモデルになります。そのため文章から文章への変換ではなく、文章自体の出現確率を表現する言語モデルを学習します。ただし、事前学習の段階では学習の方法は同様で、次の単語を予測する形をとります。

では、この学習をなぜ事前学習と呼ぶのでしょうか。すでに述べた通り、大規模言語モデルは追加学習を行わずにタスクを解くゼロショット性能が非常に高いです。しかし、従来は次の単語を予測する学習を行なったのち、解きたい自然言語タスク (翻訳などの下流タスク) に対してファインチューニングしていました。また、後述しますが大規模言語モデルでも事前学習の段階ではさまざまなタスクを解く様なモデルにはなっておらず、5.2節で解説するインストラクションチューニングが必要になります。こういった背景から、大規模言語モデルに言語モデルとしての知識をつける段階を事前学習と呼びます。

## 4.3.2 Hugging Face Transformers

これまでは、PyTorch を用いてモデルを構築していました。PyTorch は深層学習モデルの構築と学習を柔軟に行えるフレームワークです。ただし、様々なアーキテクチャや、様々なタスクの学習、推論ができるように設計されていることから、特定の手法のみを扱う場合でも、ユーザが把握しておくべき範囲が広いです。

近年は Transformer ベースのモデルが盛んに研究されており、本書の主題である大規模言語モデルも多くのバリエーションが発表されています。このような背景から、Transformer に焦点を当てたライブラリとして公開されているのが Hugging Face Transformers (以降 transformers と呼びます) です。このライブラリでは、モデルの構築や学習、推論が簡単に行えるだけでなく、Hugging Face に公開されている学習済みモデルを読み込んで利用することも可能です。本節では、このライブラリを用いて事前学習済みの日本語 GPT-2 モデルを青空文庫データで継続学習し、大規模言語モデルの学習方法を学びます。

大規模言語モデルにおいて必要になるのは以下の2点です。

1. トークナイザ
2. モデル本体

### 4.3.2.1 事前学習済みトークナイザの利用

本節では、rinna 社が公開している日本語 GPT-2 モデル (`rinna/japanese-gpt2-medium`) の事前学習済みトークナイザを利用します。transformers には AutoTokenizer というクラスが用意されており、このクラスは `from_pretrained` メソッドにより学習済みのトークナイザを読み込みます。クラス名からもわかるように、ユーザは裏で使われているトークナイザの種類 (BPE や SentencePiece など) を意識することなく利用することが可能です。

```python
from transformers import AutoTokenizer

# 事前学習済みトークナイザの読み込み
model_name = "rinna/japanese-gpt2-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

# pad_tokenが未設定の場合はeos_tokenを使用
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"語彙サイズ: {len(tokenizer)}")
print(f"BOS token: {tokenizer.bos_token}")
print(f"EOS token: {tokenizer.eos_token}")
print(f"PAD token: {tokenizer.pad_token}")
```

### 4.3.2.2 事前学習済みモデルの利用

次に、モデルの構築を行います。transformers では、モデルの詳細な構造を Config の形で定義します。事前学習済みモデルを利用する場合は、`from_pretrained` メソッドで Config とモデルの重みを同時に読み込むことができます。一般的な大規模言語モデルと同様の、文章の確率を与える様なモデルとしての GPT-2 を用いる場合、モデルクラスとしては GPT2LMHeadModel を用います。

```python
from transformers import GPT2Config, GPT2LMHeadModel

# 事前学習済みモデルとConfigをロード
config = GPT2Config.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name, config=config)

# pad_token_idを設定
model.config.pad_token_id = tokenizer.pad_token_id

print(f"モデル: {model_name}")
print(f"語彙サイズ: {config.vocab_size}")
print(f"最大シーケンス長: {config.n_positions}")
print(f"レイヤー数: {config.n_layer}")
print(f"隠れ層次元: {config.n_embd}")
```

Config には何が定義されているのでしょうか。確認してみましょう。

```python
print(config)
# GPT2Config {
#   "activation_function": "gelu_new",  # 活性化関数
#   "architectures": ["GPT2LMHeadModel"],
#   "bos_token_id": 1,
#   "eos_token_id": 2,
#   "n_ctx": 1024,
#   "n_embd": 1024,       # 埋め込みベクトルの次元
#   "n_head": 16,         # マルチヘッドアテンションのヘッド数
#   "n_layer": 24,        # レイヤー数
#   "n_positions": 1024,  # 最大シーケンス長
#   "vocab_size": 32000,
#   ...
# }
```

出力からわかる様に、以下のような内容が設定されています。

- どのアーキテクチャのための設定であるか
- 活性化関数には何を用いるか
- 埋め込みベクトルの次元はいくつか
- マルチヘッドアテンションのヘッド数はいくつか

GPT2LMHeadModel は、この設定で指定された数のヘッド数を用意したり、設定された次元の埋め込みベクトルを利用してモデルを初期化します。これは、第2章で Transformer モデルをインスタンス化する際に複数のハイパーパラメータを与えていたのと同様です。与えるべきハイパーパラメータが大量になってしまうため、Config という形にまとめていると考えましょう。

## 4.3.3 データセットの準備

4.1 節で作成した前処理済みのデータセットを読み込みます。HuggingFace Dataset 形式で保存されているため、`load_from_disk` で読み込むことができます。

```python
from datasets import load_from_disk

# 4.1節で作成した前処理済みデータを読み込み
dataset = load_from_disk("data/aozora_preprocessed")
print(f"読み込んだデータセット: {dataset}")
print(f"サンプル数: {len(dataset)}")

# train/eval に分割
eval_ratio = 0.01
split_ds = dataset.train_test_split(test_size=eval_ratio, seed=42)
train_dataset = split_ds['train']
eval_dataset = split_ds['test']

print(f"訓練データ: {len(train_dataset)} サンプル")
print(f"評価データ: {len(eval_dataset)} サンプル")
```

次に、データセットをトークン化します。datasets.Dataset クラスのインスタンスは、`map` を用いた要素の加工が可能です。`batched=True` を設定することでバッチ処理ができます。

```python
TEXT_COL = "text"
block_size = 512  # モデルの最大シーケンス長に合わせる

def tokenize_function(examples):
    return tokenizer(
        examples[TEXT_COL],
        truncation=True,
        max_length=block_size,
    )

tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])
tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])
```

## 4.3.4 Trainer を用いた学習

transformers では Trainer を用いることで学習コードをシンプルに記述できます。まず、DataCollatorForLanguageModeling を作成します。このクラスの役割は2.5.3項で示した collate_fn と同じで、学習時にデータをロードする際のデータを加工することです。具体的には、教師データやマスクを作成します。`mlm` 引数は、masked language model の略で、BERT などエンコーダベースのモデルを学習する場合などに True にします。今回はデコーダベースのモデルを学習するため False を設定してください。

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

TrainingArguments には学習時に与える設定を記述します。

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./models/rinna-gpt2-aozora-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=8,
    learning_rate=5e-5,
    weight_decay=0.1,
    warmup_steps=100,
    logging_steps=85,
    eval_strategy='steps',
    eval_steps=85,
    save_steps=85,
    save_total_limit=3,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=data_collator,
)

# 学習実行
trainer.train()

# モデルの保存
trainer.save_model("./models/rinna-gpt2-aozora-finetuned")
tokenizer.save_pretrained("./models/rinna-gpt2-aozora-finetuned")
```

## 4.3.5 学習済みモデルを用いた推論

学習したモデルを用いた推論は以下のように行います。推論時の引数は以下のものを設定しています。

- **max_new_tokens**: 生成するトークン数の上限
- **do_sample**: サンプリングを行うか
- **temperature**: サンプリング時の温度パラメータ
- **top_p**: nucleus sampling のパラメータ
- **repetition_penalty**: 同じトークンが繰り返されることへのペナルティ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 学習済みモデルの読み込み
model_dir = "./models/rinna-gpt2-aozora-finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(model_dir)

# デバイスの設定
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device).eval()

# 生成テスト
prompt = "吾輩は猫である。名前はまだ無い。"

inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)
inputs = {k: v.to(device) for k, v in inputs.items()}

with torch.no_grad():
    output = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.8,
        top_p=0.9,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)
```

入力テキストには文章の冒頭を与えましたが、その続きが生成されたのではないでしょうか。事前学習済みモデルは「次の単語を予測する」という学習を行っているため、文章の続きを生成します。第5章では、これに対してアラインメントを行うことで ChatGPT のような対話型の LLM を作成します。

## 4.3.6 補足: 分散学習のためのライブラリ

4.2 節では効率的な学習方法として 3D 並列化と ZeRO を紹介し、PyTorch だけを用いた実装をいくつか示しました。その際にも述べた通り、適切な並列化を自身で設計・実装するのは難しい場合が多いです。ありがたいことに、大規模言語モデルの学習においては、分散学習をサポートするリポジトリがいくつか存在します。ここでは代表的なものとして、以下の4つを紹介します。

1. **Megatron-LM**
Megatron-LM は NVIDIA から 2019 年に発表された並列化を駆使して開発された、パラメータ数が数十億スケールのLLM の名称です。現在は NVIDIA の管理するリポジトリとして存在しており、様々な Transformer ベースのモデルとその学習コードが管理されています。
2. **DeepSpeed**
DeepSpeed は Microsoft が管理する大規模な深層学習モデルの学習の実装を楽にするためのフレームワークです。3D 並列化や ZeRO による学習だけでなく、Mixture-of-Expert (MoE) と呼ばれる特殊なアーキテクチャや、推論の効率化、モデルサイズの圧縮など様々な機能が実装されています。
3. **Megatron-DeepSpeed**
Microsoft が Megatron-LM のコードをベースに DeepSpeed の機能を追加したリポジトリです。DeepSpeed を用いて効率的に学習するためのサンプルコードが管理されています。
4. **GPT-NeoX**
EleutherAI が管理する大規模言語モデルを学習するためのライブラリです。Megatron-DeepSpeed と同様に、Megatron-LM ベースのモデルを DeepSpeed を用いて学習できます。

本節では Trainer を用いたシンプルな学習を行いましたが、より大規模なモデルを学習する際にはこれらのライブラリを活用することで、効率的に学習を進めることができます。
