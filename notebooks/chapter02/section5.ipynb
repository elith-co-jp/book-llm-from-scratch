{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Drive のマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/drive\")\n",
    "drive_dir = Path(\"/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必要なパッケージのインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リポジトリのルートディレクトリを指定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = drive_dir / \"MyDrive/Colab Notebooks/book-llm-from-scratch-main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {root_dir.resolve().as_posix()}\n",
    "%pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5.1 クロスエントロピーによる学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "p1 = torch.tensor([1.0, 0.0, 0.0])\n",
    "p2 = torch.tensor([0.7, 0.2, 0.1])\n",
    "p3 = torch.tensor([0.1, 0.2, 0.7])\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "print(cross_entropy(p1, p2))\n",
    "print(cross_entropy(p1, p3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5.2 Padding マスク と Subsequent マスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def create_padding_mask(pad_id: int, batch_tokens: Tensor):\n",
    "    # batch_tokens.shape == (batch_size, sequence_length)\n",
    "    mask = batch_tokens == pad_id\n",
    "    mask = mask.unsqueeze(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パディングマスクの動作確認\n",
    "batch_size = 3\n",
    "sequence_length = 5\n",
    "tokens = torch.Tensor([[5, 3, 3, 0, 0], [1, 9, 4, 3, 1], [5, 3, 5, 1, 0]])\n",
    "padding_id = 0\n",
    "score = torch.randn(batch_size, sequence_length, sequence_length)\n",
    "mask = create_padding_mask(padding_id, tokens)\n",
    "print(mask.shape)  # torch.Size([3, 1, 5])\n",
    "masked_score = score.masked_fill(mask, float(\"-inf\"))\n",
    "print(masked_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsequent_mask(batch_tokens: Tensor):\n",
    "    sequence_len = batch_tokens.size(1)\n",
    "    mask = torch.triu(\n",
    "        torch.full((sequence_len, sequence_len), 1),\n",
    "        diagonal=1,\n",
    "    )\n",
    "    mask = mask == 1\n",
    "    mask = mask.unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後続マスクの動作確認\n",
    "batch_size = 3\n",
    "sequence_length = 5\n",
    "tokens = torch.randn(batch_size, sequence_length)\n",
    "score = torch.randn(batch_size, sequence_length, sequence_length)\n",
    "mask = create_subsequent_mask(tokens)\n",
    "print(mask.shape)  # torch.Size([3, 5, 5])\n",
    "masked_score = score.masked_fill(mask, float(\"-inf\"))\n",
    "print(masked_score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5.3 Transformer の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_dir = Path(\"small_parallel_enja\")\n",
    "if not data_dir.exists():\n",
    "    !git clone https://github.com/odashi/small_parallel_enja.git {data_dir}\n",
    "\n",
    "train_ja = data_dir / \"train.ja.000\"\n",
    "train_en = data_dir / \"train.en.000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def iter_corpus(\n",
    "    path: Path,\n",
    "    bos: str | None = \"<bos>\",\n",
    "    eos: str | None = \"<eos>\",\n",
    ") -> Iterator[list[str]]:\n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            if bos:\n",
    "                line = bos + \" \" + line\n",
    "            if eos:\n",
    "                line = line + \" \" + eos\n",
    "            yield line.split()\n",
    "\n",
    "\n",
    "train_tokens_ja = [tokens for tokens in iter_corpus(train_ja)]\n",
    "train_tokens_en = [tokens for tokens in iter_corpus(train_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tokens_ja[:3])\n",
    "print(train_tokens_en[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab_from_iterator(\n",
    "    iterator: Iterator[list[str]], specials: list[str]\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"語彙構築\"\"\"\n",
    "    counter = Counter()\n",
    "    for tokens in iterator:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # specials を先頭に配置\n",
    "    vocab = {token: idx for idx, token in enumerate(specials)}\n",
    "    idx = len(specials)\n",
    "    for token, _ in counter.most_common():\n",
    "        if token not in vocab:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab_ja_dict = build_vocab_from_iterator(\n",
    "    iterator=train_tokens_ja,\n",
    "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"],\n",
    ")\n",
    "\n",
    "vocab_en_dict = build_vocab_from_iterator(\n",
    "    iterator=train_tokens_en,\n",
    "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"語彙クラスの実装\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx: dict, default_index: int = 0):\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "        self.default_index = default_index\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_idx.get(token, self.default_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def set_default_index(self, index):\n",
    "        self.default_index = index\n",
    "\n",
    "    def get_itos(self):\n",
    "        \"\"\"index to string のリストを返す\"\"\"\n",
    "        return [self.idx_to_token[i] for i in range(len(self.idx_to_token))]\n",
    "\n",
    "\n",
    "vocab_ja = Vocab(vocab_ja_dict)\n",
    "vocab_ja.set_default_index(vocab_ja[\"<unk>\"])\n",
    "vocab_en = Vocab(vocab_en_dict)\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<unk>:\", vocab_ja[\"<unk>\"])\n",
    "tokens = [\"<bos>\", \"吾輩\", \"は\", \"猫\", \"で\", \"ある\", \"<eos>\"]\n",
    "for token in tokens:\n",
    "    print(vocab_ja[token], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def tokens_to_tensor(\n",
    "    tokens_list: list[list[str]], vocab: Vocab, padding_value: int\n",
    ") -> Tensor:\n",
    "    \"\"\"トークンリストをパディング済みテンソルに変換\"\"\"\n",
    "    sequences = [\n",
    "        torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n",
    "        for tokens in tokens_list\n",
    "    ]\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "\n",
    "def collate_fn(batch: list) -> tuple[Tensor, Tensor]:\n",
    "    src_texts, tgt_texts = [], []\n",
    "    for s, t in batch:\n",
    "        src_texts.append(s)\n",
    "        tgt_texts.append(t)\n",
    "\n",
    "    src_tensor = tokens_to_tensor(src_texts, vocab_ja, vocab_ja[\"<pad>\"])\n",
    "    tgt_tensor = tokens_to_tensor(tgt_texts, vocab_en, vocab_en[\"<pad>\"])\n",
    "\n",
    "    return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    list(zip(train_tokens_ja, train_tokens_en)),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from llm_from_scratch.transformer.transformer import Transformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, \"を使用\")\n",
    "\n",
    "embedding_dim = 512\n",
    "n_blocks = 6\n",
    "n_heads = 8\n",
    "expansion_rate = 1\n",
    "\n",
    "# 語彙数を取得\n",
    "src_vocab_size = len(vocab_ja)\n",
    "tgt_vocab_size = len(vocab_en)\n",
    "\n",
    "# 最も長い文章の長さを取得\n",
    "max_len_ja = len(max(train_tokens_ja, key=lambda x: len(x)))\n",
    "max_len_en = len(max(train_tokens_en, key=lambda x: len(x)))\n",
    "max_length = max(max_len_ja, max_len_en)\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    max_sequence_len=max_length,\n",
    "    d_model=embedding_dim,\n",
    "    n_blocks=n_blocks,\n",
    "    n_heads=n_heads,\n",
    "    d_k=embedding_dim,\n",
    "    d_v=embedding_dim,\n",
    "    d_ff=embedding_dim * expansion_rate,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "PAD_ID = vocab_ja[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)  # クロスエントロピー\n",
    "lr = 0.0001  # 学習率\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, log_interval: int = 10) -> list[float]:\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    for i, (src_texts, tgt_texts) in enumerate(train_loader):\n",
    "        # tgt の入力は最後の単語を除く\n",
    "        tgt_input = tgt_texts[:, :-1]\n",
    "        # tgt の出力は最初の単語を除く\n",
    "        tgt_output = tgt_texts[:, 1:]\n",
    "        src_padding_mask = create_padding_mask(PAD_ID, src_texts)\n",
    "        tgt_padding_mask = create_padding_mask(PAD_ID, tgt_input)\n",
    "        tgt_subsequent_mask = create_subsequent_mask(tgt_input)\n",
    "        tgt_mask = tgt_padding_mask + tgt_subsequent_mask\n",
    "        # Tensor のデバイスを設定\n",
    "        src_texts, tgt_input, tgt_output = (\n",
    "            src_texts.to(device),\n",
    "            tgt_input.to(device),\n",
    "            tgt_output.to(device),\n",
    "        )\n",
    "        src_padding_mask, tgt_mask = src_padding_mask.to(device), tgt_mask.to(device)\n",
    "\n",
    "        # モデル出力を取得\n",
    "        out = model(src_texts, tgt_input, src_padding_mask, tgt_mask, src_padding_mask)\n",
    "        # 出力と教師データを1次元に変換\n",
    "        out_flat = out.view(-1, out.size(-1))\n",
    "        tgt_flat = tgt_output.flatten()\n",
    "        # 誤差関数を計算\n",
    "        loss = criterion(out_flat, tgt_flat)\n",
    "        optimizer.zero_grad()\n",
    "        # 誤差逆伝播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(f\"step {i + 1}: train loss = {loss.item()}\")\n",
    "        loss_history.append(loss.item())\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_epochs = 20  # エポック数\n",
    "pbar = tqdm(total=n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(desc=\"Epoch\")\n",
    "    train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(model.state_dict(), \"transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "model.load_state_dict(torch.load(\"transformer.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<bos> 今日 の 天気 は 晴れ です 。 <eos>\"\n",
    "tokens = text.split()\n",
    "input_tokens = tokens_to_tensor([tokens], vocab_ja, vocab_ja[\"<pad>\"]).to(device)\n",
    "tgt_tokens = model.inference(\n",
    "    input_tokens, bos_token=vocab_en[\"<bos>\"], eos_token=vocab_en[\"<eos>\"]\n",
    ")\n",
    "itos = vocab_en.get_itos()\n",
    "text = \" \".join(itos[token_id] for token_id in tgt_tokens[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 発展的な生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def beam_search_inference(\n",
    "    model: nn.Module,\n",
    "    src: Tensor,\n",
    "    bos_token: int,\n",
    "    eos_token: int,\n",
    "    beam_width: int = 5,\n",
    "    max_length: int = 50,\n",
    ") -> Tensor:\n",
    "    device = src.device\n",
    "    encoder_output = model.encoder(src)\n",
    "\n",
    "    # 初期状態の作成\n",
    "    sequences = [[bos_token]]\n",
    "    scores = torch.zeros(1, device=device)\n",
    "    ended_seq_mask = [False]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            if ended_seq_mask[i]:\n",
    "                # 既に終了しているシーケンスはそのまま保持\n",
    "                all_candidates.append((scores[i], seq))\n",
    "                continue\n",
    "\n",
    "            tgt = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            decoder_output = model.decoder(tgt, encoder_output)\n",
    "            logits = model.linear(decoder_output[:, -1, :])\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            # 現在のスコアと次のトークンの確率を足して全候補を生成\n",
    "            for j in range(log_probs.size(1)):\n",
    "                candidate = seq + [j]\n",
    "                candidate_score = scores[i] + log_probs[0, j]\n",
    "                all_candidates.append((candidate_score, candidate))\n",
    "\n",
    "        # ビーム幅でソートしてトップ beam_width 個を選択\n",
    "        top_candidates = sorted(all_candidates, key=lambda tup: tup[0], reverse=True)[\n",
    "            :beam_width\n",
    "        ]\n",
    "        sequences = [x[1] for x in top_candidates]\n",
    "        scores = torch.tensor([x[0] for x in top_candidates], device=device)\n",
    "        ended_seq_mask = [seq[-1] == eos_token for seq in sequences]\n",
    "\n",
    "        # 全ての候補が終了トークンで終わっている場合、終了\n",
    "        if all(ended_seq_mask):\n",
    "            break\n",
    "\n",
    "    # スコアが最も高い候補を選択して出力\n",
    "    best_sequence = sequences[0]\n",
    "    return torch.tensor(best_sequence, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<bos> 今日 の 天気 は 晴れ です 。 <eos>\"\n",
    "tokens = text.split()\n",
    "input_tokens = tokens_to_tensor([tokens], vocab_ja, vocab_ja[\"<pad>\"]).to(device)\n",
    "tgt_tokens = beam_search_inference(\n",
    "    model, input_tokens, vocab_en[\"<bos>\"], vocab_en[\"<eos>\"], max_length=20\n",
    ")\n",
    "itos = vocab_en.get_itos()\n",
    "text = \" \".join(itos[token_id] for token_id in tgt_tokens)\n",
    "print(text)  # <bos> the weather is fine today . <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x: np.array, temperature: float) -> np.array:\n",
    "    x = x / temperature\n",
    "    x = np.exp(x)\n",
    "    return x / np.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GREEN = \"#75a075\"\n",
    "temperature = [0.1, 1.0, 2.0]\n",
    "fig, axes = plt.subplots(len(temperature), 1, figsize=(8, 10))\n",
    "score = np.random.rand(10)\n",
    "bins = np.arange(len(score))\n",
    "for t, ax in zip(temperature, axes):\n",
    "    prob = softmax(score, t)\n",
    "    # 確率のヒストグラムを描画\n",
    "    ax.bar(bins, prob, color=GREEN)\n",
    "    ax.set_title(f\"Temperature: {t}\")\n",
    "\n",
    "fig.savefig(\"softmax_temperature.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "温度パラメータを用いた推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "def temperature_inference(\n",
    "    model: nn.Module,\n",
    "    src: Tensor,\n",
    "    bos_token: int,\n",
    "    eos_token: int,\n",
    "    temperature: float = 1.0,\n",
    "    max_length: int = 50,\n",
    "):\n",
    "    tgt_tokens = torch.tensor([[bos_token]]).to(src.device)\n",
    "\n",
    "    encoder_output = model.encoder(src)\n",
    "    for _ in range(max_length):\n",
    "        decoder_output = model.decoder(tgt_tokens, encoder_output)\n",
    "        score = model.linear(decoder_output)\n",
    "        # 温度パラメータによる変換\n",
    "        score = score / temperature\n",
    "        porbability = F.softmax(score[0, -1], dim=-1)\n",
    "        # トークンをサンプリング\n",
    "        pred = torch.multinomial(porbability, 1).unsqueeze(0)\n",
    "        tgt_tokens = torch.cat((tgt_tokens, pred), axis=-1)\n",
    "        if pred[0, 0].item() == eos_token:\n",
    "            break\n",
    "\n",
    "    return tgt_tokens.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<bos> 今日 の 天気 は 晴れ です 。 <eos>\"\n",
    "tokens = text.split()\n",
    "input_tokens = tokens_to_tensor([tokens], vocab_ja, vocab_ja[\"<pad>\"]).to(device)\n",
    "tgt_tokens = temperature_inference(\n",
    "    model, input_tokens, vocab_en[\"<bos>\"], vocab_en[\"<eos>\"], max_length=20\n",
    ")\n",
    "itos = vocab_en.get_itos()\n",
    "text = \" \".join(itos[token_id] for token_id in tgt_tokens)\n",
    "print(text)  # <bos> the weather is fine today . <eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top-k サンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "def top_k_inference(\n",
    "    model: nn.Module,\n",
    "    src: Tensor,\n",
    "    bos_token: int,\n",
    "    eos_token: int,\n",
    "    temperature: float = 1.0,\n",
    "    k: float = 5,\n",
    "    max_length: int = 50,\n",
    "):\n",
    "    tgt_tokens = torch.tensor([[bos_token]]).to(src.device)\n",
    "\n",
    "    encoder_output = model.encoder(src)\n",
    "    for _ in range(max_length):\n",
    "        decoder_output = model.decoder(tgt_tokens, encoder_output)\n",
    "        score = model.linear(decoder_output)\n",
    "        # 温度パラメータによる変換\n",
    "        score = score / temperature\n",
    "        probability = F.softmax(score[0, -1], dim=-1)\n",
    "\n",
    "        # 確率の高い順にソートしたインデックスを取得\n",
    "        idx_sorted = torch.argsort(probability, descending=True)\n",
    "        # 上位k個のインデックスを取得\n",
    "        idx_k = idx_sorted[:k]\n",
    "        # 上位k個の確率を取得\n",
    "        p_k = probability[idx_k]\n",
    "        # 正規化\n",
    "        p_k /= torch.sum(p_k)\n",
    "        # トークンをサンプリング\n",
    "        pred = torch.multinomial(p_k, 1)\n",
    "        pred = idx_k[pred].unsqueeze(0)\n",
    "        tgt_tokens = torch.cat((tgt_tokens, pred), axis=-1)\n",
    "        if pred[0, 0].item() == eos_token:\n",
    "            break\n",
    "\n",
    "    return tgt_tokens.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<bos> 今日 の 天気 は 晴れ です 。 <eos>\"\n",
    "tokens = text.split()\n",
    "input_tokens = tokens_to_tensor([tokens], vocab_ja, vocab_ja[\"<pad>\"]).to(device)\n",
    "tgt_tokens = top_k_inference(\n",
    "    model, input_tokens, vocab_en[\"<bos>\"], vocab_en[\"<eos>\"], max_length=20\n",
    ")\n",
    "itos = vocab_en.get_itos()\n",
    "text = \" \".join(itos[token_id] for token_id in tgt_tokens)\n",
    "print(text)  # <bos> the weather is fine today . <eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top-p サンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "def top_p_inference(\n",
    "    model: nn.Module,\n",
    "    src: Tensor,\n",
    "    bos_token: int,\n",
    "    eos_token: int,\n",
    "    temperature: float = 1.0,\n",
    "    p: float = 0.8,\n",
    "    max_length: int = 50,\n",
    "):\n",
    "    tgt_tokens = torch.tensor([[bos_token]]).to(src.device)\n",
    "\n",
    "    encoder_output = model.encoder(src)\n",
    "    for _ in range(max_length):\n",
    "        decoder_output = model.decoder(tgt_tokens, encoder_output)\n",
    "        score = model.linear(decoder_output)\n",
    "        # 温度パラメータによる変換\n",
    "        score = score / temperature\n",
    "        porbability = F.softmax(score[0, -1], dim=-1)\n",
    "        idx_sorted = torch.argsort(porbability, descending=True)\n",
    "        p_sorted = porbability[idx_sorted]\n",
    "        # ソートされた確率の累積和を計算\n",
    "        p_cumsum = torch.cumsum(p_sorted, dim=-1)\n",
    "        # p を超える最初のインデックスを取得\n",
    "        idx = torch.sum(p_cumsum < p).item() + 1\n",
    "        # インデックスが範囲内に収まるように調整\n",
    "        idx = min(idx, len(p_cumsum) - 1)\n",
    "        # p を超えない範囲で上位の確率分布を取得\n",
    "        p_top = p_sorted[:idx]\n",
    "        # 正規化\n",
    "        p_top = p_top / torch.sum(p_top)\n",
    "        # トークンをサンプリング\n",
    "        pred = torch.multinomial(p_top, 1).unsqueeze(0)\n",
    "        pred = idx_sorted[pred]\n",
    "        tgt_tokens = torch.cat((tgt_tokens, pred), axis=-1)\n",
    "        if pred[0, 0].item() == eos_token:\n",
    "            break\n",
    "\n",
    "    return tgt_tokens.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<bos> 今日 の 天気 は 晴れ です 。 <eos>\"\n",
    "tokens = text.split()\n",
    "input_tokens = tokens_to_tensor([tokens], vocab_ja, vocab_ja[\"<pad>\"]).to(device)\n",
    "tgt_tokens = top_p_inference(\n",
    "    model, input_tokens, vocab_en[\"<bos>\"], vocab_en[\"<eos>\"], max_length=20\n",
    ")\n",
    "itos = vocab_en.get_itos()\n",
    "text = \" \".join(itos[token_id] for token_id in tgt_tokens)\n",
    "print(text)  # <bos> the weather is fine today . <eos>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
