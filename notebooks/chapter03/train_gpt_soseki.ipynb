{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 夏目漱石のテキストでGPT訓練\n\nこのノートブックでは、夏目漱石のテキストを使用してGPTモデルを訓練します。\nnanoGPTと同様のアプローチで、文字レベルの言語モデルを作成します。\n\n**Google Colabでの実行:** このノートブックはGoogle Colabで直接実行できます。必要なコードはすべてノートブック内に含まれており、追加のパッケージインストールは不要です。"
  },
  {
   "cell_type": "markdown",
   "source": "## GPU ランタイムの設定（Google Colab）\n\nこのノートブックではGPTモデルの訓練を行うため、**GPUランタイムの使用を推奨します**。\nGoogle Colabの無料版（T4 GPU）で十分に動作します。\n\n**設定手順：**\n1. メニューから「ランタイム」→「ランタイムのタイプを変更」を選択\n2. 「ハードウェア アクセラレータ」で「T4 GPU」を選択\n3. 「保存」をクリック\n\n> CPUでも実行可能ですが、訓練に大幅に時間がかかります。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport requests\nfrom bs4 import BeautifulSoup\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 夏目漱石のテキストダウンロード関数\n",
    "\n",
    "青空文庫から夏目漱石の「吾輩は猫である」をダウンロードし、テキストを前処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_soseki():\n",
    "    \"\"\"夏目漱石のテキストをダウンロード。\"\"\"\n",
    "    url = \"https://www.aozora.gr.jp/cards/000148/files/789_14547.html\"\n",
    "\n",
    "    # データディレクトリを作成\n",
    "    os.makedirs(\"data/soseki\", exist_ok=True)\n",
    "\n",
    "    # ダウンロードして処理\n",
    "    filepath = \"data/soseki/input.txt\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"夏目漱石のテキストをダウンロード中...\")\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'shift_jis'  # 青空文庫はShift_JIS\n",
    "\n",
    "        # BeautifulSoupでHTMLを解析\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 本文を抽出（青空文庫の構造に基づく）\n",
    "        main_text = soup.find('div', class_='main_text')\n",
    "        if main_text:\n",
    "            text = main_text.get_text()\n",
    "        else:\n",
    "            # フォールバック：bodyからテキストを抽出\n",
    "            text = soup.get_text()\n",
    "\n",
    "        # 青空文庫の注記や記号を除去\n",
    "        text = re.sub(r'［＃.*?］', '', text)  # 注記を除去\n",
    "        text = re.sub(r'《.*?》', '', text)    # ルビを除去\n",
    "        text = re.sub(r'｜', '', text)        # 縦線を除去\n",
    "        text = re.sub(r'　', ' ', text)       # 全角スペースを半角に\n",
    "        text = re.sub(r'\\n+', '\\n', text)     # 連続する改行を一つに\n",
    "        text = text.strip()\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"ダウンロード完了: {filepath}\")\n",
    "\n",
    "    # テキストを読み込み\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(f\"データセットサイズ: {len(text)} 文字\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取得\n",
    "\n",
    "夏目漱石のテキストをダウンロードして確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットをダウンロードして読み込み\n",
    "text = download_soseki()\n",
    "\n",
    "# テキストの一部を表示\n",
    "print(\"テキストの最初の500文字:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## トークナイザーの実装\n\n文字レベルのトークナイザーを実装します。テキスト中のユニークな文字を語彙とし、文字とIDの相互変換を行います。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleTokenizer:\n    \"\"\"文字レベルのテキスト処理用トークナイザ.\"\"\"\n\n    def __init__(self, text):\n        chars = sorted(list(set(text)))\n        self.vocab_size = len(chars)\n        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n        self.idx_to_char = {i: ch for ch, i in self.char_to_idx.items()}\n\n    def encode(self, text):\n        return [self.char_to_idx.get(ch, 0) for ch in text]\n\n    def decode(self, tokens):\n        return ''.join([self.idx_to_char.get(int(idx), '') for idx in tokens])\n\n\n# トークナイザーを作成\ntokenizer = SimpleTokenizer(text)\nprint(f\"語彙数: {tokenizer.vocab_size} ユニーク文字\")\n\n# 語彙の一部を表示\nprint(\"\\n語彙の一部:\")\nchars = list(tokenizer.char_to_idx.keys())[:20]\nprint(chars)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## データセットとデータローダーの実装\n\n自己回帰型の言語モデル学習のため、テキストを入力・ターゲットのペアに変換するデータセットクラスとデータローダーを実装します。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TextDataset(Dataset):\n    \"\"\"テキストデータセットクラス\"\"\"\n\n    def __init__(self, text, tokenizer, block_size=128):\n        self.tokenizer = tokenizer\n        self.block_size = block_size\n        self.tokens = tokenizer.encode(text)\n        print(f\"データセットサイズ: {len(self.tokens)} トークン\")\n\n    def __len__(self):\n        return len(self.tokens) - self.block_size\n\n    def __getitem__(self, idx):\n        # 入力とターゲットのペアを作成\n        chunk = self.tokens[idx:idx + self.block_size + 1]\n        x = torch.tensor(chunk[:-1], dtype=torch.long)\n        y = torch.tensor(chunk[1:], dtype=torch.long)\n        return x, y\n\n\ndef create_dataloaders(text, tokenizer, block_size=128, batch_size=64,\n                       train_split=0.9, num_workers=0):\n    \"\"\"学習用と検証用のデータローダーを作成する.\"\"\"\n    dataset = TextDataset(text, tokenizer, block_size)\n    n = len(dataset)\n    n_train = int(train_split * n)\n    n_val = n - n_train\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        dataset, [n_train, n_val]\n    )\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True,\n        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n    )\n    return train_loader, val_loader\n\n\n# データローダーを作成\ntrain_loader, val_loader = create_dataloaders(\n    text, tokenizer,\n    block_size=256,      # コンテキスト長\n    batch_size=64,       # バッチサイズ\n    train_split=0.9\n)\n\nprint(f\"訓練データバッチ数: {len(train_loader)}\")\nprint(f\"検証データバッチ数: {len(val_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## GPT-2標準設定の参考\n\n**GPT-2の各モデルサイズの仕様:**\n\n| モデル | パラメータ | n_embd | n_layer | n_head | vocab_size |\n|--------|------------|--------|---------|--------|------------|\n| Small  | 124M       | 768    | 12      | 12     | 50257      |\n| Medium | 355M       | 1024   | 24      | 16     | 50257      |\n| Large  | 774M       | 1280   | 36      | 20     | 50257      |\n| XL     | 1.5B       | 1600   | 48      | 25     | 50257      |\n\n**推奨設定:**\n- **リソース限定**: n_embd=512, n_layer=8, n_head=8 (~40M params)\n- **標準学習**: GPT-2 Small設定 (124M params)\n- **本格学習**: GPT-2 Medium以上\n\n下記では軽量な設定を使用します。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## アテンション機構（2章で実装したモジュールの再利用）\n\nGPTモデルは、2章で実装したマルチヘッドアテンション機構を再利用します。\nスケール内積アテンション → アテンションヘッド → マルチヘッドアテンションの順に定義します。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ScaledDotProductAttention(nn.Module):\n    \"\"\"スケール内積アテンション（2章で実装）\"\"\"\n\n    def forward(self, query, key, value, mask=None):\n        d_k = query.size(-1)\n        score = torch.bmm(query, key.transpose(1, 2)) / (d_k ** 0.5)\n        if mask is not None:\n            score = score.masked_fill(mask, float(\"-inf\"))\n        weight = torch.softmax(score, dim=-1)\n        output = torch.bmm(weight, value)\n        return output\n\n\nclass AttentionHead(nn.Module):\n    \"\"\"マルチヘッドアテンションの単一ヘッド（2章で実装）\"\"\"\n\n    def __init__(self, d_k, d_v, d_model):\n        super().__init__()\n        self.linear_q = nn.Linear(d_model, d_k)\n        self.linear_k = nn.Linear(d_model, d_k)\n        self.linear_v = nn.Linear(d_model, d_v)\n        self.attention = ScaledDotProductAttention()\n\n    def forward(self, query, key, value, mask=None):\n        query = self.linear_q(query)\n        key = self.linear_k(key)\n        value = self.linear_v(value)\n        return self.attention(query, key, value, mask=mask)\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"マルチヘッドアテンション（2章で実装）\"\"\"\n\n    def __init__(self, n_heads, d_k, d_v, d_model):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [AttentionHead(d_k, d_v, d_model) for _ in range(n_heads)]\n        )\n        self.linear_o = nn.Linear(n_heads * d_v, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        head_out = [head(query, key, value, mask=mask) for head in self.heads]\n        head_out = torch.cat(head_out, dim=-1)\n        return self.linear_o(head_out)\n\n\nprint(\"アテンション機構を定義しました\")"
  },
  {
   "cell_type": "markdown",
   "source": "## GPTモデルのアーキテクチャ\n\nGPT-2のアーキテクチャを実装します。2章のTransformerからの主な変更点：\n- **学習可能な位置埋め込み**: 固定の正弦波方式に代わり、`nn.Embedding`で実装\n- **Pre-Layer Normalization**: 各サブレイヤーの前に`nn.LayerNorm`を配置\n- **因果的マスク（Causal Mask）**: 未来のトークンへのアテンションを防止\n- **デコーダーのみ**: エンコーダーとソース・ターゲットアテンションを削除",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GPTMultiHeadAttention(nn.Module):\n    \"\"\"GPT用のマルチヘッドアテンション（causal mask付き）\"\"\"\n\n    def __init__(self, n_embd, n_head, dropout=0.1):\n        super().__init__()\n        assert n_embd % n_head == 0\n        self.n_head = n_head\n        self.n_embd = n_embd\n        d_k = d_v = n_embd // n_head\n        self.attention = MultiHeadAttention(n_head, d_k, d_v, n_embd)\n        self.resid_dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.size()\n        # causal mask を作成（未来のトークンへの注意を防ぐ）\n        causal_mask = torch.triu(\n            torch.ones(T, T, device=x.device), diagonal=1\n        ).bool()\n        causal_mask = causal_mask.unsqueeze(0).expand(B, -1, -1)\n        y = self.attention(x, x, x, mask=causal_mask)\n        y = self.resid_dropout(y)\n        return y\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformerブロック（Pre-LN方式）\"\"\"\n\n    def __init__(self, n_embd, n_head, dropout=0.1):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = GPTMultiHeadAttention(n_embd, n_head, dropout)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass GPT(nn.Module):\n    \"\"\"GPTモデルの基本構造\"\"\"\n\n    def __init__(self, vocab_size, n_embd=768, n_layer=12, n_head=12,\n                 block_size=1024, dropout=0.1):\n        super().__init__()\n        self.block_size = block_size\n        self.n_embd = n_embd\n        # トークンと位置の埋め込み\n        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding = nn.Embedding(block_size, n_embd)\n        self.drop = nn.Dropout(dropout)\n        # Transformerブロック（n_layer個）\n        self.blocks = nn.Sequential(*[\n            TransformerBlock(n_embd, n_head, dropout)\n            for _ in range(n_layer)\n        ])\n        # 最終層の正規化と出力層\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n        # 重みの初期化\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.ones_(module.weight)\n            torch.nn.init.zeros_(module.bias)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # トークンと位置の埋め込みを加算\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)\n        tok_emb = self.token_embedding(idx)\n        pos_emb = self.position_embedding(pos)\n        x = self.drop(tok_emb + pos_emb)\n        # Transformerブロックを通す\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        # 語彙への投影\n        logits = self.head(x)\n        # 損失の計算（学習時のみ）\n        loss = None\n        if targets is not None:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"テキストを自己回帰的に生成する.\"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n\nclass GPTConfig:\n    \"\"\"GPTモデルの設定クラス\"\"\"\n\n    def __init__(self, **kwargs):\n        self.vocab_size = kwargs.get('vocab_size', 50257)\n        self.n_embd = kwargs.get('n_embd', 768)\n        self.n_layer = kwargs.get('n_layer', 12)\n        self.n_head = kwargs.get('n_head', 12)\n        self.block_size = kwargs.get('block_size', 1024)\n        self.dropout = kwargs.get('dropout', 0.1)\n\n    def get_model_size(self):\n        \"\"\"モデルのパラメータ数を概算する（百万単位）.\"\"\"\n        params = self.vocab_size * self.n_embd\n        params += self.block_size * self.n_embd\n        params_per_block = (\n            self.n_head * (self.n_embd // self.n_head) * self.n_embd * 3 +\n            self.n_head * (self.n_embd // self.n_head) * self.n_embd +\n            self.n_embd * 4 * self.n_embd +\n            4 * self.n_embd * self.n_embd +\n            self.n_embd * 4\n        )\n        params += params_per_block * self.n_layer\n        params += self.n_embd * 2\n        params += self.n_embd * self.vocab_size\n        return params / 1e6\n\n\nprint(\"GPTモデルを定義しました\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## モデル設定とモデル作成\n\nnanoGPTのshakespeareと同様の設定でGPTモデルを作成します。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# モデル設定（nanoGPTのshakespeareと同様）\nconfig = GPTConfig(\n    vocab_size=tokenizer.vocab_size,\n    n_embd=384,          # 埋め込み次元\n    n_layer=6,           # レイヤー数\n    n_head=6,            # アテンションヘッド数\n    block_size=256,      # コンテキストウィンドウ\n    dropout=0.2\n)\n\nprint(f\"モデルサイズ: 約{config.get_model_size():.2f}Mパラメータ\")\n\n# モデルを作成\nmodel = GPT(\n    vocab_size=config.vocab_size,\n    n_embd=config.n_embd,\n    n_layer=config.n_layer,\n    n_head=config.n_head,\n    block_size=config.block_size,\n    dropout=config.dropout\n)\n\nprint(\"モデル作成完了\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## トレーナーの実装\n\nAdamWオプティマイザーと学習率スケジューリング（線形ウォームアップ＋コサイン減衰）を使用するトレーナーを実装します。Weight decayはバイアス項・LayerNorm・埋め込み層には適用しません。"
  },
  {
   "cell_type": "code",
   "source": "class GPTTrainer:\n    \"\"\"GPTモデル用のトレーナークラス.\"\"\"\n\n    def __init__(self, model, train_loader, val_loader,\n                 learning_rate=3e-4, weight_decay=0.1,\n                 warmup_steps=1000, max_steps=10000,\n                 grad_clip=1.0, device=None):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.max_steps = max_steps\n        self.warmup_steps = warmup_steps\n        self.grad_clip = grad_clip\n\n        if device is None:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            self.device = device\n        self.model = self.model.to(self.device)\n\n        self.optimizer = self._configure_optimizer(learning_rate, weight_decay)\n        self.scheduler = CosineAnnealingLR(\n            self.optimizer, T_max=max_steps - warmup_steps\n        )\n\n    def _configure_optimizer(self, learning_rate, weight_decay):\n        \"\"\"重み減衰付きAdamWオプティマイザを設定する.\"\"\"\n        decay_params = []\n        no_decay_params = []\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                if 'bias' in name or 'ln' in name or 'embedding' in name:\n                    no_decay_params.append(param)\n                else:\n                    decay_params.append(param)\n        optimizer_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': no_decay_params, 'weight_decay': 0.0}\n        ]\n        return AdamW(optimizer_groups, lr=learning_rate, betas=(0.9, 0.95))\n\n    def get_lr(self, step):\n        if step < self.warmup_steps:\n            return self.optimizer.param_groups[0]['lr'] * step / self.warmup_steps\n        return self.optimizer.param_groups[0]['lr']\n\n    @torch.no_grad()\n    def evaluate(self, max_batches=10):\n        \"\"\"検証セットでモデルを評価する.\"\"\"\n        self.model.eval()\n        losses = []\n        for i, (x, y) in enumerate(self.val_loader):\n            if i >= max_batches:\n                break\n            x, y = x.to(self.device), y.to(self.device)\n            _, loss = self.model(x, y)\n            losses.append(loss.item())\n        self.model.train()\n        return np.mean(losses) if losses else float('inf')\n\n    def train(self, log_interval=100, eval_interval=500):\n        \"\"\"メイン学習ループ.\"\"\"\n        self.model.train()\n        train_losses = []\n        val_losses = []\n        step = 0\n        epoch = 0\n\n        print(f\"学習デバイス: {self.device}\")\n        print(f\"モデルパラメータ数: {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M\")\n        start_time = time.time()\n\n        while step < self.max_steps:\n            epoch += 1\n            for x, y in self.train_loader:\n                if step >= self.max_steps:\n                    break\n                x, y = x.to(self.device), y.to(self.device)\n\n                # 順伝播・逆伝播\n                logits, loss = self.model(x, y)\n                self.optimizer.zero_grad()\n                loss.backward()\n                if self.grad_clip > 0:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n                self.optimizer.step()\n                train_losses.append(loss.item())\n\n                if step >= self.warmup_steps:\n                    self.scheduler.step()\n\n                if step % log_interval == 0:\n                    avg_loss = np.mean(train_losses[-log_interval:]) if len(train_losses) >= log_interval else loss.item()\n                    elapsed = time.time() - start_time\n                    print(f\"Step {step}/{self.max_steps} | Loss: {avg_loss:.4f} | \"\n                          f\"LR: {self.get_lr(step):.6f} | Time: {elapsed:.1f}s\")\n\n                if step % eval_interval == 0 and step > 0:\n                    val_loss = self.evaluate()\n                    val_losses.append(val_loss)\n                    print(f\"検証損失: {val_loss:.4f}\")\n\n                step += 1\n\n        print(f\"学習完了！総時間: {time.time() - start_time:.1f}s\")\n        return {'train_losses': train_losses, 'val_losses': val_losses}\n\n    def save_checkpoint(self, path):\n        \"\"\"モデルチェックポイントを保存する.\"\"\"\n        checkpoint = {\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }\n        torch.save(checkpoint, path)\n        print(f\"チェックポイントを{path}に保存しました\")\n\n\nprint(\"GPTTrainerを定義しました\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## トレーナーの設定\n\nモデルの訓練に必要なトレーナーを設定します。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーナーを作成\n",
    "trainer = GPTTrainer(\n",
    "    model, train_loader, val_loader,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    max_steps=5000,      # 5000ステップ訓練\n",
    "    grad_clip=1.0\n",
    ")\n",
    "\n",
    "print(\"トレーナー設定完了\")\n",
    "print(f\"デバイス: {trainer.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練\n",
    "\n",
    "GPTモデルを夏目漱石のテキストで訓練します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを訓練\n",
    "print(\"訓練開始...\")\n",
    "losses = trainer.train(log_interval=100, eval_interval=500)\n",
    "print(\"訓練完了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成のテスト\n",
    "\n",
    "訓練されたモデルを使用して、夏目漱石風のテキストを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"夏目漱石風のテキストを生成中...\")\nprint(\"=\" * 50)\n\nmodel.eval()\n\n# プロンプトで開始\nprompts = [\n    \"吾輩は\",\n    \"夏の\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nプロンプト: {prompt}\")\n    print(\"-\" * 30)\n    \n    # プロンプトをエンコード\n    prompt_tokens = torch.tensor(\n        tokenizer.encode(prompt),\n        dtype=torch.long\n    ).unsqueeze(0).to(trainer.device)\n    \n    # テキストを生成\n    with torch.no_grad():\n        generated = model.generate(\n            prompt_tokens,\n            max_new_tokens=200,\n            temperature=0.8,\n            top_k=40\n        )\n    \n    generated_text = tokenizer.decode(generated[0].cpu().numpy())\n    print(generated_text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存\n",
    "\n",
    "訓練されたモデルを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "checkpoint_path = \"models/soseki_gpt_checkpoint.pt\"\n",
    "trainer.save_checkpoint(checkpoint_path)\n",
    "print(f\"モデル保存: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練損失の可視化（オプション）\n",
    "\n",
    "訓練過程の損失をプロットして確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# 損失をプロット\nif losses:\n    train_losses = losses['train_losses']\n    val_losses = losses['val_losses']\n    \n    plt.figure(figsize=(12, 4))\n    \n    # 訓練損失\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses)\n    plt.title('Training Loss')\n    plt.xlabel('Step')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    \n    # 検証損失\n    if val_losses:\n        plt.subplot(1, 2, 2)\n        plt.plot(val_losses)\n        plt.title('Validation Loss')\n        plt.xlabel('Evaluation Step')\n        plt.ylabel('Loss')\n        plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"損失データが見つかりません\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは以下を実行しました：\n",
    "\n",
    "1. 夏目漱石のテキストを青空文庫からダウンロード\n",
    "2. テキストの前処理（注記・ルビの除去など）\n",
    "3. 文字レベルトークナイザーの作成\n",
    "4. データローダーの作成\n",
    "5. GPTモデルの設定と作成\n",
    "6. モデルの訓練\n",
    "7. テキスト生成のテスト\n",
    "8. モデルの保存\n",
    "\n",
    "訓練されたモデルは夏目漱石のスタイルを学習し、類似したテキストを生成できるようになります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}