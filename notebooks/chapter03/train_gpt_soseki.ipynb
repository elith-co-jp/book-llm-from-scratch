{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 夏目漱石のテキストでGPT訓練\n",
    "\n",
    "このノートブックでは、夏目漱石のテキストを使用してGPTモデルを訓練します。\n",
    "nanoGPTと同様のアプローチで、文字レベルの言語モデルを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from llm_from_scratch.gpt import (\n",
    "    GPT, GPTConfig, SimpleTokenizer,\n",
    "    create_dataloaders, GPTTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 夏目漱石のテキストダウンロード関数\n",
    "\n",
    "青空文庫から夏目漱石の「吾輩は猫である」をダウンロードし、テキストを前処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_soseki():\n",
    "    \"\"\"夏目漱石のテキストをダウンロード。\"\"\"\n",
    "    url = \"https://www.aozora.gr.jp/cards/000148/files/789_14547.html\"\n",
    "\n",
    "    # データディレクトリを作成\n",
    "    os.makedirs(\"data/soseki\", exist_ok=True)\n",
    "\n",
    "    # ダウンロードして処理\n",
    "    filepath = \"data/soseki/input.txt\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"夏目漱石のテキストをダウンロード中...\")\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'shift_jis'  # 青空文庫はShift_JIS\n",
    "\n",
    "        # BeautifulSoupでHTMLを解析\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 本文を抽出（青空文庫の構造に基づく）\n",
    "        main_text = soup.find('div', class_='main_text')\n",
    "        if main_text:\n",
    "            text = main_text.get_text()\n",
    "        else:\n",
    "            # フォールバック：bodyからテキストを抽出\n",
    "            text = soup.get_text()\n",
    "\n",
    "        # 青空文庫の注記や記号を除去\n",
    "        text = re.sub(r'［＃.*?］', '', text)  # 注記を除去\n",
    "        text = re.sub(r'《.*?》', '', text)    # ルビを除去\n",
    "        text = re.sub(r'｜', '', text)        # 縦線を除去\n",
    "        text = re.sub(r'　', ' ', text)       # 全角スペースを半角に\n",
    "        text = re.sub(r'\\n+', '\\n', text)     # 連続する改行を一つに\n",
    "        text = text.strip()\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"ダウンロード完了: {filepath}\")\n",
    "\n",
    "    # テキストを読み込み\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(f\"データセットサイズ: {len(text)} 文字\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取得\n",
    "\n",
    "夏目漱石のテキストをダウンロードして確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットをダウンロードして読み込み\n",
    "text = download_soseki()\n",
    "\n",
    "# テキストの一部を表示\n",
    "print(\"テキストの最初の500文字:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークナイザーの作成\n",
    "\n",
    "文字レベルのトークナイザーを作成し、語彙サイズを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字レベルのトークナイザーを作成\n",
    "tokenizer = SimpleTokenizer(text)\n",
    "print(f\"語彙数: {tokenizer.vocab_size} ユニーク文字\")\n",
    "\n",
    "# 語彙の一部を表示\n",
    "print(\"\\n語彙の一部:\")\n",
    "chars = list(tokenizer.char_to_idx.keys())[:20]\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダーの作成\n",
    "\n",
    "訓練用と検証用のデータローダーを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーを作成\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    text, tokenizer,\n",
    "    block_size=256,      # コンテキスト長\n",
    "    batch_size=64,       # バッチサイズ\n",
    "    train_split=0.9\n",
    ")\n",
    "\n",
    "print(f\"訓練データバッチ数: {len(train_loader)}\")\n",
    "print(f\"検証データバッチ数: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## GPT-2標準設定の参考\n\n**GPT-2の各モデルサイズの仕様:**\n\n| モデル | パラメータ | n_embd | n_layer | n_head | vocab_size |\n|--------|------------|--------|---------|--------|------------|\n| Small  | 124M       | 768    | 12      | 12     | 50257      |\n| Medium | 355M       | 1024   | 24      | 16     | 50257      |\n| Large  | 774M       | 1280   | 36      | 20     | 50257      |\n| XL     | 1.5B       | 1600   | 48      | 25     | 50257      |\n\n**推奨設定:**\n- **リソース限定**: n_embd=512, n_layer=8, n_head=8 (~40M params)\n- **標準学習**: GPT-2 Small設定 (124M params)\n- **本格学習**: GPT-2 Medium以上\n\n下記では軽量な設定を使用します。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル設定とモデル作成\n",
    "\n",
    "nanoGPTのshakespeareと同様の設定でGPTモデルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル設定（nanoGPTのshakespeareと同様）\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=384,          # 埋め込み次元\n",
    "    n_layer=6,           # レイヤー数\n",
    "    n_head=6,            # アテンションヘッド数\n",
    "    block_size=256,      # コンテキストウィンドウ\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(f\"モデルサイズ: 約{config.get_model_size():.2f}Mパラメータ\")\n",
    "\n",
    "# モデルを作成\n",
    "model = GPT(\n",
    "    vocab_size=config.vocab_size,\n",
    "    n_embd=config.n_embd,\n",
    "    n_layer=config.n_layer,\n",
    "    n_head=config.n_head,\n",
    "    block_size=config.block_size,\n",
    "    dropout=config.dropout\n",
    ")\n",
    "\n",
    "print(\"モデル作成完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーナーの設定\n",
    "\n",
    "モデルの訓練に必要なトレーナーを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーナーを作成\n",
    "trainer = GPTTrainer(\n",
    "    model, train_loader, val_loader,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    max_steps=5000,      # 5000ステップ訓練\n",
    "    grad_clip=1.0\n",
    ")\n",
    "\n",
    "print(\"トレーナー設定完了\")\n",
    "print(f\"デバイス: {trainer.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練\n",
    "\n",
    "GPTモデルを夏目漱石のテキストで訓練します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを訓練\n",
    "print(\"訓練開始...\")\n",
    "losses = trainer.train(log_interval=100, eval_interval=500)\n",
    "print(\"訓練完了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成のテスト\n",
    "\n",
    "訓練されたモデルを使用して、夏目漱石風のテキストを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"夏目漱石風のテキストを生成中...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# プロンプトで開始\n",
    "prompts = [\n",
    "    \"吾輩は猫である\",\n",
    "    \"明治\",\n",
    "    \"東京\",\n",
    "    \"先生\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nプロンプト: {prompt}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # プロンプトをエンコード\n",
    "    prompt_tokens = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(trainer.device)\n",
    "    \n",
    "    # テキストを生成\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            prompt_tokens,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.8,\n",
    "            top_k=40\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated[0].cpu().numpy())\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存\n",
    "\n",
    "訓練されたモデルを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "checkpoint_path = \"models/soseki_gpt_checkpoint.pt\"\n",
    "trainer.save_checkpoint(checkpoint_path)\n",
    "print(f\"モデル保存: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練損失の可視化（オプション）\n",
    "\n",
    "訓練過程の損失をプロットして確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 損失をプロット\n",
    "if losses:\n",
    "    train_losses = [loss['train_loss'] for loss in losses if 'train_loss' in loss]\n",
    "    val_losses = [loss['val_loss'] for loss in losses if 'val_loss' in loss]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 訓練損失\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 検証損失\n",
    "    if val_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(val_losses)\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Evaluation Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"損失データが見つかりません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは以下を実行しました：\n",
    "\n",
    "1. 夏目漱石のテキストを青空文庫からダウンロード\n",
    "2. テキストの前処理（注記・ルビの除去など）\n",
    "3. 文字レベルトークナイザーの作成\n",
    "4. データローダーの作成\n",
    "5. GPTモデルの設定と作成\n",
    "6. モデルの訓練\n",
    "7. テキスト生成のテスト\n",
    "8. モデルの保存\n",
    "\n",
    "訓練されたモデルは夏目漱石のスタイルを学習し、類似したテキストを生成できるようになります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}