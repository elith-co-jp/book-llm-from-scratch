{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 インストラクションチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 インストラクションチューニングの基礎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{ \"from\": \"human\", \"value\": \"冷たいシャワーを毎日浴びることの長所と短所について、短いパラグラフを書いてください。\" }, { \"from\": \"gpt\", \"value\": \"冷たいシャワーを毎日浴びることのメリットは、注意力を高め、気分を高揚させること、健康全般を改善すること、ガスや電気を使ってお湯を沸かさないので光熱費の節約になることです。一方、冷たいシャワーはあまり歓迎されるものではありません。特に寒い冬には、冷たいシャワーに浸かるという精神的な障壁を超えられず、失敗や失望を考えてしまうかもしれません。\" }]\n",
    "\n",
    "prompt_template_example = \"{from}: {value}\"\n",
    "instruction_text = \"\"\n",
    "for d in data[:-1]:\n",
    "    instruction_text += prompt_template_example.format(**d)\n",
    "response_text = prompt_template_example.format(**data[-1])\n",
    "\n",
    "print(\"指示部分:\")\n",
    "print(instruction_text)\n",
    "print(\"\\n応答部分:\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 インストラクションチューニングの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "print(ds[\"train\"]) \n",
    "# Dataset({\n",
    "#     features: ['instruction', 'context', 'response', 'category'],\n",
    "#     num_rows: 15011\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds[\"train\"].filter(lambda x: x[\"context\"] == \"\")\n",
    "print(f\"コンテキスト空のデータ: {ds_train.num_rows}\") # 10544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "### Question: {instruction}\n",
    "### Answer: {response}{eos_token}\"\"\"\n",
    "\n",
    "def format_input(example):\n",
    "    \"\"\"バッチ処理用のフォーマット関数\"\"\"\n",
    "    texts = []\n",
    "    for instruction, response in zip(example['instruction'], example['response']):\n",
    "        text = prompt_template.format(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            eos_token=tokenizer.eos_token\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "sample = ds_train[0]\n",
    "print(\"サンプルデータ:\")\n",
    "print(f\"  instruction: {sample['instruction'][:50]}...\")\n",
    "print(f\"  response: {sample['response'][:50]}...\")\n",
    "\n",
    "#  サンプルデータ:\n",
    "#  instruction: Which is a species of fish? Tope or Rope...\n",
    "#  response: Tope..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "print(f\"パラメータ数: {sum(p.numel() for p in model.parameters()):,}\") # パラメータ数: 124,439,808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "lengths = [len(tokenizer.encode(text)) for text in format_input(ds_train)]\n",
    "ax.hist(lengths, bins=200)\n",
    "ax.set_xlim(0, 1000)\n",
    "ax.set_xlabel(\"トークン数\")\n",
    "ax.set_ylabel(\"レコード数\")\n",
    "fig.savefig(\"./output/histogram.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "def token_length_filter(x):\n",
    "    text = prompt_template.format(\n",
    "        instruction=x[\"instruction\"],\n",
    "        response=x[\"response\"],\n",
    "        eos_token=tokenizer.eos_token\n",
    "    )\n",
    "    return len(tokenizer.encode(text)) <= max_length\n",
    "\n",
    "ds_train = ds_train.filter(token_length_filter)\n",
    "print(f\"トークン数フィルタ後: {ds_train.num_rows}\") # トークン数フィルタ後: 10400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_infer = \"\"\"\\\n",
    "### Question: {instruction}\n",
    "### Answer: \"\"\"\n",
    "response_template = \"### Answer:\"\n",
    "\n",
    "@torch.inference_mode()\n",
    "def inference(model, tokenizer, user_input):\n",
    "    prompt = prompt_template_infer.format(instruction=user_input)\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    response_start = generated_text.find(response_template) + len(response_template)\n",
    "    response_end = generated_text.find(tokenizer.eos_token, response_start)\n",
    "    if response_end == -1:\n",
    "        response_end = len(generated_text)\n",
    "    response = generated_text[response_start:response_end].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is the capital of Japan?\"\n",
    "]\n",
    "\n",
    "print(\"チューニング前の応答:\")\n",
    "print(\"=\"*80)\n",
    "before_responses = {}\n",
    "for question in test_questions:\n",
    "    response = inference(model, tokenizer, question)\n",
    "    before_responses[question] = response\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "config = SFTConfig(\n",
    "    output_dir='./output/sft_model',\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    max_seq_length=max_length,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=config,\n",
    "    train_dataset=ds_train,\n",
    "    formatting_func=format_input,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の実行\n",
    "\n",
    "以下のセルでインストラクションチューニングを実行します。\n",
    "\n",
    "> **注意**: GPUが必要です。GPUがない環境では学習セル（下記セル）をスキップし、「5.2.3 学習済みモデルによる推論」へ進んでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./output/sft_model\"\n",
    "trainer.train()\n",
    "trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 学習済みモデルによる推論\n",
    "\n",
    "学習をスキップした場合、または学習済みモデルで推論のみ行いたい場合は、以下のセルを実行してください。ローカルに重みがあればそれを使用し、なければ Hugging Face からダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# モデルパスの設定\n",
    "local_model_path = \"./output/sft_model\"\n",
    "hf_repo = \"elith/llm-book-models\"\n",
    "hf_subfolder = \"chapter05/sft_model\"\n",
    "\n",
    "# ローカルに重みがあればそれを使用、なければHFからダウンロード\n",
    "if os.path.exists(os.path.join(local_model_path, \"config.json\")):\n",
    "    print(f\"ローカルのモデルを使用: {local_model_path}\")\n",
    "    sft_model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "    sft_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "else:\n",
    "    print(f\"Hugging Faceからモデルをダウンロード: {hf_repo}/{hf_subfolder}\")\n",
    "    sft_model = AutoModelForCausalLM.from_pretrained(hf_repo, subfolder=hf_subfolder)\n",
    "    sft_tokenizer = AutoTokenizer.from_pretrained(hf_repo, subfolder=hf_subfolder)\n",
    "\n",
    "sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
    "\n",
    "# デバイス設定\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sft_model = sft_model.to(device)\n",
    "print(f\"デバイス: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTモデルによる推論\n",
    "test_questions = [\n",
    "    \"What is the capital of Japan?\"\n",
    "    ]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SFTモデルによる推論\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for question in test_questions:\n",
    "    response = inference(sft_model, sft_tokenizer, question)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
