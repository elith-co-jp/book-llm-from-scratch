{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092d937b",
   "metadata": {},
   "source": [
    "# 4.1 データセットと前処理 — ハードコーディング版\n",
    "\n",
    "このノートブックでは、第4.1節の内容に沿って、モデル非依存の前処理パイプラインをハードコーディングで実装します。\n",
    "- 文字列正規化（Unicode NFKC、空白・改行整形）\n",
    "- 簡易クリーニング（制御・ゼロ幅文字の除去）\n",
    "- 厳密重複除去（SHA1）\n",
    "- 長さ・低情報フィルタ\n",
    "- シャッフルと train/val 分割\n",
    "- TXT/JSONL/マニフェストの保存\n",
    "\n",
    "入出力パスや閾値は下のセルで固定値として定義しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ad04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# ハードコーディングされた設定\n",
    "INPUT_DIR = Path(\"data/raw\")  # 入力となるテキストファイルディレクトリ\n",
    "OUTPUT_DIR = Path(\"data/processed\")  # 出力先\n",
    "GLOB = \"*.txt\"  # 収集する拡張子\n",
    "TRAIN_RATIO = 0.98\n",
    "MIN_CHARS = 50\n",
    "MAX_CHARS = 0  # 0なら無効\n",
    "UNIQUE_CHAR_RATIO_MIN = 0.01  # 0なら無効\n",
    "SEED = 42\n",
    "\n",
    "RE_MULTISPACE = re.compile(r\"\\s+\")\n",
    "RE_MULTI_NL = re.compile(r\"\\n{2,}\")\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"基本的な正規化：\n",
    "    - 改行をLFへ統一\n",
    "    - Unicode NFKC\n",
    "    - 行内の空白連続を1つに、前後空白の削除\n",
    "    - 連続空行の圧縮\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            line = RE_MULTISPACE.sub(\" \", line)\n",
    "        lines.append(line)\n",
    "    text = \"\\n\".join(lines)\n",
    "    text = RE_MULTI_NL.sub(\"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def simple_clean_hooks(text: str) -> str:\n",
    "    \"\"\"簡易クリーニング：ゼロ幅文字や制御文字の除去\n",
    "    （必要に応じてルールを追加）\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def exact_deduplicate(docs: List[str]) -> List[str]:\n",
    "    \"\"\"SHA1による文書単位の厳密重複除去\"\"\"\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for d in docs:\n",
    "        key = hashlib.sha1(d.encode(\"utf-8\")).hexdigest()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        uniq.append(d)\n",
    "    return uniq\n",
    "\n",
    "def length_filter(docs: List[str], min_chars: int | None, max_chars: int | None, unique_char_ratio_min: float | None):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        n = len(d)\n",
    "        if min_chars is not None and n < min_chars:\n",
    "            continue\n",
    "        if max_chars is not None and n > max_chars:\n",
    "            continue\n",
    "        if unique_char_ratio_min is not None and n > 0:\n",
    "            ratio = len(set(d)) / n\n",
    "            if ratio < unique_char_ratio_min:\n",
    "                continue\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "def shuffle_and_split(docs: List[str], train_ratio: float, seed: int):\n",
    "    idx = list(range(len(docs)))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(idx)\n",
    "    cut = int(len(idx) * train_ratio)\n",
    "    train = [docs[i] for i in idx[:cut]]\n",
    "    val = [docs[i] for i in idx[cut:]]\n",
    "    return train, val\n",
    "\n",
    "def save_text(path: Path, docs: List[str]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for d in docs:\n",
    "            f.write(d.strip())\n",
    "            f.write(\"\\n\\n\")  # 空行区切り\n",
    "\n",
    "def save_jsonl(path: Path, docs: List[str]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for d in docs:\n",
    "            f.write(json.dumps({\"text\": d}, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def write_manifest(path: Path, manifest: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ab05b",
   "metadata": {},
   "source": [
    "## 入力収集\n",
    "`INPUT_DIR` 以下から `GLOB` に一致するテキストを再帰的に収集します。各ファイルを1文書として扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24601a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(p for p in INPUT_DIR.rglob(GLOB) if p.is_file())\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No input files under {INPUT_DIR} matching {GLOB}\")\n",
    "len(files), files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d078b",
   "metadata": {},
   "source": [
    "## 正規化・クリーニング\n",
    "UTF-8として読み込み、NFKC正規化→簡易クレンジング→空白・改行整形を適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = []\n",
    "for p in files:\n",
    "    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = normalize_text(simple_clean_hooks(text))\n",
    "    if text:\n",
    "        raw_docs.append(text)\n",
    "len(raw_docs), sum(len(d) for d in raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50006d",
   "metadata": {},
   "source": [
    "## 重複除去（厳密）\n",
    "SHA1キーで厳密な重複を除去します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8febc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_docs = exact_deduplicate(raw_docs)\n",
    "len(dedup_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66bff7",
   "metadata": {},
   "source": [
    "## フィルタリング\n",
    "- `MIN_CHARS` 未満を除外\n",
    "- `MAX_CHARS` が正なら上限適用\n",
    "- `UNIQUE_CHAR_RATIO_MIN` が正ならユニーク文字比の閾値を適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558515fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chars = MAX_CHARS if MAX_CHARS and MAX_CHARS > 0 else None\n",
    "ucrm = UNIQUE_CHAR_RATIO_MIN if UNIQUE_CHAR_RATIO_MIN and UNIQUE_CHAR_RATIO_MIN > 0 else None\n",
    "filtered_docs = length_filter(dedup_docs, MIN_CHARS, max_chars, ucrm)\n",
    "len(filtered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298cf0de",
   "metadata": {},
   "source": [
    "## 分割（train/val）\n",
    "シャッフルの上、`TRAIN_RATIO` で分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00187c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, val_docs = shuffle_and_split(filtered_docs, TRAIN_RATIO, SEED)\n",
    "len(train_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf97e5c",
   "metadata": {},
   "source": [
    "## 保存（TXT/JSONL/manifest）\n",
    "各文書はTXTでは空行区切り、JSONLでは1行1文書(`{text: ...}`)で保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_text(OUTPUT_DIR / \"train.txt\", train_docs)\n",
    "save_text(OUTPUT_DIR / \"val.txt\", val_docs)\n",
    "save_jsonl(OUTPUT_DIR / \"train.jsonl\", train_docs)\n",
    "save_jsonl(OUTPUT_DIR / \"val.jsonl\", val_docs)\n",
    "manifest = {\n",
    "    'total_docs': len(raw_docs),\n",
    "    'total_chars': int(sum(len(d) for d in raw_docs)),\n",
    "    'after_dedup_docs': len(dedup_docs),\n",
    "    'after_filter_docs': len(filtered_docs),\n",
    "    'train_docs': len(train_docs),\n",
    "    'val_docs': len(val_docs),\n",
    "    'params': {\n",
    "        'train_ratio': TRAIN_RATIO,\n",
    "        'min_chars': MIN_CHARS,\n",
    "        'max_chars': MAX_CHARS,\n",
    "        'unique_char_ratio_min': UNIQUE_CHAR_RATIO_MIN,\n",
    "        'seed': SEED,\n",
    "        'glob': GLOB,\n",
    "    },\n",
    "}\n",
    "write_manifest(OUTPUT_DIR / \"manifest.json\", manifest)\n",
    "manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb7890",
   "metadata": {},
   "source": [
    "## プレビュー\n",
    "前処理済みの先頭の文書を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86887a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_docs[0][:1000] if train_docs else '(no docs)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
