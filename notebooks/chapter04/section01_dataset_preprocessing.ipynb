{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 データセットと前処理 — ハードコーディング版\n",
    "\n",
    "このノートブックでは、第4.1節の内容に沿って、モデル非依存の前処理パイプラインをハードコーディングで実装します。\n",
    "- データ取得（Hugging Face datasets: globis-university/aozorabunko-clean の train）\n",
    "- 文字列正規化（Unicode NFKC、空白・改行整形）\n",
    "- 簡易クリーニング（制御・ゼロ幅文字の除去）\n",
    "- 厳密重複除去（SHA1）\n",
    "- 長さ・低情報フィルタ\n",
    "- シャッフルと train/val 分割\n",
    "- TXT/JSONL/マニフェストの保存（出力先: data/aozora）\n",
    "なお、トークナイザの学習・保存は4.3節で実施します。\n",
    "\n",
    "入出力パスや閾値は下のセルで固定値として定義しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84af5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# ハードコーディングされた設定\n",
    "OUTPUT_DIR = Path('data')  # 出力先（HFデータからの前処理成果物）\n",
    "TRAIN_RATIO = 0.98\n",
    "MIN_CHARS = 50\n",
    "MAX_CHARS = 0  # 0なら無効\n",
    "UNIQUE_CHAR_RATIO_MIN = 0.01  # 0なら無効\n",
    "SEED = 42\n",
    "\n",
    "RE_MULTISPACE = re.compile(r'\\s+')\n",
    "RE_MULTI_NL = re.compile(r'\\n{2,}')\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"基本的な正規化：\n",
    "    - 改行をLFへ統一\n",
    "    - Unicode NFKC\n",
    "    - 行内の空白連続を1つに、前後空白の削除\n",
    "    - 連続空行の圧縮\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    lines = []\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            line = RE_MULTISPACE.sub(' ', line)\n",
    "        lines.append(line)\n",
    "    text = '\\n'.join(lines)\n",
    "    text = RE_MULTI_NL.sub('\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def simple_clean_hooks(text: str) -> str:\n",
    "    \"\"\"簡易クリーニング：ゼロ幅文字や制御文字の除去（必要に応じてルールを追加）\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', text)\n",
    "    return text\n",
    "\n",
    "def exact_deduplicate(docs: List[str]) -> List[str]:\n",
    "    \"\"\"SHA1による文書単位の厳密重複除去\"\"\"\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for d in docs:\n",
    "        key = hashlib.sha1(d.encode('utf-8')).hexdigest()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        uniq.append(d)\n",
    "    return uniq\n",
    "\n",
    "def length_filter(docs: List[str], min_chars: int | None, max_chars: int | None, unique_char_ratio_min: float | None):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        n = len(d)\n",
    "        if min_chars is not None and n < min_chars:\n",
    "            continue\n",
    "        if max_chars is not None and n > max_chars:\n",
    "            continue\n",
    "        if unique_char_ratio_min is not None and n > 0:\n",
    "            ratio = len(set(d)) / n\n",
    "            if ratio < unique_char_ratio_min:\n",
    "                continue\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "def shuffle_and_split(docs: List[str], train_ratio: float, seed: int):\n",
    "    idx = list(range(len(docs)))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(idx)\n",
    "    cut = int(len(idx) * train_ratio)\n",
    "    train = [docs[i] for i in idx[:cut]]\n",
    "    val = [docs[i] for i in idx[cut:]]\n",
    "    return train, val\n",
    "\n",
    "def save_text(path: Path, docs: List[str]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open('w', encoding='utf-8') as f:\n",
    "        for d in docs:\n",
    "            f.write(d.strip())\n",
    "            f.write('\\n\\n')  # 空行区切り\n",
    "\n",
    "def save_jsonl(path: Path, docs: List[str]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open('w', encoding='utf-8') as f:\n",
    "        for d in docs:\n",
    "            f.write(json.dumps({'text': d}, ensure_ascii=False))\n",
    "            f.write('\\n')\n",
    "\n",
    "def write_manifest(path: Path, manifest: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open('w', encoding='utf-8') as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743cbca2",
   "metadata": {},
   "source": [
    "## データ取得（Hugging Face datasets）\n",
    "`globis-university/aozorabunko-clean` の train split を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c2be53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16951,\n",
       " '深いおどろきにうたれて、\\n名高いウェストミンスターに\\n真鍮や石の記念碑となって\\nすべての王侯貴族が集まっているのをみれば、\\n今はさげすみも、ほこりも、見栄もない')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "TEXT_COL = 'text'\n",
    "ds = load_dataset('globis-university/aozorabunko-clean')\n",
    "texts = ds['train'][TEXT_COL]\n",
    "len(texts), (texts[0][:80] if texts else '(no text)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36353fdd",
   "metadata": {},
   "source": [
    "## 正規化・クリーニング\n",
    "NFKC正規化→簡易クレンジング→空白・改行整形を適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c7c145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16950, 229433847)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs = []\n",
    "for t in texts:\n",
    "    text = normalize_text(simple_clean_hooks(t))\n",
    "    if text:\n",
    "        raw_docs.append(text)\n",
    "len(raw_docs), sum(len(d) for d in raw_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b2f8f",
   "metadata": {},
   "source": [
    "## 重複除去（厳密）\n",
    "SHA1キーで厳密な重複を除去します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e918d838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16950"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup_docs = exact_deduplicate(raw_docs)\n",
    "len(dedup_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6160f1b",
   "metadata": {},
   "source": [
    "## フィルタリング\n",
    "- `MIN_CHARS` 未満を除外\n",
    "- `MAX_CHARS` が正なら上限適用\n",
    "- `UNIQUE_CHAR_RATIO_MIN` が正ならユニーク文字比の閾値を適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "362f3d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16799"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_chars = MAX_CHARS if MAX_CHARS and MAX_CHARS > 0 else None\n",
    "ucrm = UNIQUE_CHAR_RATIO_MIN if UNIQUE_CHAR_RATIO_MIN and UNIQUE_CHAR_RATIO_MIN > 0 else None\n",
    "filtered_docs = length_filter(dedup_docs, MIN_CHARS, max_chars, ucrm)\n",
    "len(filtered_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ad7b0",
   "metadata": {},
   "source": [
    "## 分割（train/val）\n",
    "シャッフルの上、`TRAIN_RATIO` で分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32a57835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16463, 336)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs, val_docs = shuffle_and_split(filtered_docs, TRAIN_RATIO, SEED)\n",
    "len(train_docs), len(val_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0792d",
   "metadata": {},
   "source": [
    "## 保存（TXT/JSONL/manifest）\n",
    "各文書はTXTでは空行区切り、JSONLでは1行1文書（{text: ...}）で保存します。\n",
    "保存先は data/ です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f50c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_docs': 16950,\n",
       " 'total_chars': 229433847,\n",
       " 'after_dedup_docs': 16950,\n",
       " 'after_filter_docs': 16799,\n",
       " 'train_docs': 16463,\n",
       " 'val_docs': 336,\n",
       " 'params': {'train_ratio': 0.98,\n",
       "  'min_chars': 50,\n",
       "  'max_chars': 0,\n",
       "  'unique_char_ratio_min': 0.01,\n",
       "  'seed': 42}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_text(OUTPUT_DIR / 'train.txt', train_docs)\n",
    "save_text(OUTPUT_DIR / 'val.txt', val_docs)\n",
    "save_jsonl(OUTPUT_DIR / 'train.jsonl', train_docs)\n",
    "save_jsonl(OUTPUT_DIR / 'val.jsonl', val_docs)\n",
    "manifest = {\n",
    "    'total_docs': len(raw_docs),\n",
    "    'total_chars': int(sum(len(d) for d in raw_docs)),\n",
    "    'after_dedup_docs': len(dedup_docs),\n",
    "    'after_filter_docs': len(filtered_docs),\n",
    "    'train_docs': len(train_docs),\n",
    "    'val_docs': len(val_docs),\n",
    "    'params': {\n",
    "        'train_ratio': TRAIN_RATIO,\n",
    "        'min_chars': MIN_CHARS,\n",
    "        'max_chars': MAX_CHARS,\n",
    "        'unique_char_ratio_min': UNIQUE_CHAR_RATIO_MIN,\n",
    "        'seed': SEED,\n",
    "    },\n",
    "}\n",
    "write_manifest(OUTPUT_DIR / 'manifest.json', manifest)\n",
    "manifest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979628bd",
   "metadata": {},
   "source": [
    "## プレビュー\n",
    "前処理済みの先頭の文書を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e7916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "改造社の古木鉄太郎君の言ふには、「短歌は将来の文芸からとり残されるかどうか?」に就き、僕にも何か言へとのことである。僕は作歌上の素人たる故、再三古木君に断つたところ、素人なればこそ尋ねに来たと言ふ、即ちやむを得ずペンを執り、原稿用紙に向つて見るに、とり残されさうな気もして来れば、とり残されぬらしい気もして来る。\n",
      "まづ明治大正の間のやうに偉い歌よみが沢山ゐれば、とり残したくともとり残されぬであらう。そこで将来も偉い詩人が生まれ、その詩人の感情を盛るのに短歌の形式を用ふるとすれば、やはりとり残されぬのに相違ない。するととり残されるかとり残されぬかを決するものは未だ生まれざる大詩人が短歌の形式を用ふるかどうかである。\n",
      "偉い詩人が生まれるかどうかは誰も判然とは保証出来ぬ。しかしその又偉い詩人が短歌の形式を用ふるかどうかは幾分か見当のつかぬこともない。尤も僕等が何かの拍子に四つ這ひになつて見たいやうに、未だ生まれざる大詩人も何かの拍子に短歌の形式を用ふる気もちになるかも知れぬ。しかしそれは例外とし、まづ一般に短歌の形式が将来の詩人の感情を盛るに足るかどうかは考へられぬ筈である。\n",
      "然るに元来短歌なるものは格別他の抒情詩と変りはない。変りのあるのは三十一文字に限られてゐる形式ばかりである。若し三十一文字と云ふ形式に限られてゐる為に、その又形式に纏綿した或短歌的情調の為に盛ることは出来ぬと云ふならば、それは明治大正の間の歌よみの仕事を無視したものであらう。たとへば斎藤氏や北原氏の歌は前人の少しも盛らなかつた感情を盛つてゐる筈である。しかし更に懐疑的になれば、明治大正の間の歌よみの短歌も或は猪口でシロツプを嘗めてゐると言はれるかも知れぬ。かう云ふ問題になつて来ると、素人の僕には見当がつかない。唯僕に言はせれば、たとへば斎藤氏や北原氏の短歌に或は猪口でシロツプを嘗めてゐるものがあるとしても、その又猪口の中のシロツプも愛するに足ると思ふだけである。\n",
      "尤も物盛なれば必ず衰ふるは天命なれば、余り明治大正の間に偉い歌よみが出過ぎた為にそれ等の人人の耄碌したり死んでしまつたりした後の短歌は月並みになつてしまふかも知れぬ。それを将来の文芸からとり残されると云ふ意味に解釈すれば、或はとり残されると云ふ意味に解釈すれば、或はとり残されることもあるであらう。これは前にも書いたやうに作歌上の素人談義たるの\n"
     ]
    }
   ],
   "source": [
    "print(train_docs[0][:1000] if train_docs else '(no docs)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
