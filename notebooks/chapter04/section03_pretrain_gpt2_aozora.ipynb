{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.3 事前学習: 4.1前処理 + Tokenizer学習 + GPT-2事前学習\n",
        "\n",
        "このノートブックは、4.1節の前処理（正規化→連結→チャンク化）を適用したうえで、4.3節で\n",
        "Byte-level BPE のトークナイザを学習し、Hugging Face Transformers の GPT-2 を用いて\n",
        "Causal Language Modeling の事前学習を行います。\n",
        "\n",
        "- データ: `globis-university/aozorabunko-clean`（train split）\n",
        "- トークナイザ: ByteLevel BPE（train のみで学習）\n",
        "- モデル: GPT-2（ランダム初期化、config は語彙サイズとコンテキスト長に合わせて作成）\n",
        "\n",
        "注意: 大規模学習には時間とGPUが必要です。まずは小さな `max_steps` で動作確認してから、\n",
        "徐々にスケールさせてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab88d4c0",
      "metadata": {},
      "source": [
        "## インポートと設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52106829",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/home/akira-nagasawa/book-llm-from-scratch/data/processed/tokenizer',\n",
              " '/home/akira-nagasawa/book-llm-from-scratch/models/gpt2-aozora')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import Iterable, Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "try:\n",
        "    import neologdn  # 日本語用正規化（任意）\n",
        "except Exception:\n",
        "    neologdn = None\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import (\n",
        "    PreTrainedTokenizerFast,\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "TEXT_COL = \"text\"\n",
        "SEP = \"\\n\\n<|doc|>\\n\\n\"\n",
        "\n",
        "# パラメータ（必要に応じて変更）\n",
        "block_size = 512            # 例: 512/1024/2048\n",
        "vocab_size = 30000          # 例: 30k/50k\n",
        "train_split = 'train'\n",
        "eval_ratio = 0.01\n",
        "per_device_train_batch_size = 2\n",
        "gradient_accumulation_steps = 8\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.1\n",
        "warmup_steps = 100\n",
        "max_steps = 200              # デモ用に小さめ（本番は増やす）\n",
        "logging_steps = 20\n",
        "\n",
        "# 保存先（ノートブック相対パス -> リポジトリ直下に配置）\n",
        "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "tokenizer_dir = os.path.join(REPO_ROOT, 'data', 'processed', 'tokenizer')\n",
        "output_dir = os.path.join(REPO_ROOT, 'models', 'gpt2-aozora')\n",
        "os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "tokenizer_dir, output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6511272",
      "metadata": {},
      "source": [
        "## 4.1: 前処理（正規化→連結→チャンク化）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "040236cf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(385410, 3893)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def normalize_ja(text: str) -> str:\n",
        "    \"\"\"4.1節の方針に沿った簡易正規化。neologdn があれば利用。\n",
        "    - 全角英数字を半角へ\n",
        "    - クオート半角化\n",
        "    - 三点リーダ統一\n",
        "    - 連続空白の圧縮\n",
        "    \"\"\"\n",
        "    if neologdn is not None:\n",
        "        text = neologdn.normalize(text)\n",
        "\n",
        "    # 全角英数 → 半角（A-Z/a-z/0-9）\n",
        "    def z2h_alnum(match):\n",
        "        ch = match.group(0)\n",
        "        return chr(ord(ch) - 0xFEE0)\n",
        "    text = re.sub(r'[Ａ-Ｚａ-ｚ０-９]', z2h_alnum, text)\n",
        "\n",
        "    # クオートの半角化\n",
        "    text = text.replace('＂', chr(34)).replace('＇', chr(39))\n",
        "\n",
        "    # 三点リーダの統一\n",
        "    text = text.replace('･･･', '…').replace('・・・', '…')\n",
        "\n",
        "    # 連続空白の圧縮\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def apply_normalize(batch):\n",
        "    texts = batch[TEXT_COL]\n",
        "    return {TEXT_COL: [normalize_ja(t) for t in texts]}\n",
        "\n",
        "def make_long_string(dsdict, split: str, col: str = TEXT_COL) -> str:\n",
        "    texts = dsdict[split][col]\n",
        "    return SEP.join(texts)\n",
        "\n",
        "def chunk_text(s: str, size: int) -> Iterable[str]:\n",
        "    for i in range(0, len(s), size):\n",
        "        yield s[i : i + size]\n",
        "\n",
        "def load_texts_local() -> list[str]:\n",
        "    import json, os\n",
        "    base = os.path.join(REPO_ROOT, 'data')\n",
        "    jl = os.path.join(base, 'train.jsonl')\n",
        "    txt = os.path.join(base, 'train.txt')\n",
        "    if os.path.exists(jl):\n",
        "        with open(jl, 'r', encoding='utf-8') as f:\n",
        "            return [json.loads(line)['text'] for line in f if line.strip()]\n",
        "    if os.path.exists(txt):\n",
        "        with open(txt, 'r', encoding='utf-8') as f:\n",
        "            raw = f.read()\n",
        "        return [s.strip() for s in raw.split('\\n\\n') if s.strip()]\n",
        "    raise FileNotFoundError(f'Local data not found under {base}. Prepare data/train.jsonl or data/train.txt in 4.1.')\n",
        "\n",
        "# ローカルデータを取得し、連結→チャンク化\n",
        "texts = load_texts_local()\n",
        "\n",
        "\n",
        "# ローカル or HF からテキスト一覧を取得し、連結→チャンク化\n",
        "texts = load_texts_local_or_hf()\n",
        "long_text = SEP.join(texts)\n",
        "chunks = list(chunk_text(long_text, block_size))\n",
        "train_text_ds = Dataset.from_dict({TEXT_COL: chunks})\n",
        "\n",
        "# eval用にごく一部を分割\n",
        "eval_size = max(1, int(len(train_text_ds) * eval_ratio))\n",
        "split = train_text_ds.train_test_split(test_size=eval_size)\n",
        "train_text_ds, eval_text_ds = split['train'], split['test']\n",
        "len(train_text_ds), len(eval_text_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cce66ec",
      "metadata": {},
      "source": [
        "## 4.3: Byte-level BPE トークナイザの学習と保存\n",
        "時間かかるので目安を書く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24153a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_bytelevel_bpe(iterator: Iterable[str], vocab_size: int, special_tokens: Optional[list[str]] = None):\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer.train_from_iterator(\n",
        "        iterator,\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=special_tokens or ['[PAD]', '[BOS]', '[EOS]', '[UNK]'],\n",
        "        show_progress=True,\n",
        "    )\n",
        "    return tokenizer\n",
        "\n",
        "def wrap_transformers_tokenizer(bytelevel_tokenizer, save_dir: str) -> PreTrainedTokenizerFast:\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    bytelevel_tokenizer.save_model(save_dir)\n",
        "    fast = PreTrainedTokenizerFast(\n",
        "        tokenizer_file=None,\n",
        "        vocab_file=os.path.join(save_dir, 'vocab.json'),\n",
        "        merges_file=os.path.join(save_dir, 'merges.txt'),\n",
        "        bos_token='[BOS]',\n",
        "        eos_token='[EOS]',\n",
        "        pad_token='[PAD]',\n",
        "        unk_token='[UNK]',\n",
        "    )\n",
        "    fast.save_pretrained(save_dir)\n",
        "    return fast\n",
        "\n",
        "byte_bpe = train_bytelevel_bpe(\n",
        "    iterator=(ex for ex in train_text_ds[TEXT_COL]),\n",
        "    vocab_size=vocab_size,\n",
        "    special_tokens=['[PAD]', '[BOS]', '[EOS]', '[UNK]'],\n",
        ")\n",
        "hf_tokenizer = wrap_transformers_tokenizer(byte_bpe, tokenizer_dir)\n",
        "len(hf_tokenizer), hf_tokenizer.bos_token, hf_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67051104",
      "metadata": {},
      "source": [
        "## データセットのトークン化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91ac3dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return hf_tokenizer(\n",
        "        examples[TEXT_COL],\n",
        "        truncation=True,\n",
        "        max_length=block_size,\n",
        "    )\n",
        "\n",
        "tokenized_train = train_text_ds.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])\n",
        "tokenized_eval = eval_text_ds.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])\n",
        "tokenized_train[0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0a029b",
      "metadata": {},
      "source": [
        "## GPT-2の構築と事前学習（Causal LM）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a563c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size=len(hf_tokenizer),\n",
        "    n_positions=block_size,\n",
        "    n_ctx=block_size,\n",
        "    bos_token_id=hf_tokenizer.bos_token_id,\n",
        "    eos_token_id=hf_tokenizer.eos_token_id,\n",
        ")\n",
        "model = GPT2LMHeadModel(config)\n",
        "model.config.pad_token_id = hf_tokenizer.pad_token_id\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=hf_tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    warmup_steps=warmup_steps,\n",
        "    max_steps=max_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=logging_steps * 5,\n",
        "    save_steps=logging_steps * 5,\n",
        "    report_to=['none'],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=hf_tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "hf_tokenizer.save_pretrained(output_dir)\n",
        "'saved to: ' + output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb1daf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 直近の学習ログを表示（必要に応じて調整）\n",
        "import pandas as pd\n",
        "pd.DataFrame(trainer.state.log_history).tail(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9aa2f0",
      "metadata": {},
      "source": [
        "## 簡単な生成テスト（任意）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e16adb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "prompt = '吾輩は猫である'\n",
        "inputs = hf_tokenizer(prompt, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "    )\n",
        "print(hf_tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7f15c9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-from-scratch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
