{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f118f7dc",
   "metadata": {},
   "source": [
    "# 4.3 事前学習: rinnaの事前学習済みGPT-2モデルで継続学習\n",
    "\n",
    "このノートブックは、4.1節の前処理（正規化→連結→チャンク化）を適用したうえで、\n",
    "rinnaの事前学習済み日本語GPT-2モデル (`rinna/japanese-gpt2-medium`) を用いて\n",
    "Causal Language Modeling の継続学習を行います。\n",
    "\n",
    "- データ: `globis-university/aozorabunko-clean`（train split）\n",
    "- トークナイザ: rinnaの事前学習済みトークナイザー\n",
    "- モデル: rinna/japanese-gpt2-medium（事前学習済み）\n",
    "\n",
    "注意: 大規模学習には時間とGPUが必要です。まずは小さな `max_steps` で動作確認してから、\n",
    "徐々にスケールさせてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54985e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 使用するGPUを制限（カンマ区切りで複数指定可能）\n",
    "# 例: \"0\" → GPU 0のみ使用, \"0,1\" → GPU 0と1を使用\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88d4c0",
   "metadata": {},
   "source": [
    "## インポートと設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52106829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "try:\n",
    "    import neologdn  # 日本語用正規化（任意）\n",
    "except Exception:\n",
    "    neologdn = None\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "TEXT_COL = \"text\"\n",
    "SEP = \"\\n\\n<|doc|>\\n\\n\"\n",
    "\n",
    "# rinnaモデルの指定\n",
    "model_name = \"rinna/japanese-gpt2-medium\"\n",
    "\n",
    "# パラメータ（必要に応じて変更）\n",
    "block_size = 512            # rinnaモデルのmax_position_embeddingsに合わせて調整可能\n",
    "train_split = 'train'\n",
    "eval_ratio = 0.01\n",
    "per_device_train_batch_size = 16\n",
    "gradient_accumulation_steps = 8\n",
    "learning_rate = 5e-5        # 事前学習済みモデルなので学習率を下げる\n",
    "weight_decay = 0.1\n",
    "warmup_steps = 100\n",
    "num_train_epochs = 3        # 10エポック学習\n",
    "logging_steps = None         # 後で計算（0.5エポックごと）\n",
    "eval_steps = None            # 後で計算（0.5エポックごと）\n",
    "\n",
    "# 保存先（ノートブック相対パス -> リポジトリ直下に配置）\n",
    "REPO_ROOT = Path.cwd().parent.parent\n",
    "output_dir = REPO_ROOT / 'models' / 'rinna-gpt2-aozora-finetuned'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "str(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6511272",
   "metadata": {},
   "source": [
    "## セクション01の前処理済みデータを利用（連結→チャンク化のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040236cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セクション01の成果物（notebooks/chapter04/data）を読み込み、連結→チャンク化のみ実施\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def chunk_text(s: str, size: int) -> Iterable[str]:\n",
    "    for i in range(0, len(s), size):\n",
    "        yield s[i : i + size]\n",
    "\n",
    "# notebooks/chapter04/ からの相対パス\n",
    "DATA_ROOT = Path('data')\n",
    "CANDIDATES = [DATA_ROOT / 'aozora', DATA_ROOT]  # 優先順に探す\n",
    "\n",
    "def load_docs(base: Path, split: str) -> list[str]:\n",
    "    jsonl = base / f'{split}.jsonl'\n",
    "    txt   = base / f'{split}.txt'\n",
    "    if jsonl.exists():\n",
    "        with jsonl.open('r', encoding='utf-8') as f:\n",
    "            return [json.loads(line)['text'] for line in f if line.strip()]\n",
    "    if txt.exists():\n",
    "        raw = txt.read_text(encoding='utf-8')\n",
    "        return [s.strip() for s in raw.split('\\n\\n') if s.strip()]\n",
    "    return []\n",
    "\n",
    "train_docs, val_docs = [], []\n",
    "for base in CANDIDATES:\n",
    "    if not train_docs:\n",
    "        train_docs = load_docs(base, 'train')\n",
    "    if not val_docs:\n",
    "        val_docs = load_docs(base, 'val')\n",
    "\n",
    "if not train_docs or not val_docs:\n",
    "    raise FileNotFoundError('前処理済みデータが見つかりません。notebooks/chapter04/data/(aozora)/{train,val}.{jsonl,txt} を用意してください。')\n",
    "\n",
    "# 文書をセパレータで連結し、block_size 文字ごとにチャンク\n",
    "train_long = SEP.join(train_docs)\n",
    "val_long   = SEP.join(val_docs)\n",
    "train_chunks = list(chunk_text(train_long, block_size))\n",
    "val_chunks   = list(chunk_text(val_long,   block_size))\n",
    "\n",
    "# Hugging Face Datasets へ\n",
    "train_text_ds = Dataset.from_dict({TEXT_COL: train_chunks})\n",
    "eval_text_ds  = Dataset.from_dict({TEXT_COL: val_chunks})\n",
    "len(train_text_ds), len(eval_text_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce66ec",
   "metadata": {},
   "source": [
    "## rinnaの事前学習済みトークナイザーのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24153a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rinnaの事前学習済みトークナイザーをロード\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# pad_tokenが未設定の場合はeos_tokenを使用\n",
    "if hf_tokenizer.pad_token is None:\n",
    "    hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
    "\n",
    "print(f\"語彙サイズ: {len(hf_tokenizer)}\")\n",
    "print(f\"BOS token: {hf_tokenizer.bos_token}\")\n",
    "print(f\"EOS token: {hf_tokenizer.eos_token}\")\n",
    "print(f\"PAD token: {hf_tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67051104",
   "metadata": {},
   "source": [
    "## データセットのトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ac3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return hf_tokenizer(\n",
    "        examples[TEXT_COL],\n",
    "        truncation=True,\n",
    "        max_length=block_size,\n",
    "    )\n",
    "\n",
    "tokenized_train = train_text_ds.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])\n",
    "tokenized_eval = eval_text_ds.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])\n",
    "tokenized_train[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a029b",
   "metadata": {},
   "source": [
    "## rinnaの事前学習済みGPT-2モデルで継続学習（Causal LM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a563c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rinnaの事前学習済みモデルとコンフィグをロード\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# pad_token_idを設定\n",
    "model.config.pad_token_id = hf_tokenizer.pad_token_id\n",
    "\n",
    "print(f\"モデル: {model_name}\")\n",
    "print(f\"語彙サイズ: {config.vocab_size}\")\n",
    "print(f\"最大シーケンス長: {config.n_positions}\")\n",
    "print(f\"レイヤー数: {config.n_layer}\")\n",
    "print(f\"隠れ層次元: {config.n_embd}\")\n",
    "\n",
    "# データコレーターの設定\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=hf_tokenizer, mlm=False)\n",
    "\n",
    "# 1エポックあたりのステップ数を計算\n",
    "train_dataset_size = len(tokenized_train)\n",
    "steps_per_epoch = train_dataset_size // (per_device_train_batch_size * gradient_accumulation_steps)\n",
    "print(f\"\\nデータセットサイズ: {train_dataset_size}\")\n",
    "print(f\"1エポックあたりのステップ数: {steps_per_epoch}\")\n",
    "\n",
    "# 0.5エポックごとにログ・評価・保存\n",
    "logging_steps = max(1, steps_per_epoch // 10)\n",
    "eval_steps = logging_steps\n",
    "save_steps = logging_steps\n",
    "print(f\"ログ・評価・保存間隔: {logging_steps} steps (0.5エポックごと)\")\n",
    "\n",
    "# トレーニング引数の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=3,\n",
    "    report_to=['none'],\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# テスト用プロンプト\n",
    "test_prompts = [\n",
    "    '吾輩は猫である。名前はまだ無い。',\n",
    "    '明治時代の',\n",
    "    '東京の街には',\n",
    "    '先生は言った。「',\n",
    "]\n",
    "\n",
    "# カスタムTrainerクラス（定期的にテキスト生成を実行）\n",
    "from transformers import TrainerCallback\n",
    "import logging\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, test_prompts, generation_interval):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_prompts = test_prompts\n",
    "        self.generation_interval = generation_interval\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.has_generated_initial = False\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        file_handler = logging.FileHandler('training_generation.log')\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        self.logger.addHandler(file_handler)\n",
    "    \n",
    "    def on_step_begin(self, args, state, control, model=None, **kwargs):\n",
    "        # 最初のステップで初期生成を実行\n",
    "        if not self.has_generated_initial:\n",
    "            self._generate_samples(model, 0)\n",
    "            self.has_generated_initial = True\n",
    "    \n",
    "    def on_log(self, args, state, control, model=None, **kwargs):\n",
    "        # generation_intervalごとに生成テストを実行\n",
    "        current_step = state.global_step\n",
    "        if current_step > 0 and current_step % self.generation_interval == 0:\n",
    "            self._generate_samples(model, current_step)\n",
    "    \n",
    "    def _generate_samples(self, model, step):\n",
    "        \"\"\"テストプロンプトで生成サンプルを表示\"\"\"\n",
    "        self.logger.info(f\"\\n{'='*60}\")\n",
    "        self.logger.info(f\"Step {step}: テキスト生成サンプル\")\n",
    "        self.logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # モデルの状態を保存\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        \n",
    "        for prompt in self.test_prompts:\n",
    "            self.logger.info(f\"\\nプロンプト: {prompt}\")\n",
    "            self.logger.info(\"-\" * 50)\n",
    "            \n",
    "            inputs = self.tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            generated_text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            self.logger.info(generated_text)\n",
    "        \n",
    "        self.logger.info(f\"\\n{'='*60}\\n\")\n",
    "        \n",
    "        # モデルの状態を復元\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "# コールバックの作成（0.5エポックごとに生成）\n",
    "generation_callback = GenerationCallback(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    test_prompts=test_prompts,\n",
    "    generation_interval=logging_steps\n",
    ")\n",
    "\n",
    "# トレーナーの作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    callbacks=[generation_callback],\n",
    ")\n",
    "\n",
    "# トレーニング実行\n",
    "trainer.train()\n",
    "\n",
    "# モデルとトークナイザーの保存\n",
    "trainer.save_model(str(output_dir))\n",
    "hf_tokenizer.save_pretrained(str(output_dir))\n",
    "print(f'モデルを保存しました: {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直近の学習ログを表示（必要に応じて調整）\n",
    "import pandas as pd\n",
    "pd.DataFrame(trainer.state.log_history).tail(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9aa2f0",
   "metadata": {},
   "source": [
    "## 簡単な生成テスト（任意）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルでの生成テスト\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 最新のチェックポイントまたは最終保存モデルをロード\n",
    "# チェックポイントがある場合はそちらを使用、なければ output_dir を使用\n",
    "checkpoint_dirs = [d for d in output_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint-')]\n",
    "if checkpoint_dirs:\n",
    "    # 最新のチェックポイントを選択（番号順でソート）\n",
    "    latest_checkpoint = sorted(checkpoint_dirs, key=lambda x: int(x.name.split('-')[1]))[-1]\n",
    "    model_dir = latest_checkpoint\n",
    "    print(f\"チェックポイントからロード: {model_dir}\")\n",
    "else:\n",
    "    model_dir = output_dir\n",
    "    print(f\"最終モデルからロード: {model_dir}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
    "model = AutoModelForCausalLM.from_pretrained(str(model_dir))\n",
    "\n",
    "# pad_token が未設定の場合は eos を代用\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# デバイスの設定\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device).eval()\n",
    "print(f\"デバイス: {device}\")\n",
    "\n",
    "# 生成テスト\n",
    "prompts = [\n",
    "    '吾輩は猫である。名前はまだ無い。',\n",
    "    '明治時代の',\n",
    "    '東京の街には',\n",
    "    '先生は言った。「',\n",
    "]\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nプロンプト: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # 生成されたテキスト全体を表示\n",
    "    generated_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のプロンプトでテスト\n",
    "test_prompts = [\n",
    "    '吾輩は猫である。名前はまだ無い。',\n",
    "    '明治時代の',\n",
    "    '東京の街には',\n",
    "    '先生は言った。「',\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nプロンプト: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d47939",
   "metadata": {},
   "source": [
    "## 比較: Zero-shot（学習前の元のrinnaモデル）での出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llgjhtf8svr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元のrinnaモデルをロードしてzero-shotでの生成を試す\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Zero-shot: 学習前の元のrinnaモデルでの生成\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 元のrinnaモデルをロード\n",
    "zeroshot_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "zeroshot_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# pad_tokenが未設定の場合はeos_tokenを使用\n",
    "if zeroshot_tokenizer.pad_token is None:\n",
    "    zeroshot_tokenizer.pad_token = zeroshot_tokenizer.eos_token\n",
    "    zeroshot_model.config.pad_token_id = zeroshot_tokenizer.pad_token_id\n",
    "\n",
    "# デバイスの設定\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "zeroshot_model = zeroshot_model.to(device).eval()\n",
    "print(f\"デバイス: {device}\\n\")\n",
    "\n",
    "# 同じプロンプトでテスト\n",
    "test_prompts = [\n",
    "    '吾輩は猫である。名前はまだ無い。',\n",
    "    '明治時代の',\n",
    "    '東京の街には',\n",
    "    '先生は言った。「',\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nプロンプト: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    inputs = zeroshot_tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = zeroshot_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=zeroshot_tokenizer.pad_token_id,\n",
    "            eos_token_id=zeroshot_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = zeroshot_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69cc94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
