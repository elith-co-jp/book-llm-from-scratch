{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.3 事前学習: 4.1前処理 + Tokenizer学習 + GPT-2事前学習\n",
        "\n",
        "このノートブックは、4.1節の前処理（正規化→連結→チャンク化）を適用したうえで、4.3節で\n",
        "Byte-level BPE のトークナイザを学習し、Hugging Face Transformers の GPT-2 を用いて\n",
        "Causal Language Modeling の事前学習を行います。\n",
        "\n",
        "- データ: `globis-university/aozorabunko-clean`（train split）\n",
        "- トークナイザ: ByteLevel BPE（train のみで学習）\n",
        "- モデル: GPT-2（ランダム初期化、config は語彙サイズとコンテキスト長に合わせて作成）\n",
        "\n",
        "注意: 大規模学習には時間とGPUが必要です。まずは小さな `max_steps` で動作確認してから、\n",
        "徐々にスケールさせてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54985e67",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ab88d4c0",
      "metadata": {},
      "source": [
        "## インポートと設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52106829",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/home/akira-nagasawa/book-llm-from-scratch/data/processed/tokenizer',\n",
              " '/home/akira-nagasawa/book-llm-from-scratch/models/gpt2-aozora')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import Iterable, Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "try:\n",
        "    import neologdn  # 日本語用正規化（任意）\n",
        "except Exception:\n",
        "    neologdn = None\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import (\n",
        "    PreTrainedTokenizerFast,\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "TEXT_COL = \"text\"\n",
        "SEP = \"\\n\\n<|doc|>\\n\\n\"\n",
        "\n",
        "# パラメータ（必要に応じて変更）\n",
        "block_size = 512            # 例: 512/1024/2048\n",
        "vocab_size = 30000          # 例: 30k/50k\n",
        "train_split = 'train'\n",
        "eval_ratio = 0.01\n",
        "per_device_train_batch_size = 10\n",
        "gradient_accumulation_steps = 8\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.1\n",
        "warmup_steps = 100\n",
        "max_steps = 200              # デモ用に小さめ（本番は増やす）\n",
        "logging_steps = 20\n",
        "\n",
        "# 保存先（ノートブック相対パス -> リポジトリ直下に配置）\n",
        "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "tokenizer_dir = os.path.join(REPO_ROOT, 'data', 'processed', 'tokenizer')\n",
        "output_dir = os.path.join(REPO_ROOT, 'models', 'gpt2-aozora')\n",
        "os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "tokenizer_dir, output_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6511272",
      "metadata": {},
      "source": [
        "## セクション01の前処理済みデータを利用（連結→チャンク化のみ）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "040236cf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(389303, 7009)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# セクション01の成果物（notebooks/chapter04/data）を読み込み、連結→チャンク化のみ実施\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "def chunk_text(s: str, size: int) -> Iterable[str]:\n",
        "    for i in range(0, len(s), size):\n",
        "        yield s[i : i + size]\n",
        "\n",
        "# notebooks/chapter04/ からの相対パス\n",
        "DATA_ROOT = Path('data')\n",
        "CANDIDATES = [DATA_ROOT / 'aozora', DATA_ROOT]  # 優先順に探す\n",
        "\n",
        "def load_docs(base: Path, split: str) -> list[str]:\n",
        "    jsonl = base / f'{split}.jsonl'\n",
        "    txt   = base / f'{split}.txt'\n",
        "    if jsonl.exists():\n",
        "        with jsonl.open('r', encoding='utf-8') as f:\n",
        "            return [json.loads(line)['text'] for line in f if line.strip()]\n",
        "    if txt.exists():\n",
        "        raw = txt.read_text(encoding='utf-8')\n",
        "        return [s.strip() for s in raw.split('\\n\\n') if s.strip()]\n",
        "    return []\n",
        "\n",
        "train_docs, val_docs = [], []\n",
        "for base in CANDIDATES:\n",
        "    if not train_docs:\n",
        "        train_docs = load_docs(base, 'train')\n",
        "    if not val_docs:\n",
        "        val_docs = load_docs(base, 'val')\n",
        "\n",
        "if not train_docs or not val_docs:\n",
        "    raise FileNotFoundError('前処理済みデータが見つかりません。notebooks/chapter04/data/(aozora)/{train,val}.{jsonl,txt} を用意してください。')\n",
        "\n",
        "# 文書をセパレータで連結し、block_size 文字ごとにチャンク\n",
        "train_long = SEP.join(train_docs)\n",
        "val_long   = SEP.join(val_docs)\n",
        "train_chunks = list(chunk_text(train_long, block_size))\n",
        "val_chunks   = list(chunk_text(val_long,   block_size))\n",
        "\n",
        "# Hugging Face Datasets へ\n",
        "train_text_ds = Dataset.from_dict({TEXT_COL: train_chunks})\n",
        "eval_text_ds  = Dataset.from_dict({TEXT_COL: val_chunks})\n",
        "len(train_text_ds), len(eval_text_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cce66ec",
      "metadata": {},
      "source": [
        "## 4.3: Byte-level BPE トークナイザの学習と保存\n",
        "時間かかるので目安を書く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a24153a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m     tokenizer.train_from_iterator(\n\u001b[32m      4\u001b[39m         iterator,\n\u001b[32m      5\u001b[39m         vocab_size=vocab_size,\n\u001b[32m      6\u001b[39m         special_tokens=special_tokens \u001b[38;5;129;01mor\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33m[PAD]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m[BOS]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m[EOS]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m[UNK]\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      7\u001b[39m         show_progress=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m byte_bpe = \u001b[43mtrain_bytelevel_bpe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_text_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEXT_COL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[PAD]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[BOS]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[EOS]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[UNK]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtrain_bytelevel_bpe\u001b[39m\u001b[34m(iterator, vocab_size, special_tokens)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_bytelevel_bpe\u001b[39m(iterator: Iterable[\u001b[38;5;28mstr\u001b[39m], vocab_size: \u001b[38;5;28mint\u001b[39m, special_tokens: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m     tokenizer = ByteLevelBPETokenizer()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[PAD]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[BOS]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[EOS]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[UNK]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/tokenizers/implementations/byte_level_bpe.py:118\u001b[39m, in \u001b[36mByteLevelBPETokenizer.train_from_iterator\u001b[39m\u001b[34m(self, iterator, vocab_size, min_frequency, show_progress, special_tokens, length)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Train the model using the given iterator\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m trainer = trainers.BpeTrainer(\n\u001b[32m    112\u001b[39m     vocab_size=vocab_size,\n\u001b[32m    113\u001b[39m     min_frequency=min_frequency,\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m     initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n\u001b[32m    117\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def train_bytelevel_bpe(iterator: Iterable[str], vocab_size: int, special_tokens: Optional[list[str]] = None):\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer.train_from_iterator(\n",
        "        iterator,\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=special_tokens or ['[PAD]', '[BOS]', '[EOS]', '[UNK]'],\n",
        "        show_progress=True,\n",
        "    )\n",
        "    return tokenizer\n",
        "\n",
        "byte_bpe = train_bytelevel_bpe(\n",
        "    iterator=(ex for ex in train_text_ds[TEXT_COL]),\n",
        "    vocab_size=vocab_size,\n",
        "    special_tokens=['[PAD]', '[BOS]', '[EOS]', '[UNK]'],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497553b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30000, '[BOS]', '[EOS]')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def wrap_transformers_tokenizer(bytelevel_tokenizer, save_dir: str) -> PreTrainedTokenizerFast:\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    # Save vocab/merges (for reference) and also tokenizer.json for direct fast loading\n",
        "    bytelevel_tokenizer.save_model(save_dir)\n",
        "    bytelevel_tokenizer.save(os.path.join(save_dir, 'tokenizer.json'))\n",
        "    # Load fast tokenizer directly from tokenizer.json to avoid slow->fast conversion\n",
        "    fast = PreTrainedTokenizerFast(\n",
        "        tokenizer_file=os.path.join(save_dir, 'tokenizer.json'),\n",
        "        bos_token='[BOS]',\n",
        "        eos_token='[EOS]',\n",
        "        pad_token='[PAD]',\n",
        "        unk_token='[UNK]',\n",
        "    )\n",
        "    fast.save_pretrained(save_dir)\n",
        "    return fast\n",
        "\n",
        "\n",
        "hf_tokenizer = wrap_transformers_tokenizer(byte_bpe, tokenizer_dir)\n",
        "len(hf_tokenizer), hf_tokenizer.bos_token, hf_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67051104",
      "metadata": {},
      "source": [
        "## データセットのトークン化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91ac3dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba41f51f516b437abd5b5d34292d0907",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/389303 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "205c88655ad04a61a386cfee5257f150",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7009 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    return hf_tokenizer(\n",
        "        examples[TEXT_COL],\n",
        "        truncation=True,\n",
        "        max_length=block_size,\n",
        "    )\n",
        "\n",
        "tokenized_train = train_text_ds.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])\n",
        "tokenized_eval = eval_text_ds.map(tokenize_function, batched=True, remove_columns=[TEXT_COL])\n",
        "tokenized_train[0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0a029b",
      "metadata": {},
      "source": [
        "## GPT-2の構築と事前学習（Causal LM）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a563c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2513027/2431564990.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7401' max='60830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 7401/60830 2:05:50 < 15:08:45, 0.98 it/s, Epoch 1.22/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.072000</td>\n",
              "      <td>8.308963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.001800</td>\n",
              "      <td>8.044173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.944800</td>\n",
              "      <td>7.829145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.916900</td>\n",
              "      <td>7.650832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.884400</td>\n",
              "      <td>7.516081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.841800</td>\n",
              "      <td>7.413182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.823400</td>\n",
              "      <td>7.340619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.821500</td>\n",
              "      <td>7.285051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.786300</td>\n",
              "      <td>7.212158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.782100</td>\n",
              "      <td>7.172904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.776300</td>\n",
              "      <td>7.125195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.775300</td>\n",
              "      <td>7.096273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.746100</td>\n",
              "      <td>7.034092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.729500</td>\n",
              "      <td>6.991798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.736000</td>\n",
              "      <td>6.956245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.726600</td>\n",
              "      <td>6.920377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.715400</td>\n",
              "      <td>6.895678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.715800</td>\n",
              "      <td>6.847795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.700300</td>\n",
              "      <td>6.815788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.718300</td>\n",
              "      <td>6.789000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.669100</td>\n",
              "      <td>6.755122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.665500</td>\n",
              "      <td>6.726799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.672400</td>\n",
              "      <td>6.695943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.675200</td>\n",
              "      <td>6.677969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.660700</td>\n",
              "      <td>6.655748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.656200</td>\n",
              "      <td>6.632362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.638400</td>\n",
              "      <td>6.605312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.652300</td>\n",
              "      <td>6.581257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.628600</td>\n",
              "      <td>6.562339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.652200</td>\n",
              "      <td>6.541916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.624000</td>\n",
              "      <td>6.528840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.609300</td>\n",
              "      <td>6.510553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.618300</td>\n",
              "      <td>6.499137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.634500</td>\n",
              "      <td>6.485337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.610100</td>\n",
              "      <td>6.471883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.637700</td>\n",
              "      <td>6.456344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.600700</td>\n",
              "      <td>6.443230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.604500</td>\n",
              "      <td>6.423381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.610600</td>\n",
              "      <td>6.419292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>6.419409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>1.582800</td>\n",
              "      <td>6.412618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.619500</td>\n",
              "      <td>6.406555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>1.593000</td>\n",
              "      <td>6.407651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>1.604400</td>\n",
              "      <td>6.376719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.600900</td>\n",
              "      <td>6.365615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>1.584400</td>\n",
              "      <td>6.366978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>1.608400</td>\n",
              "      <td>6.368733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>1.583000</td>\n",
              "      <td>6.351886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>1.592800</td>\n",
              "      <td>6.345116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.577800</td>\n",
              "      <td>6.340428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>1.575700</td>\n",
              "      <td>6.336541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>1.587200</td>\n",
              "      <td>6.331317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>1.577700</td>\n",
              "      <td>6.342637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>1.595500</td>\n",
              "      <td>6.348792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.577800</td>\n",
              "      <td>6.346781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>1.611600</td>\n",
              "      <td>6.374088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>1.589700</td>\n",
              "      <td>6.385518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>1.586200</td>\n",
              "      <td>6.368509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>6.365384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.579100</td>\n",
              "      <td>6.335515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>1.586200</td>\n",
              "      <td>6.327255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>1.563200</td>\n",
              "      <td>6.316188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>1.563600</td>\n",
              "      <td>6.295593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>1.572500</td>\n",
              "      <td>6.309319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.573500</td>\n",
              "      <td>6.306152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>1.585500</td>\n",
              "      <td>6.302769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>1.571200</td>\n",
              "      <td>6.300312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>1.572100</td>\n",
              "      <td>6.323537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>1.574500</td>\n",
              "      <td>6.317061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.589900</td>\n",
              "      <td>6.334055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>1.586100</td>\n",
              "      <td>6.307418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>1.585900</td>\n",
              "      <td>6.323447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>1.584800</td>\n",
              "      <td>6.312745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>1.593000</td>\n",
              "      <td>6.362552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/home/akira-nagasawa/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at inline_container.cc:664] . unexpected pos 376256192 vs 376256084",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/serialization.py:967\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/serialization.py:1268\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:858] . PytorchStreamWriter failed writing file data/49: file write failed",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     13\u001b[39m training_args = TrainingArguments(\n\u001b[32m     14\u001b[39m     output_dir=output_dir,\n\u001b[32m     15\u001b[39m     overwrite_output_dir=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     report_to=[\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     model=model,\n\u001b[32m     32\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     tokenizer=hf_tokenizer,\n\u001b[32m     37\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m trainer.save_model(output_dir)\n\u001b[32m     40\u001b[39m hf_tokenizer.save_pretrained(output_dir)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/transformers/trainer.py:2754\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2752\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2753\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2754\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2765\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/transformers/trainer.py:3234\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3231\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3235\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/transformers/trainer.py:3342\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3338\u001b[39m         \u001b[38;5;28mself\u001b[39m.state.best_model_checkpoint = best_checkpoint_dir\n\u001b[32m   3340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3341\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3342\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3343\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_scaler(output_dir)\n\u001b[32m   3344\u001b[39m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/transformers/trainer.py:3469\u001b[39m, in \u001b[36mTrainer._save_optimizer_and_scheduler\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m   3464\u001b[39m     save_fsdp_optimizer(\n\u001b[32m   3465\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.state.fsdp_plugin, \u001b[38;5;28mself\u001b[39m.accelerator, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.model, output_dir\n\u001b[32m   3466\u001b[39m     )\n\u001b[32m   3467\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m   3468\u001b[39m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3469\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3471\u001b[39m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[32m   3472\u001b[39m is_deepspeed_custom_scheduler = \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3473\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[32m   3474\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/serialization.py:966\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m     f = os.fspath(f)\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    967\u001b[39m         _save(\n\u001b[32m    968\u001b[39m             obj,\n\u001b[32m    969\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m             _disable_byteorder_record,\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/book-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/serialization.py:798\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    800\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
            "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:664] . unexpected pos 376256192 vs 376256084"
          ]
        }
      ],
      "source": [
        "#検証用上書き\n",
        "per_device_train_batch_size = 10\n",
        "\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=len(hf_tokenizer),\n",
        "    n_positions=block_size,\n",
        "    n_ctx=block_size,\n",
        "    bos_token_id=hf_tokenizer.bos_token_id,\n",
        "    eos_token_id=hf_tokenizer.eos_token_id,\n",
        ")\n",
        "model = GPT2LMHeadModel(config)\n",
        "model.config.pad_token_id = hf_tokenizer.pad_token_id\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=hf_tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=logging_steps,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=logging_steps * 5,\n",
        "    save_steps=logging_steps * 5,\n",
        "    report_to=['none'],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=hf_tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "hf_tokenizer.save_pretrained(output_dir)\n",
        "'saved to: ' + output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "acb1daf9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>grad_norm</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>epoch</th>\n",
              "      <th>step</th>\n",
              "      <th>eval_loss</th>\n",
              "      <th>eval_runtime</th>\n",
              "      <th>eval_samples_per_second</th>\n",
              "      <th>eval_steps_per_second</th>\n",
              "      <th>train_runtime</th>\n",
              "      <th>train_samples_per_second</th>\n",
              "      <th>train_steps_per_second</th>\n",
              "      <th>total_flos</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.3741</td>\n",
              "      <td>0.224923</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.003288</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.1858</td>\n",
              "      <td>0.777973</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.006576</td>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0781</td>\n",
              "      <td>0.290171</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>0.009864</td>\n",
              "      <td>60</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0673</td>\n",
              "      <td>0.219140</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.013152</td>\n",
              "      <td>80</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0720</td>\n",
              "      <td>0.285811</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.016440</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.016440</td>\n",
              "      <td>100</td>\n",
              "      <td>8.308963</td>\n",
              "      <td>71.9135</td>\n",
              "      <td>97.464</td>\n",
              "      <td>12.195</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.0415</td>\n",
              "      <td>0.225071</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.019728</td>\n",
              "      <td>120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.0499</td>\n",
              "      <td>0.323680</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.023015</td>\n",
              "      <td>140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.0463</td>\n",
              "      <td>0.169706</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.026303</td>\n",
              "      <td>160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.0157</td>\n",
              "      <td>0.183917</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.029591</td>\n",
              "      <td>180</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2.0009</td>\n",
              "      <td>0.153031</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.032879</td>\n",
              "      <td>200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.032879</td>\n",
              "      <td>200</td>\n",
              "      <td>8.058093</td>\n",
              "      <td>70.8286</td>\n",
              "      <td>98.957</td>\n",
              "      <td>12.382</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.032879</td>\n",
              "      <td>200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>207.1719</td>\n",
              "      <td>61.784</td>\n",
              "      <td>0.965</td>\n",
              "      <td>2.039507e+15</td>\n",
              "      <td>2.093157</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
              "0   2.3741   0.224923       0.000095  0.003288    20        NaN           NaN   \n",
              "1   2.1858   0.777973       0.000195  0.006576    40        NaN           NaN   \n",
              "2   2.0781   0.290171       0.000295  0.009864    60        NaN           NaN   \n",
              "3   2.0673   0.219140       0.000395  0.013152    80        NaN           NaN   \n",
              "4   2.0720   0.285811       0.000495  0.016440   100        NaN           NaN   \n",
              "5      NaN        NaN            NaN  0.016440   100   8.308963       71.9135   \n",
              "6   2.0415   0.225071       0.000405  0.019728   120        NaN           NaN   \n",
              "7   2.0499   0.323680       0.000305  0.023015   140        NaN           NaN   \n",
              "8   2.0463   0.169706       0.000205  0.026303   160        NaN           NaN   \n",
              "9   2.0157   0.183917       0.000105  0.029591   180        NaN           NaN   \n",
              "10  2.0009   0.153031       0.000005  0.032879   200        NaN           NaN   \n",
              "11     NaN        NaN            NaN  0.032879   200   8.058093       70.8286   \n",
              "12     NaN        NaN            NaN  0.032879   200        NaN           NaN   \n",
              "\n",
              "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
              "0                       NaN                    NaN            NaN   \n",
              "1                       NaN                    NaN            NaN   \n",
              "2                       NaN                    NaN            NaN   \n",
              "3                       NaN                    NaN            NaN   \n",
              "4                       NaN                    NaN            NaN   \n",
              "5                    97.464                 12.195            NaN   \n",
              "6                       NaN                    NaN            NaN   \n",
              "7                       NaN                    NaN            NaN   \n",
              "8                       NaN                    NaN            NaN   \n",
              "9                       NaN                    NaN            NaN   \n",
              "10                      NaN                    NaN            NaN   \n",
              "11                   98.957                 12.382            NaN   \n",
              "12                      NaN                    NaN       207.1719   \n",
              "\n",
              "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
              "0                        NaN                     NaN           NaN         NaN  \n",
              "1                        NaN                     NaN           NaN         NaN  \n",
              "2                        NaN                     NaN           NaN         NaN  \n",
              "3                        NaN                     NaN           NaN         NaN  \n",
              "4                        NaN                     NaN           NaN         NaN  \n",
              "5                        NaN                     NaN           NaN         NaN  \n",
              "6                        NaN                     NaN           NaN         NaN  \n",
              "7                        NaN                     NaN           NaN         NaN  \n",
              "8                        NaN                     NaN           NaN         NaN  \n",
              "9                        NaN                     NaN           NaN         NaN  \n",
              "10                       NaN                     NaN           NaN         NaN  \n",
              "11                       NaN                     NaN           NaN         NaN  \n",
              "12                    61.784                   0.965  2.039507e+15    2.093157  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 直近の学習ログを表示（必要に応じて調整）\n",
        "import pandas as pd\n",
        "pd.DataFrame(trainer.state.log_history).tail(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9aa2f0",
      "metadata": {},
      "source": [
        "## 簡単な生成テスト（任意）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e16adb8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "「いや、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、\n"
          ]
        }
      ],
      "source": [
        "# 学習済みチェックポイントからの推論（簡単な生成テスト）\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 事前に定義済みの REPO_ROOT を利用してチェックポイントを指す\n",
        "ckpt_dir = os.path.join(REPO_ROOT, 'models', 'gpt2-aozora', 'checkpoint-7000')\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckpt_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(ckpt_dir)\n",
        "\n",
        "# pad_token が未設定の場合は eos を代用\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
        "model = model.to(device).eval()\n",
        "\n",
        "prompt = '吾輩は猫である。名前はまだ無い。'\n",
        "inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# 入力部分を除いた新規生成のみを表示\n",
        "gen_only = out[0][inputs['input_ids'].shape[1]:]\n",
        "print(tokenizer.decode(gen_only, skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8c7f15c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "「いや、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、君、\n"
          ]
        }
      ],
      "source": [
        "はprompt = ''\n",
        "inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=False,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# 入力部分を除いた新規生成のみを表示\n",
        "gen_only = out[0][inputs['input_ids'].shape[1]:]\n",
        "print(tokenizer.decode(gen_only, skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "book-llm-from-scratch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
