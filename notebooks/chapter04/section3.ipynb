{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        response = requests.get(url)\n",
    "        file.write(response.content)\n",
    "\n",
    "\n",
    "path = \"arxiv.jsonl\"\n",
    "download_url = \"https://data.together.xyz/redpajama-data-1T/v1.0.0/arxiv/arxiv_023827cd-7ee8-42e6-aa7b-661731f4c70f.jsonl\"\n",
    "download_file(download_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの確認\n",
    "import json\n",
    "\n",
    "path = \"arxiv.jsonl\"\n",
    "with open(path, \"r\") as f:\n",
    "    d = json.loads(f.readline())\n",
    "\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d[\"meta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "import json\n",
    "\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    texts = [json.loads(line)[\"text\"] for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トークナイザの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=texts,\n",
    "    vocab_size=30_000,\n",
    ")\n",
    "eos_token = \"<|endoftext|>\"\n",
    "my_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"abstract: This is an example of text. introduction: This is an introduction. conclusion: This is a conclusion.\"\n",
    "print(gpt2_tokenizer.tokenize(example))\n",
    "print(my_tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_tokens = []\n",
    "for text in tqdm(texts):\n",
    "    tokens = my_tokenizer(text)\n",
    "    all_tokens += [my_tokenizer.eos_token_id] + tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_length = 512\n",
    "\n",
    "with open(\"chunked_tokens.jsonl\", \"w\") as f:\n",
    "    for i in range(0, len(all_tokens), token_length):\n",
    "        chunk = all_tokens[i : i + token_length]\n",
    "        f.write(json.dumps({\"input_ids\": chunk}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chunked_tokens.jsonl\", \"r\") as f:\n",
    "    tokens = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(transformer_tokenizer),\n",
    "    n_ctx=512,\n",
    "    bos_token_id=transformer_tokenizer.bos_token_id,\n",
    "    eos_token_id=transformer_tokenizer.eos_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)\n",
    "# GPT2Config {\n",
    "#   \"_name_or_path\": \"gpt2\",\n",
    "#   \"activation_function\": \"gelu_new\",\n",
    "#   \"architectures\": [\n",
    "#     \"GPT2LMHeadModel\"\n",
    "#   ],\n",
    "#   \"attn_pdrop\": 0.1,\n",
    "#   \"bos_token_id\": 30000,\n",
    "#   \"embd_pdrop\": 0.1,\n",
    "#   \"eos_token_id\": 30001,\n",
    "#   \"initializer_range\": 0.02,\n",
    "#   \"layer_norm_epsilon\": 1e-05,\n",
    "#   \"model_type\": \"gpt2\",\n",
    "#   \"n_ctx\": 512,\n",
    "#   \"n_embd\": 768,\n",
    "#   \"n_head\": 12,\n",
    "#   \"n_inner\": null,\n",
    "#   \"n_layer\": 12,\n",
    "#   \"n_positions\": 1024,\n",
    "#   \"reorder_and_upcast_attn\": false,\n",
    "#   \"resid_pdrop\": 0.1,\n",
    "#   \"scale_attn_by_inverse_layer_idx\": false,\n",
    "#   \"scale_attn_weights\": true,\n",
    "#   \"summary_activation\": null,\n",
    "#   \"summary_first_dropout\": 0.1,\n",
    "# ...\n",
    "#   \"use_cache\": true,\n",
    "#   \"vocab_size\": 30004\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda data: tokenizer(data[\"text\"], truncation=True, max_length=512), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "    if len(data[\"input_ids\"]) > 512:\n",
    "        print(len(data[\"input_ids\"]))\n",
    "        print(\"Too long text\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 0\n",
    "for data in tqdm(dataset):\n",
    "    tokens += len(data[\"input_ids\"])\n",
    "print(tokens)  # 0.1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator([dataset[\"input_ids\"][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"pretrained_model\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "\n",
    "prompt = \"Abstract: \"\n",
    "with torch.no_grad():\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
