{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        response = requests.get(url)\n",
    "        file.write(response.content)\n",
    "\n",
    "\n",
    "path = \"arxiv.jsonl\"\n",
    "download_url = \"https://data.together.xyz/redpajama-data-1T/v1.0.0/arxiv/arxiv_023827cd-7ee8-42e6-aa7b-661731f4c70f.jsonl\"\n",
    "download_file(download_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの確認\n",
    "import json\n",
    "\n",
    "path = \"arxiv.jsonl\"\n",
    "with open(path, \"r\") as f:\n",
    "    d = json.loads(f.readline())\n",
    "\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d[\"meta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "import json\n",
    "\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    texts = [json.loads(line)[\"text\"] for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "\n",
    "def text_splitter(document: str, max_length: int = 512) -> list[str]:\n",
    "    text_splitter = SpacyTextSplitter(separator=\"[SEP]\")\n",
    "    docs = text_splitter.split_text(document.replace(\"\\n\", \"\"))\n",
    "\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    if len(docs) == 0:\n",
    "        return []\n",
    "    for text in docs[0].split(\"[SEP]\"):\n",
    "        if len(chunk) + len(text) > max_length:\n",
    "            chunks.append(chunk)\n",
    "            chunk = text\n",
    "        else:\n",
    "            chunk += text\n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並列バージョン\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_texts = Parallel(n_jobs=-1)(\n",
    "    delayed(text_splitter)(text) for text in tqdm(texts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as jsonl with key \"text\"\n",
    "import json\n",
    "\n",
    "with open(\"chunked_dataset.jsonl\", \"w\") as f:\n",
    "    for texts in dataset_texts:\n",
    "        for text in texts:\n",
    "            json.dump({\"text\": text}, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "with open(\"chunked_dataset.jsonl\", \"r\") as f:\n",
    "    dataset_list = [json.loads(line) for line in f]\n",
    "dataset = Dataset.from_list(dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トークナイザの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    dataset[\"text\"],\n",
    "    vocab_size=30_000,\n",
    ")\n",
    "my_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"abstract: This is an example of text. introduction: This is an introduction. conclusion: This is a conclusion.\"\n",
    "print(gpt2_tokenizer.tokenize(example))\n",
    "print(my_tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(transformer_tokenizer),\n",
    "    n_ctx=512,\n",
    "    bos_token_id=transformer_tokenizer.bos_token_id,\n",
    "    eos_token_id=transformer_tokenizer.eos_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)\n",
    "# GPT2Config {\n",
    "#   \"_name_or_path\": \"gpt2\",\n",
    "#   \"activation_function\": \"gelu_new\",\n",
    "#   \"architectures\": [\n",
    "#     \"GPT2LMHeadModel\"\n",
    "#   ],\n",
    "#   \"attn_pdrop\": 0.1,\n",
    "#   \"bos_token_id\": 30000,\n",
    "#   \"embd_pdrop\": 0.1,\n",
    "#   \"eos_token_id\": 30001,\n",
    "#   \"initializer_range\": 0.02,\n",
    "#   \"layer_norm_epsilon\": 1e-05,\n",
    "#   \"model_type\": \"gpt2\",\n",
    "#   \"n_ctx\": 512,\n",
    "#   \"n_embd\": 768,\n",
    "#   \"n_head\": 12,\n",
    "#   \"n_inner\": null,\n",
    "#   \"n_layer\": 12,\n",
    "#   \"n_positions\": 1024,\n",
    "#   \"reorder_and_upcast_attn\": false,\n",
    "#   \"resid_pdrop\": 0.1,\n",
    "#   \"scale_attn_by_inverse_layer_idx\": false,\n",
    "#   \"scale_attn_weights\": true,\n",
    "#   \"summary_activation\": null,\n",
    "#   \"summary_first_dropout\": 0.1,\n",
    "# ...\n",
    "#   \"use_cache\": true,\n",
    "#   \"vocab_size\": 30004\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda data: tokenizer(data[\"text\"], truncation=True, max_length=512), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "    if len(data[\"input_ids\"]) > 512:\n",
    "        print(len(data[\"input_ids\"]))\n",
    "        print(\"Too long text\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
