{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 学習の効率化（LoRA）: HuggingFace PEFTを用いたLoRAファインチューニング\n",
    "\n",
    "このノートブックでは、4.4節の内容に沿って、HuggingFace PEFTライブラリを用いた\n",
    "LoRA（Low-Rank Adaptation）によるファインチューニングを実装します。\n",
    "\n",
    "- rinnaの事前学習済み日本語GPT-2モデル (`rinna/japanese-gpt2-medium`) をベースモデルとして使用\n",
    "- 4.1節で前処理した青空文庫データセットでファインチューニング\n",
    "- LoRAにより、全パラメータの一部のみを効率的に学習\n",
    "\n",
    "**前提条件**: `section01_dataset_preprocessing.ipynb` を実行済みで、`data/aozora_preprocessed` が存在すること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "\n",
    "4.1節で前処理・保存した青空文庫データセットを読み込み、訓練用と評価用に分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理済みデータの読み込み\n",
    "data_dir = Path(\"data\") / \"aozora_preprocessed\"\n",
    "dataset = load_from_disk(str(data_dir))\n",
    "print(f\"読み込んだデータセット: {dataset}\")\n",
    "print(f\"サンプル数: {len(dataset)}\")\n",
    "\n",
    "# 訓練/評価に分割（1%を評価用に使用）\n",
    "EVAL_RATIO = 0.01\n",
    "split_ds = dataset.train_test_split(test_size=EVAL_RATIO, seed=42)\n",
    "train_dataset = split_ds[\"train\"]\n",
    "eval_dataset = split_ds[\"test\"]\n",
    "\n",
    "print(f\"訓練データ: {len(train_dataset)} サンプル\")\n",
    "print(f\"評価データ: {len(eval_dataset)} サンプル\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークナイザとモデルの読み込み\n",
    "\n",
    "rinnaが公開している日本語GPT-2 mediumモデルとそのトークナイザを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"rinna/japanese-gpt2-medium\"\n",
    "\n",
    "# トークナイザの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"語彙サイズ: {len(tokenizer)}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "\n",
    "# モデルの読み込み\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"\\nモデル: {MODEL_NAME}\")\n",
    "print(f\"レイヤー数: {model.config.n_layer}\")\n",
    "print(f\"隠れ層次元: {model.config.n_embd}\")\n",
    "print(f\"パラメータ数: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットのトークン化\n",
    "\n",
    "テキストデータをトークナイザで数値列に変換します。最大長は512トークンに設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 512\n",
    "TEXT_COL = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COL],\n",
    "        truncation=True,\n",
    "        max_length=BLOCK_SIZE,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[TEXT_COL]\n",
    ")\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[TEXT_COL]\n",
    ")\n",
    "\n",
    "print(f\"トークン化後の訓練データ: {tokenized_train}\")\n",
    "print(f\"トークン化後の評価データ: {tokenized_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRAの設定と適用\n",
    "\n",
    "LoRA (Low-Rank Adaptation) では、事前学習済みモデルの重みを凍結し、\n",
    "各対象層に低ランクの行列ペア（A, B）を追加して学習します。\n",
    "\n",
    "主要なハイパーパラメータ:\n",
    "- `r` (ランク): 低ランク行列の次元数。小さいほどパラメータ効率が高い\n",
    "- `lora_alpha`: スケーリングファクター。実際のスケールは `alpha / r` で決まる\n",
    "- `lora_dropout`: LoRA層に適用するドロップアウト率\n",
    "- `target_modules`: LoRAを適用する層の名前。GPT-2では `c_attn`（QKV射影）と `c_proj`（出力射影）が一般的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA設定\n",
    "LORA_RANK = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_TARGET_MODULES = [\"c_attn\", \"c_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# LoRAをモデルに適用\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 学習可能なパラメータ数を確認\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行\n",
    "\n",
    "HuggingFace Trainerを使用してLoRAファインチューニングを実行します。\n",
    "LoRAでは通常のフルファインチューニングよりも高い学習率（1e-4）を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習パラメータ\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.1\n",
    "WARMUP_STEPS = 100\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "OUTPUT_DIR = \"./models/rinna-gpt2-aozora-lora\"\n",
    "\n",
    "# ログ・評価間隔の計算（1エポックの1/10ごと）\n",
    "steps_per_epoch = len(tokenized_train) // (\n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    ")\n",
    "logging_steps = max(1, steps_per_epoch // 10)\n",
    "print(f\"1エポックあたりのステップ数: {steps_per_epoch}\")\n",
    "print(f\"ログ・評価間隔: {logging_steps} steps\")\n",
    "\n",
    "# データコレータ（言語モデル用）\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 学習設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=logging_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=logging_steps,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Trainerの作成と学習実行\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRAアダプターの保存\n",
    "\n",
    "学習済みのLoRAアダプターを保存します。LoRAではベースモデル全体ではなく、\n",
    "追加された低ランク行列のみが保存されるため、保存サイズが非常に小さくなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAアダプターの保存\n",
    "adapter_dir = Path(OUTPUT_DIR) / \"adapter\"\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(f\"LoRAアダプター保存先: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成テスト\n",
    "\n",
    "学習済みモデルを使って、いくつかのプロンプトからテキスト生成を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用プロンプト\n",
    "test_prompts = [\n",
    "    \"吾輩は猫である。名前はまだ無い。\",\n",
    "    \"明治時代の\",\n",
    "    \"東京の街には\",\n",
    "    \"先生は言った。「\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_part = generated_text[len(prompt):]\n",
    "    print(f\"[{i}] プロンプト: {prompt}\")\n",
    "    print(f\"    生成結果: {generated_part[:200]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、以下の手順でLoRAファインチューニングを実装しました:\n",
    "\n",
    "1. **データ準備**: 4.1節で前処理した青空文庫データを読み込み、訓練/評価に分割\n",
    "2. **モデル読み込み**: `rinna/japanese-gpt2-medium` をベースモデルとして使用\n",
    "3. **LoRA適用**: PEFTライブラリで低ランクアダプターを注入（学習パラメータ数を大幅に削減）\n",
    "4. **学習**: HuggingFace Trainerで効率的にファインチューニング\n",
    "5. **生成テスト**: 学習済みモデルでテキスト生成を確認\n",
    "\n",
    "LoRAの利点:\n",
    "- 学習パラメータ数がフルファインチューニングの1%未満\n",
    "- メモリ使用量が大幅に削減され、単一GPUでも大規模モデルの学習が可能\n",
    "- アダプターのみの保存で、ストレージ効率が高い"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
