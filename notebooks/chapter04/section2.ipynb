{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.2 データ並列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2章のコピー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "from typing import Iterator\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import transforms\n",
    "\n",
    "\n",
    "def create_padding_mask(pad_id: int, batch_tokens: Tensor):\n",
    "    # batch_tokens.shape == (batch_size, sequence_length)\n",
    "    mask = batch_tokens == pad_id\n",
    "    mask = mask.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_subsequent_mask(batch_tokens: Tensor):\n",
    "    sequence_len = batch_tokens.size(1)\n",
    "    mask = torch.triu(\n",
    "        torch.full((sequence_len, sequence_len), 1),\n",
    "        diagonal=1,\n",
    "    )\n",
    "    mask = mask == 1\n",
    "    mask = mask.unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def iter_corpus(\n",
    "    path: Path,\n",
    "    bos: str | None = \"<bos>\",\n",
    "    eos: str | None = \"<eos>\",\n",
    ") -> Iterator[list[str]]:\n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            if bos:\n",
    "                line = bos + \" \" + line\n",
    "            if eos:\n",
    "                line = line + \" \" + eos\n",
    "            yield line.split()\n",
    "\n",
    "\n",
    "# データの準備\n",
    "data_dir = Path(\"small_parallel_enja\")\n",
    "if not data_dir.exists():\n",
    "    !git clone https://github.com/odashi/small_parallel_enja.git {data_dir}\n",
    "\n",
    "train_ja = data_dir / \"train.ja.000\"\n",
    "train_en = data_dir / \"train.en.000\"\n",
    "train_tokens_ja = [tokens for tokens in iter_corpus(train_ja)]\n",
    "train_tokens_en = [tokens for tokens in iter_corpus(train_en)]\n",
    "vocab_ja = build_vocab_from_iterator(\n",
    "    iterator=train_tokens_ja,\n",
    "    specials=(\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"),\n",
    ")\n",
    "vocab_ja.set_default_index(vocab_ja[\"<unk>\"])\n",
    "vocab_en = build_vocab_from_iterator(\n",
    "    iterator=train_tokens_en,\n",
    "    specials=(\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"),\n",
    ")\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "\n",
    "src_transforms = transforms.Sequential(\n",
    "    transforms.VocabTransform(vocab_ja),\n",
    "    transforms.ToTensor(padding_value=vocab_ja[\"<pad>\"]),\n",
    ")\n",
    "tgt_transforms = transforms.Sequential(\n",
    "    transforms.VocabTransform(vocab_en),\n",
    "    transforms.ToTensor(padding_value=vocab_en[\"<pad>\"]),\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    src_texts, tgt_texts = [], []\n",
    "    for s, t in batch:\n",
    "        src_texts.append(s)\n",
    "        tgt_texts.append(t)\n",
    "\n",
    "    src_texts = src_transforms(src_texts)\n",
    "    tgt_texts = tgt_transforms(tgt_texts)\n",
    "\n",
    "    return src_texts, tgt_texts\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    list(zip(train_tokens_ja, train_tokens_en)),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from llm_from_scratch.transformer.transformer import Transformer\n",
    "\n",
    "\n",
    "def train(rank, n_gpu, batch_size, n_epochs, train_dataset):\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=n_gpu)\n",
    "    # create local model\n",
    "    embedding_dim = 512\n",
    "    n_blocks = 6\n",
    "    n_heads = 8\n",
    "    expansion_rate = 1\n",
    "\n",
    "    # 語彙数を取得\n",
    "    src_vocab_size = len(vocab_ja)\n",
    "    tgt_vocab_size = len(vocab_en)\n",
    "\n",
    "    # 最も長い文章の長さを取得\n",
    "    max_len_ja = len(max(train_tokens_ja, key=lambda x: len(x)))\n",
    "    max_len_en = len(max(train_tokens_en, key=lambda x: len(x)))\n",
    "    max_length = max(max_len_ja, max_len_en)\n",
    "\n",
    "    model = Transformer(\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        max_sequence_len=max_length,\n",
    "        d_model=embedding_dim,\n",
    "        n_blocks=n_blocks,\n",
    "        n_heads=n_heads,\n",
    "        d_k=embedding_dim,\n",
    "        d_v=embedding_dim,\n",
    "        d_ff=embedding_dim * expansion_rate,\n",
    "    )\n",
    "    # ここで各 rank の GPU にモデルを配置\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # rank ごとにデータを分割するためのサンプラーを作成\n",
    "    sampler = DistributedSampler(\n",
    "        train_dataset, num_replicas=n_gpu, rank=rank, shuffle=True\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    PAD_ID = vocab_ja[\"<pad>\"]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)  # クロスエントロピー\n",
    "    lr = 0.0001  # 学習率\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10.0, gamma=0.95)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (src_texts, tgt_texts) in enumerate(train_loader):\n",
    "            # tgt の入力は最後の単語を除く\n",
    "            tgt_input = tgt_texts[:, :-1]\n",
    "            # tgt の出力は最初の単語を除く\n",
    "            tgt_output = tgt_texts[:, 1:]\n",
    "            src_padding_mask = create_padding_mask(PAD_ID, src_texts)\n",
    "            tgt_padding_mask = create_padding_mask(PAD_ID, tgt_input)\n",
    "            tgt_subsequent_mask = create_subsequent_mask(tgt_input)\n",
    "            tgt_mask = tgt_padding_mask + tgt_subsequent_mask\n",
    "            # Tensor のデバイスを設定\n",
    "            src_texts, tgt_input, tgt_output = (\n",
    "                src_texts.to(rank),\n",
    "                tgt_input.to(rank),\n",
    "                tgt_output.to(rank),\n",
    "            )\n",
    "            src_padding_mask, tgt_mask = src_padding_mask.to(rank), tgt_mask.to(rank)\n",
    "\n",
    "            # モデル出力を取得\n",
    "            out = model(\n",
    "                src_texts, tgt_input, src_padding_mask, tgt_mask, src_padding_mask\n",
    "            )\n",
    "            # 出力と教師データを1次元に変換\n",
    "            out_flat = out.view(-1, out.size(-1))\n",
    "            tgt_flat = tgt_output.flatten()\n",
    "            # 誤差関数を計算\n",
    "            loss = criterion(out_flat, tgt_flat)\n",
    "            optimizer.zero_grad()\n",
    "            # 誤差逆伝播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "def main():\n",
    "    n_gpu = 2\n",
    "    batch_size = 64\n",
    "    n_epochs = 10\n",
    "    train_dataset = list(zip(train_tokens_ja, train_tokens_en))\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    mp.spawn(\n",
    "        train,\n",
    "        args=(n_gpu, batch_size, n_epochs, train_dataset),\n",
    "        nprocs=n_gpu,\n",
    "        join=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
