# LLM from Scratch

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®åŸºç¤ã‹ã‚‰å®Ÿè£…ã¾ã§ã‚’å­¦ã¶ãŸã‚ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‹ã‚‰GPTãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã¾ã§ã€æ®µéšçš„ã«ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹æˆ

### Chapter 2: Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
Transformerã®åŸºæœ¬çš„ãªä»•çµ„ã¿ã¨å®Ÿè£…ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚

### Chapter 3: GPTãƒ¢ãƒ‡ãƒ«
- `3_1_GPTãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦.md` - GPTã®åŸºæœ¬æ¦‚å¿µã¨é€²åŒ–ã®æ­´å²
- `3_2_Tokenizerã¨å…¥åŠ›å‡¦ç†.md` - ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
- `3_3_GPTãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’.md` - GPTãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¨å®Ÿè£…
- `3_4_ä»–ã®LLMã®ç´¹ä»‹.md` - ãã®ä»–ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆuvä½¿ç”¨ï¼‰

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/elith-co-jp/book-llm-from-scratch.git
cd book-llm-from-scratch

# uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã¾ã ã®å ´åˆï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
uv sync

# ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate  # Windows
```

### GPTãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ä¾‹

```python
from llm_from_scratch.gpt import (
    GPT, GPTConfig, SimpleTokenizer,
    create_dataloaders, GPTTrainer
)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
text = "Your training text here..."

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä½œæˆ
tokenizer = SimpleTokenizer(text)

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
train_loader, val_loader = create_dataloaders(
    text, tokenizer,
    block_size=64,
    batch_size=8
)

# ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã¨ä½œæˆ
config = GPTConfig(
    vocab_size=tokenizer.vocab_size,
    n_embd=128,
    n_layer=4,
    n_head=4,
    block_size=64
)

model = GPT(
    vocab_size=config.vocab_size,
    n_embd=config.n_embd,
    n_layer=config.n_layer,
    n_head=config.n_head,
    block_size=config.block_size
)

# å­¦ç¿’ã®å®Ÿè¡Œ
trainer = GPTTrainer(model, train_loader, val_loader)
trainer.train()
```

è©³ç´°ãªå®Ÿè¡Œä¾‹ã¯ `examples/train_gpt.py` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```bash
# ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
uv run python examples/train_gpt.py
```

## ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
book-llm-from-scratch/
â”œâ”€â”€ docs/                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â”‚   â”œâ”€â”€ chapter02/          # Transformerã®è§£èª¬
â”‚   â””â”€â”€ chapter03/          # GPTãƒ¢ãƒ‡ãƒ«ã®è§£èª¬
â”œâ”€â”€ llm_from_scratch/       # å®Ÿè£…ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ transformer/        # Transformerã®å®Ÿè£…
â”‚   â””â”€â”€ gpt/               # GPTãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
â”‚       â”œâ”€â”€ model.py       # GPTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
â”‚       â”œâ”€â”€ tokenizer.py   # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
â”‚       â”œâ”€â”€ dataset.py     # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚       â””â”€â”€ trainer.py     # å­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ examples/              # å®Ÿè¡Œä¾‹
â”‚   â””â”€â”€ train_gpt.py      # GPTå­¦ç¿’ã®ã‚µãƒ³ãƒ—ãƒ«
â”œâ”€â”€ notebooks/             # Jupyter notebooks
â”œâ”€â”€ tests/                # ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
â””â”€â”€ pyproject.toml        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã¨ä¾å­˜é–¢ä¿‚
```

## ğŸ”§ ä¸»è¦ãªæ©Ÿèƒ½

### GPTãƒ¢ãƒ‡ãƒ«å®Ÿè£…
- **ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**: åŠ¹ç‡çš„ãªæ–‡è„ˆç†è§£
- **Transformerãƒ–ãƒ­ãƒƒã‚¯**: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã¨residual connection
- **ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ä½ç½®æƒ…å ±
- **å› æœçš„ãƒã‚¹ã‚¯**: è‡ªå·±å›å¸°çš„ãªç”Ÿæˆã®ãŸã‚ã®æœªæ¥æƒ…å ±ã®ãƒã‚¹ã‚­ãƒ³ã‚°

### å­¦ç¿’æ©Ÿèƒ½
- **AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼**: é©å¿œçš„å­¦ç¿’ç‡ã¨é‡ã¿æ¸›è¡°
- **å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°**: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¨ã‚³ã‚µã‚¤ãƒ³æ¸›è¡°
- **å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**: å­¦ç¿’ã®å®‰å®šåŒ–
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨å¾©å…ƒ

### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- **Temperature sampling**: ç”Ÿæˆã®å¤šæ§˜æ€§åˆ¶å¾¡
- **Top-k sampling**: é«˜ç¢ºç‡ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- **è‡ªå·±å›å¸°ç”Ÿæˆ**: æ–‡è„ˆã«åŸºã¥ãé€æ¬¡çš„ãªãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ

## ğŸ“Š ãƒ¢ãƒ‡ãƒ«è¨­å®šä¾‹

### å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’ãƒ»å®Ÿé¨“ç”¨ï¼‰
```python
config = GPTConfig(
    vocab_size=1000,
    n_embd=128,
    n_layer=4,
    n_head=4,
    block_size=64
)
# ~0.5M parameters
```

### ä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
```python
config = GPTConfig(
    vocab_size=50257,
    n_embd=768,
    n_layer=12,
    n_head=12,
    block_size=1024
)
# ~124M parameters (GPT-2 smallç›¸å½“)
```

## ğŸ› ï¸ é–‹ç™ºç’°å¢ƒ

### å¿…è¦ãªä¾å­˜é–¢ä¿‚

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯[uv](https://github.com/astral-sh/uv)ã‚’ä½¿ç”¨ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’ç®¡ç†ã—ã¦ã„ã¾ã™ã€‚ä¸»ãªä¾å­˜é–¢ä¿‚ï¼š

- Python >= 3.9, < 3.12
- PyTorch >= 2.1.1
- NumPy >= 1.26.2
- matplotlib >= 3.8.2
- tqdm >= 4.66.2
- TensorBoard >= 2.12.0
- Jupyter Notebook >= 7.0.6

### uvã‚’ä½¿ã£ãŸé–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -LsSf https://astral.sh/uv/install.sh | sh

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
uv sync

# é–‹ç™ºç”¨ä¾å­˜é–¢ä¿‚ã‚‚å«ã‚ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
uv sync --dev

# Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®èµ·å‹•
uv run jupyter notebook

# ã‚³ãƒ¼ãƒ‰ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆé–‹ç™ºæ™‚ï¼‰
uv run black llm_from_scratch/
uv run isort llm_from_scratch/

# ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆé–‹ç™ºæ™‚ï¼‰
uv run pytest tests/
```

### Pythonç’°å¢ƒã®ç®¡ç†

```bash
# æ–°ã—ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è¿½åŠ 
uv add package_name

# é–‹ç™ºç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è¿½åŠ 
uv add --dev package_name

# ç‰¹å®šã®Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨
uv python pin 3.11
```

## ğŸ”— å‚è€ƒè³‡æ–™

- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathyã«ã‚ˆã‚‹æœ€å°é™ã®GPTå®Ÿè£…
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformerã®åŸè«–æ–‡
- [Language Models are Unsupervised Multitask Learners](https://openai.com/research/better-language-models) - GPT-2ã®è«–æ–‡
- [uv](https://github.com/astral-sh/uv) - é«˜é€ŸãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ¤ è²¢çŒ®

Issueå ±å‘Šã‚„Pull Requestã‚’æ­“è¿ã—ã¾ã™ã€‚å¤§ããªå¤‰æ›´ã‚’è¡Œã†å ´åˆã¯ã€ã¾ãšIssueã‚’é–‹ã„ã¦å¤‰æ›´å†…å®¹ã«ã¤ã„ã¦è­°è«–ã—ã¦ãã ã•ã„ã€‚

## âœ‰ï¸ ãŠå•ã„åˆã‚ã›

è³ªå•ã‚„ææ¡ˆãŒã‚ã‚‹å ´åˆã¯ã€GitHubã®Issueãƒšãƒ¼ã‚¸ã‹ã‚‰ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚