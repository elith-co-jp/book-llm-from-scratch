# 第3章

# GPTモデル の作成

![](_page_0_Picture_2.jpeg)

**本章では、Transformerのデコーダのみを使用した GPTモデ ルについて解説します。まず、GPTという名前の意味、GPT-1 からGPT-5までの変遷などを紹介します。次に、テキストを数値 に変換するトークナイザーの仕組みを解説したあと、実際に GPTモデルを実装します。データセットの準備、モデルアーキテ クチャの定義、学習ループの実装、そしてテキスト生成まで、一 連のプロセスを体験できます。日本語での学習例として、夏目 漱石の作品を使ったサンプルも用意しています。最後に、GPT 以外の主要な LLM(BERT、T5、LLaMAなど)についても簡単 に紹介します。**

# 3.1 GPTモデルの概要

 本節では、自然言語処理の分野で革新をもたらした GPT( Generative Pre-trained Transformer)モデルについて詳しく解説します。まず、3.1.1項で GPTの基本概念と構造を解説し、 なぜ高い言語理解能力と生成能力を持つのかを説明します。次に、3.1.2項では GPTモデルの進化 の過程を追い、各バージョンの特徴と性能向上の要因を見ていきます。最後に、3.1.3項で GPTモデ ルの具体的な応用例を紹介し、多様な分野での革新的な活用方法を探ります。本章を通じて、GPT 技術の基礎から応用までの全体像を把握できるでしょう。

### 3.1.1 GPTモデルの概要

本書で実際に実装していくGPTモデルについて、その基本概念と構造を解説します。GPTモデ ルの各要素を詳しく見ていくことで、このモデルが、なぜ高い言語理解能力と生成能力を持つのかを 理解できます。

#### GPTとは

 GPTは「Generative Pre-trained Transformer」の略称です。GPTという言葉は 2018年に OpenAI が発表した論文「Improving Language Understanding by Generative Pre-Training」で初めて提案され、2022年11月30日にリリースされた ChatGPTによって広く認知される ようになりました。ChatGPTは公開から 2カ月で 1億ユーザーを突破し、史上最速で成長したコン シューマーサービスとなりました。「Generative Pre-trained Transformer」を直訳すると「生成的 な事前学習された Transformer」となりますが、この分野の論文を普段から読み慣れていないと、 意味は分かりにくいでしょう。それぞれの用語について詳しく見ていきましょう。

#### Generative(生成的)

 生成的とは、新しいデータを生成する能力を持つことを指します。GPTモデルはテキストを生成す るために設計されており、利用者の入力に応じて自然な文章を作成します。特に、GPTモデルは次の 単語を予測することで連続したテキストを生成する能力に優れており、対話型のアプリケーションや文 章生成タスクにおいて高い性能を発揮します。

#### Pre-trained(事前学習された)

 事前学習とは、大量のデータセットを使ってモデルをあらかじめ訓練するプロセスです。GPTモデ ルは膨大な量のテキストデータを用いて事前に学習されており、この基礎知識を基に様々なタスクに 対応できます。事前学習の過程では、モデルは文法や文脈、一般的な知識を習得し、その後のファイ ンチューニング(微調整)によって特定のタスクに適応させられます [2]。

 具体的にどのようなデータセットが GPTモデルの事前学習に利用されているかを見てみましょう。 OpenAIの GPTシリーズの事前学習には、以下のような大規模なテキストデータセットが使用されて きました。

- 1. CommonCrawl: Web全体からクロールされた大量のデータセットです。様々なトピッ クが含まれており、収集されたデータは WARC(Web ARChive)形式で保存されて います。このデータには HTMLや画像など、様々な形式のデータが含まれており、多様 な言語表現や情報を学習するために利用されます。
- 2. BooksCorpus: 約1万1000冊の小説から構成されるデータセットです。長文の文脈 や物語の流れを理解するためのトレーニングに適しています。
- 3. Wikipedia: 多言語で豊富な情報を提供するオンライン百科事典です。信頼性の高い 知識をモデルに供給するために使用されます。
- 4. OpenWebText: 次に示す非公開の WebTextデータセットと同様、Reddit上で共 有されたリンク先のコンテンツを元にしたオープンソースのデータセットです。ニュース 記事やブログ、技術文書など、様々なジャンルかつ高品質なテキストデータが含まれてい ます。
- 5. WebText: OpenAIが作成した高品質なオンライン記事をまとめたデータセットで、 Reddit上で共有されたリンク先をたどって収集されたコンテンツが含まれています。イ ンターネット上の多様なコンテンツを反映していますが、具体的なデータセットは公開さ れていません。

 これらのデータセットは、単一のデータソースに依存せず、多様な言語リソースを統合することで、 AIのモデルが広範な言語知識を習得できるように設計されています。このような大規模かつ多様な データセットに基づいて事前学習された GPTモデルは、非常に高い言語理解能力を持ち、様々なタ スクに対して効果的に応用できます。

 さらに、事前学習には教師なし学習が用いられ、文脈を理解するために自己教師あり学習の手法 が採用されます。具体的には、文章の一部を隠して、その隠された部分を予測するタスクを通じて、モ デルは文脈の理解と単語の予測能力を向上させます。これにより、GPTモデルは広範なトピックに関 する知識を蓄積し、様々な自然言語処理タスクにおいて高い性能を発揮するのです。

#### Transformer

 Transformerは第2章で解説したように、自然言語処理(NLP)における比較的最新のアーキテ クチャであり、高い性能を発揮します。Transformerは、アテンション機構(Attention Mechanism) を用いてテキストデータの意味を効率的に捉えることができます。

#### GPTのアーキテクチャ

 第2章では Transformerアーキテクチャについて、実際に作りながら解説しました。ここで理解し てほしいのは、Transformerが採用しているエンコーダ・デコーダ型のアーキテクチャです。エンコー ダ・デコーダ型アーキテクチャは、自然言語処理や機械翻訳にとどまらず U-Net や VAE (Variational Autoencoder)といった画像処理モデルでも広く使われています。エンコーダは入力 データを受け取り、それを内部表現に変換(エンコード)します。デコーダはこの内部表現を基にして、 目的の出力を生成(デコード)します。

 2022年終わりの ChatGPTが到来する少し前には、BERTなど、エンコーダ部分だけを使ったアー キテクチャの研究が盛んに行われていました。このエンコーダ型アーキテクチャを採用した BERT は、入力テキストの文脈を理解するために設計されています。一方の GPTは、デコーダ部分だけを 利用していて、与えられた文脈に基づいて次の単語を予測する生成タスクに優れています。

#### GPTの特性と応用

 GPTの主な特性の一つは、そのスケーラビリティです。モデルの規模を大きくすることで、より高度 な言語理解力を獲得し、様々なタスクにおいて優れた性能を発揮します。例えば、GPT-3では 1750 億のパラメータが使用されており、その膨大なパラメータ数が高度な文章生成能力を支えています。 ただし、ここで重要なのが、モデルの規模だけでなく、使用するデータのサイズや計算リソースも性能 向上に寄与するということです。これらの要素、すなわちパラメータ数、データサイズ、計算リソースの 3つがモデルの性能に影響を与える法則をスケーリング則と呼びます。

 GPTは汎用性の高いモデルであり、質問応答、翻訳、要約生成、クリエイティブな文章作成など、 幅広い応用が可能です。特に、少数の例示から新しいタスクを学習する Few-shot 学習の能力によ り、ファインチューニングなしでも多様なタスクに対応できる点が注目されています。

 Transformerのアーキテクチャの採用により、GPTは「離れた文章」の依存関係を効果的に捉え ることが可能であり、文脈理解や一貫性のある長文生成において優れた性能を示します。この特性 が、GPTの多様なタスクへの適用可能性を高めています。

#### まとめ

 GPTモデルは、大規模な事前学習と効率的な Transformerアーキテクチャにより、高度な言語理 解と生成能力を実現しています。多様なデータセットを用いた事前学習により、幅広い知識を獲得し、

様々なタスクに適用できる汎用性を持っています。デコーダ型のアーキテクチャを採用することで、文 脈に基づいた自然な文章生成が可能となり、多岐にわたる自然言語処理タスクで高い性能を発揮し ます。GPTモデルの登場により、AI技術の応用範囲は大きく拡大し、今後もさらなる発展が期待され ています。

## 3.1.2 GPTモデルの変遷

 GPTモデルは、その進化とともに大幅な性能向上を遂げてきました。各バージョンの進化は、モデル のサイズ、学習データセット、アーキテクチャの改良などに基づいています。ここでは、GPT-1 から GPT-5までの各バージョンの特徴と進化について見ていきます。

#### GPT-1

 GPT-1は、2018年に OpenAIが発表した初めての GPTモデルです。約1億1700万のパラメータ を持ち、BooksCorpusデータセットを用いて事前学習されました。12層の Transformerデコーダを 用い、自己アテンション機構(Self-Attention Mechanism)を活用して文脈を理解する構造を持っ ています。質問応答、意味的類似性評価、含意判定、テキスト分類などの自然言語処理タスクで有効 性を示しました。

#### GPT-2

 2019年に発表された GPT-2は、GPT-1から大幅にスケールアップされ、15億パラメータを持ちま す。WebTextデータセットを使用して事前学習され、テキスト生成能力が飛躍的に向上しました。 Transformerデコーダの層は 48層に増やし、各層にマルチヘッドアテンション機構(Multi-Head Attention)と位置エンコーディング(Positional Encoding)を組み込みました。その高い生成能力 から、OpenAIはフェイクニュースやなりすましなどの安全性懸念を提起しました。

#### GPT-3

 2020 年にリリースされた GPT-3 は、1750 億ものパラメータを持つ大規模モデルです。 CommonCrawl、WebText2、English Wikipedia、Books1、Books2からなる多様で大量のデー タで事前学習されました。96層の Transformerデコーダを用い、自己アテンション機構とフィードフォ ワードネットワークを強化しました。Zero-shot、One-shot、Few-shotのいずれの形式のプロンプトで も高い性能を示し、テキスト生成、質問応答、翻訳、要約など様々なタスクで人間に近い性能を発揮し ます。

#### GPT-4

 2023年に発表された GPT-4は、GPT-3をさらに改良したモデルです。より多くのパラメータと改良 されたアーキテクチャにより、学習効率や性能が向上しています。深い文脈理解、多言語対応能力、 安全性や倫理面への考慮が特徴です。アーキテクチャにおいては、レイヤー間の情報伝達を効率化 する新しいアテンション機構を導入したり、動的な位置エンコーディングを改善したりしています。

 その後、GPT-4 Turbo(2023年11月)で 12万8000トークンのコンテキストウィンドウに拡張され、 GPT-4V(2024年初頭)で画像理解機能が追加されました。そして 2024年5月の GPT-4oでは、テ キスト、画像、音声、動画を統合的に処理できる本格的なマルチモーダルモデルへと進化しました。こ れにより、GPTは純粋なテキストモデルから、複数の入力形式を扱える汎用AI へと変貌を遂げまし た。

#### GPT-5

 2025年8月にリリースされた GPT-5は、GPT-4の技術を大幅に拡張した OpenAIの最新モデル です。最大40万トークン(API版)のコンテキストウィンドウを実現し、テキスト、画像、音声、動画の処理 を単一のアーキテクチャで統合しました。

 最大の特徴は、質問の難易度に応じて処理方法を自動的に切り替える機能です。簡単な質問に は従来通り即座に回答し、数学の証明や複雑なコード生成のような難しい問題には、段階的に考える 「思考モード」を自動的に起動します。ユーザーは特別な設定をする必要がなく、システムが問題の 複雑さを判断して最適な処理を選択します。

#### まとめ

 GPTシリーズは、2018年の GPT-1(1.17億パラメータ)から 2025年の GPT-5まで、着実な進化を 遂げてきました。初期の GPT-1と GPT-2では主にテキスト生成能力の向上に焦点が当てられ、 GPT-3(1750億パラメータ)で Few-shot学習による汎用性を実現しました。

 GPT-4以降は大きな転換点となり、テキスト処理だけでなくマルチモーダル機能が搭載されました。 特に GPT-4o(2024年5月)ではテキスト、画像、音声、動画の統合処理が可能となり、GPT-5(2025 年8月)では 40万トークンのコンテキストウィンドウと適応的処理システムにより、単純なタスクから複雑 な推論まで自動的に最適な処理を選択できるようになりました。

 パラメータ数は 1億から数千億規模へと増大し、同時にマルチモーダル処理や動的な計算リソー ス配分といった新しい技術が導入されました。これらの技術的進化により、GPTはテキスト生成に加 えて、GPT-4以降では画像、音声、動画を処理するマルチモーダル機能を搭載しています。

### 3.1.3 GPTモデルの応用例

 GPTモデルは、その高度な言語理解能力と生成能力を生かし、様々な分野で革新的な応用が進 められています。本項では、自然言語処理、視覚情報処理、音声処理の各分野における GPTモデ ルの具体的な応用例を紹介します。これらの例を通じて、GPT技術がいかに多様な課題解決に貢献 し、私たちの生活や仕事を変革する可能性があるかを理解できるでしょう。

#### 自然言語処理タスク

 GPTモデルは、多岐にわたる自然言語処理(NLP)タスクに応用されています。以下は、その代表 的な例です。

- ・テキスト生成:GPTモデルは、特定のテーマやスタイルに基づいて自然な文章を生成でき ます。これにより、ブログ記事、物語、詩、技術文書など、様々な種類のテキストを自動生成 できます。最新モデルである GPT-5は 40万トークンまで文脈を保持できるため、長編小 説のような一貫性が求められる文章でも破綻なく生成できるようになりました。
- ・質問応答:GPTモデルは、与えられた文脈や知識ベースに基づいて、質問に対する適切な 回答を生成できます。これにより、カスタマーサポートや教育分野での自動応答システムの 構築が進められています。
- ・翻訳:GPTモデルは、多言語に対応しており、テキスト1つの言語から別の言語に翻訳でき ます。特に、文脈を理解した自然な翻訳を生成する能力に優れています。
- ・要約:長文のテキストを短く要約するタスクでも、GPTモデルは有用です。ニュース記事や 研究論文の要約生成に活用され、情報を効率的に取得する手段として注目されています。

#### 画像処理タスク

- ・画像キャプション生成:画像を解析し、その画像の説明を生成する場合においても GPTモ デルが使えます。ニュース写真に対して自動的にキャプションを生成することで、ニュース 記事の作成を効率化したり、オンラインショッピングサイトにおいて商品画像にキャプショ ンをつけることで、商品の特徴の説明文を自動生成したりできます。GPT-4以降のマルチ モーダル機能により、画像とテキストを同時に理解し、より文脈に即した説明が可能になっ ています。
- ・視覚推論:GPTモデルはさらに画像を解析することで、視覚情報を認識した上で推論する こともできます。

#### 動画処理タスク

・リアルタイムビデオ解析:GPT-4o以降のモデルでは、リアルタイムでビデオを解析するこ とができ、スポーツの実況解説や、ビデオ会議のリアルタイム翻訳など様々な応用の可能 性が注目されています。

#### 音声処理タスク

- ・音声認識:GPT-4o 以降のモデルは音声も入力として受け付けており、対話することがで きます。これにより、これまで文章入力のみだったカスタマーサポートや教育分野における 自動応答システムがさらに進化すると考えられます。
- ・音声生成:GPT-4o以降のモデルは音声を入力として受け付けているだけでなく、音声の 出力もできます。そのため、音声のみを介した対話が可能となり、あらゆる対人のサービス においてカスタマー体験向上に貢献することが期待されます。

#### まとめ

 GPTモデルの応用範囲は、テキスト生成や質問応答といった従来の自然言語処理タスクから、テ キスト、画像、音声などを統合的に扱うマルチモーダル処理を前提に、自律的にタスクを計画・実行する 「AIエージェント」としての活用が現実のものとなっています。引き続き今後も各分野でのさらなる精 度向上や、新たな応用領域の開拓が期待されます。同時に、その影響力の大きさから、AIの自律性 をどう制御するかというガバナンスの確立や、社会システムへの安全な実装方法が、喫緊の課題とし て議論されています。