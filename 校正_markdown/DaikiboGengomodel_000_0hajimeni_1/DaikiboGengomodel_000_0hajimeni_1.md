## はじめに

### ChatGPTの衝撃 - なぜ今、LLMを学ぶのか

2022年11月30日、OpenAIが ChatGPTを公開した瞬間、世界は変わりました。

 公開からわずか 5日で 100万ユーザー、2カ月で 1億ユーザーを突破。Instagramが同じ規模に 達するまでに 2年半、TikTokでも9カ月かかったことを考えると、これがいかに異常な速度だったか がわかります。

ChatGPTの何が私たちをこれほど驚かせたのでしょうか。

 それは、人間と自然に対話できるAIが、ついに実現したという事実です。過去の AIシステムは特 定のタスクに特化していました。画像認識AI、翻訳AI、チェスAI――それぞれが別々のシステムでし た。一方の ChatGPTは、質問応答、文章生成、翻訳、コード生成、要約、創作など、あらゆるタスクを 単一のモデルでこなします。まるで知識豊富な友人と会話しているかのような自然さで。

 この革新的な技術の背後にあるのが、大規模言語モデル(Large Language Model:LLM)で す。

 LLMは膨大なテキストデータから言語のパターンを学習し、文脈を理解し、適切な応答を生成す る能力を獲得しています。本書は、この驚くべき技術を基礎から理解し、自分の手で実装できるように なることを目指します。

#### 本書の特徴と学べること

 本書は、LLMの基礎理論から実装まで、体系的に学べることを目指しています。単に概念を説明 するだけでなく、実際に動くコードを通じて、LLMの仕組みを深く理解できるように構成されています。

### 相当

- ・一から LLM(GPT-2)を作りながら学べる:Transformerの基礎から GPTモデルの実 装まで、段階的にコードを書きながら理解を深めます
- ・アラインメントの仕組みを実装で学べる:GPT-2だけでは実現できなかった「人間の意図 に沿った応答」を生成するための技術(SFT、DPO)を、実際のコードで体験します
- ・最新の推論モデルまでカバー:Chain-of-Thought(CoT)や推論強化モデルなど、 2024年以降の最新技術についても解説します

#### この本で学べること

- ・Transformerアーキテクチャの詳細な仕組み:アテンション機構、位置エンコーディング、 フィードフォワード層など、Transformerを構成するすべての要素を理解
- ・GPTモデルの学習と推論:「nanoGPT」をベースに、GPTモデルをゼロから実装し、実 際のデータで学習させる方法
- ・Tokenizer(トークナイザー)の実装:BPE、WordPiece、SentencePieceなど、テ キストを数値に変換する仕組み
- ・大規模学習のテクニック:Scaling Law(スケーリング則)、データセットの準備と前処理、 学習の効率化(LoRA)
- ・アラインメント技術:インストラクションチューニング(SFT)と DPOによる、人間の好みへ の適合方法
- ・実践的な実装スキル:PyTorchを使った実装、学習ループの構築、モデルの評価

#### できるようになること

本書を読み終えた後、以下のことができるようになります。

- ・Transformerモデルをゼロから実装し、翻訳タスクで学習させる
- ・GPTモデルを自分のデータで学習させ、文章を生成させる
- ・インストラクションチューニングにより、モデルを特定のタスクに適応させる
- ・DPO(Direct Preference Optimization)により、モデルの出力を人間の好みに合 わせる
- ・LoRAなどの効率化技術を使って、限られた計算資源でファインチューニングを行う

#### 本書の構成スタイル

本書は、各トピックについて以下の 3段階で説明します。

- 1. おおまかな説明:概念を直感的に理解するための説明や図
- 2. 数式を含む詳細な説明:理論的な背景を数式で厳密に理解
- 3. 実装コード:実際に動くPyTorchコードで実践的に理解

 この 3段階のアプローチにより、「何となくわかった」ではなく、「理論も実装も理解した」という深い 理解を得られます。数式が苦手な方は、おおまかな説明と実装コードを中心に読み進めることもできま すし、理論を深く理解したい方は数式の部分もじっくり読むことで、より深い洞察を得られます。

#### 対象読者

本書は、以下のような方を主な読者として想定しています。

- ・LLMの仕組みを根本から理解したいエンジニア:API(Application Programming Interface)を叩くだけでなく、内部で何が起きているのかを知りたい方
- ・機械学習・深層学習を学び始めた方:ニューラルネットワークの基礎は知っているが、最新 の LLM技術を体系的に学びたい方
- ・AI技術に興味のある学生:将来AI分野でキャリアを築きたいと考えている方

#### 前提知識

本書を読み進めるにあたり、以下の知識があると理解がスムーズです。

- ・Pythonの基礎:関数、クラス、リスト内包表記など、基本的な文法を理解していること
- ・PyTorchの基礎:テンソル操作、nn.Module、学習ループの基本的な書き方を知ってい ること(基本的な内容は巻末の付録で解説しています)
- ・機械学習の基礎概念:損失関数、勾配降下法、過学習といった基本用語を理解していること
- ・線形代数の基礎:行列の積、転置、内積などの基本的な演算を知っていること(本稿の最後 に基礎的な解説があります) 次ページ

 これらの前提知識に不安がある場合でも、コードを動かしながら読み進めることで、徐々に理解を 深めることができます。必要に応じて、PyTorchの公式チュートリアルや機械学習の入門書を参照し てください。

#### 謝辞

本書の執筆にあたり、多くの方々のご支援とご協力をいただきました。

#### ■■■■■(初校で追加)

 最後に、本書を手に取ってくださった読者の皆様に感謝いたします。LLMという急速に発展する 分野において、本書が皆様の学習の一助となれば幸いです。

# 線形代数に関する補足

 ここでは本書で現れる数式に関する表記方法の説明に加えて、いくつかの概念についての説明 を記載します。本文を読んでいる際にわからない点などがあれば、適宜参照してください。

#### ベクトル

ベクトルは横ベクトルを標準として太字で *v* , *w* のように表します。

例えば *n* 次元ベクトル*v* は次のようなベクトルを表します。

$$\boldsymbol{v}=(v_1,v_2,\cdots,v_n)$$

#### 行列

行列は通常の太さで *A* , *B* のように表します。

 行列の転置は *A*<sup>Τ</sup> のようにΤをつけて表します。ベクトルにΤがついている場合は、ベクトルを 1行 の行列とみなして転置しています。

#### 行列のランク

 ランクについて理解するために行列を、ベクトルをベクトルに変換する装置として捉えてみましょう。 なおランクは、4.4節(LoRA)を解説するときにだけ出てくる概念です。

例えば、次のような 2行2列の行列と2次元ベクトルを考えます。

$$A=\begin{pmatrix} 1 & 1 \ 2 & 2 \end{pmatrix}, \; \boldsymbol{v}=(1,1)$$

ベクトル*v* は *A* を用いて次のように 2次元ベクトルに変換できます。

$$vA = (3,3)$$

 では、*v* を 2次元の様々なベクトルに変更すれば、*v* は 2次元中のあらゆるベクトルを表現できるで しょうか。試しに、*v* = (*a, b* )と文字で置いて計算してみましょう。

$$vA = (a+2b, a+2b)$$

 式からわかるように、*a , b* の値をどのように変えても、表現できるのは 2つの要素の値が等しいベ クトル、つまりある直線上を指すようなベクトルだけです。

このように、行列による変換では変換後のベクトルの次元に関わらず、実際に表現できる幅が狭まっ

ている場合があります。この、表現できる幅のことをランクと呼びます。つまり、ランクが低い行列という のは、ベクトルの変換装置として使った場合、情報の損失が大きい行列ということです。情報の損失 が大きいというと、悪いことのように感じますが、実際は悪いことだけではありません。計算機的にはラ ンクの低い行列というのは省メモリで保持できたり、機械学習的には学習する要素が少ないため安定 して早く学習できたりするといった利点があります。本書で扱うLoRAという手法もこのような性質を 利用しています。

#### 内積

*n* 次元実ベクトル*v* , *w* 内積は *v*・*w* と表します。特に断らない場合この内積は次式で与えられ る標準内積とします。

の

$$\boldsymbol{v}\cdot\boldsymbol{w}=\sum_{i=1}^n v_i w_i$$