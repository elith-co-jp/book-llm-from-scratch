# 第5章

### アラインメント

**本章では、大規模言語モデルのアラインメント(Alignment)に ついて説明します。「Alignment」という単語自体には、調整する といった意味があります。LLMにおけるアラインメントは、人間 の意図や価値観に沿うように調整するといった意味で使われま す。これまでに扱ったのは次のトークンを予測するようなモデル の学習でしたが、何をすれば人間の意図や価値観とマッチした モデルが学習できるのでしょうか。その答えに入る前に 5.1節で はアラインメントの必要性や目的について説明します。その後、 5.2、5.3節で実際にアラインメントを実現する方法の説明と、4 章までに学習したモデルに対するアラインメントを行います。**

## 5.1 アラインメントの基礎

### 5.1.1 AIアラインメント

 人間の価値観に沿うアラインメントの必要性は、LLMの台頭以前から言及されていました。特に、 汎用人工知能( Artificial General Intelligence: AGI )や人工超知能( Artificial Super Intelligence: ASI)のような強力な AIの文脈で扱われることが多いです。

 ではなぜ、強力な AIではアラインメントが重要になるのでしょうか。その理由の 1つは、十分賢い AIは指示に対して人間の想定と異なる解法を思いついてしまうためです。

 有名な例として、スウェーデンの哲学者Nick Bostrom氏が挙げた「ペーパークリップ問題」があり ます。

 できるだけ多くのペーパークリップを作ることだけを目的とする AIがあるとします。AIは、人間 がいない方がずっとよいことにすぐに気付くでしょう。なぜなら、人間がスイッチを切るかもしれな いからです。人間がスイッチを切ると、ペーパークリップの数が少なくなってしまいます。また、人体 にはペーパークリップにできる原子がたくさん含まれています。AIが目指す未来は、ペーパーク リップはたくさんあるが人間がいない未来です。

 読者の中には、指示が悪かったと考える人もいるでしょう。当然、上述の問題が実際に起こるとは考 えにくいですが、実際にはより複雑な形で同じような問題が起きうるのです。そのため、人間と同等以 上に賢い AIができる前には、それに伴うリスクをなくす方法が必要になります。

 ペーパークリップの例とも関連しますが、Bostrom氏は AIに関する知的能力と倫理観の直交性に も言及しています。これは賢さと倫理観は無関係であり、あらゆる賢さとあらゆる倫理観の組み合わ せがありうるという考えです **<sup>1</sup>**。人間で例えると、賢いからといって善人とは限らず、無知だからといっ て悪人とは限らないということです。AIにおいては、特に賢い場合に倫理観が人間とズレているとリス クが大きいため、アラインメントが重要になります。

**<sup>1</sup>** Orthogonal thesisといいます。これは数理的に証明されたものではなく、考えの 1つですので、反対に賢さと比例 して倫理観が身につくと主張する研究者もいます。

#### 5.1.2 LLMアラインメント

LLMのアラインメントは、以下の 2通りに大別されます。

- 1. 人間の指示通りにタスクをこなせるよう調整する
- 2. 人間の嗜好に合った応答をするよう調整する

 本書では 1の手法としてインストラクションチューニング、2の手法として人間のフィードバックを用い た強化学習(Reinforcement Learning from Human Feedback: RLHF)を扱います。

 事前学習では、文章から次のトークンを予測することを通して学習していました。4章でも述べた通 り、このような学習をしたモデルは、コンテキストとして文章を与えるとその続きを生成します。このような LLMも、適切に用いることで一部の自然言語処理タスクを解くことはできます。一方、ChatGPTのよ うに対話形式でタスクを与えてタスクを解かせることはできません。これを実現するのがインストラクショ ンチューニングです。

 詳細は 5.2節で説明しますが、インストラクションチューニングはモデルにタスクの指示文と正解とな る応答例を見せることで学習を行います。そのため、どのような応答が望ましいのかは学習できる反 面、どのような応答が望ましくないのかというネガティブフィードバックを与えることができません。5.1.1 項で述べた、価値観に沿うような学習を行うためには、してはいけないことについての学習も必要で す。RLHFは応答例に対する人間の評価を通して学習するため、モデルに対してネガティブフィード バックが可能になります。

 LLMにおいてアラインメントが必要になる背景には、学習データの問題があります。4章で説明し た通りLLMの学習データの多くはインターネットからクローリングされたデータです。このようなデータ にはハッキングの方法、爆弾の作り方などの悪意ある情報や、罵詈雑言のような望ましくないテキスト、 嘘の情報が含まれます。また、インターネット以外のデータにおいても、人間の生み出してきた文章に は、人間社会におけるバイアスが反映されてしまいます。実際、適切なアラインメントができていない LLMにおいては性別や宗教に関するステレオタイプを示すことが実験的にもわかっています。

 以上のような背景から、LLMにおいては AIが賢すぎることに起因する問題の他に、データによる 問題を解消する必要があり、アラインメントが必須の技術になります。