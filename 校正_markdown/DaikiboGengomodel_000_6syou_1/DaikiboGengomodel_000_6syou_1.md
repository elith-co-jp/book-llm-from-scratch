# 第6章

### 推論モデル

**エクセルでは、セル□に入力したデータが□入力した通りに表 示□ず、勝手に変換され□困ることがある。例●ば、「1/4」と 入□したいのに「1月4□日」となるケースだ□。また、桁の大き い□数値は、入力したま■まの表示では読みづ□らい。できれ ば、10□万5000円などと□万」を入れて表示し□たい。これら を実現●できるのが「表示形□」だ。入力したデー□タの見せ 方を自由自□に操れる(図1)。□同じ数値でも、表示■形式の 設定次第で、□さまざまな見せ方が□できる(図2)。さ□らに工 夫しだいで、□条件に合ったセルの●色を変えるといった□高 度なワザもある。□回は、その使い方を□習得しよう。セルを□**

## 6.1 推論モデルとは何か

 3.4節で紹介した通り、2024年後半から 2025年にかけて、LLMの新しいカテゴリーとして「推論強 化モデル」が登場しました。OpenAIの o1/o3シリーズ、DeepSeek-R1、Gemini 2.5 Pro、Claude 3.7、Mistral AIの Magistralなど、各企業が推論特化型のモデルを相次いで発表しています。こ れらのモデルは、数学、科学、プログラミングといった複雑な推論を要するタスクにおいて、従来の LLMを大幅に上回る性能を示しています。

 従来の LLMは、大量のテキストデータから学習することで幅広い知識を獲得し、人間の質問に流 暢に応答できるようになりました。しかし、これらのモデルは基本的に「次のトークンを予測する」という 仕組みで動作しているため、複雑な問題に対して段階的に考えを深めていくような推論プロセスが 不足していました。例えば、数学の証明問題や複雑なプログラミングタスクでは、最終的な答えに到達 するまでに複数のステップを踏んで論理的に思考を進める必要がありますが、従来の LLMはこのよ うな段階的推論が苦手でした。

 推論強化モデルは、この課題を克服するために設計されています。これらのモデルの最大の特徴 は、応答を生成する前に内部で「思考プロセス」を経るという点です。ユーザーに最終的な答えを提 示する前に、モデルは問題を分析し、複数のアプローチを検討し、段階的に解決策を構築していきま す。このプロセスは、人間が難しい問題に取り組む際に「まず問題を理解し、次に解決策の方針を立 て、それから具体的な手順を実行する」という思考パターンに似ています。

 推論強化モデルの性能向上は顕著です。例えば、OpenAI o3は MATHベンチマーク(高校・大 学レベルの数学問題)で 90% 以上の正解率を達成し、DeepSeek-R1は 97.3%という驚異的なスコ アを記録しています。また、AIME(アメリカ数学オリンピック予選)では、Gemini 2.5 Proが Deep Thinkモードで 88.0%という人間の専門家に匹敵する成績を示しました。これらの成果は、単なるパ ラメータ数の増加やデータ量の拡大だけでは達成できず、推論プロセス自体を学習の中心に据えた 新しいアプローチによって実現されています。

 推論強化モデルが注目される背景には、LLMの応用範囲の拡大があります。対話や文章生成と いった従来のタスクだけでなく、科学研究の支援、複雑なソフトウェア開発、高度な分析業務など、より 知的な判断を要する領域への LLM 適用が進んでいます。このような応用においては、単に知識を 持っているだけでなく、その知識を適切に組み合わせて論理的に推論する能力が不可欠です。推論 強化モデルは、この需要に応えるために登場した次世代の LLMと言えるでしょう。

# Chain-of-Thought: 6.2 推論の基盤技術

 推論強化モデルの核となる技術が Chain-of-Thought(CoT)です。CoTは、直訳すると「思考 の連鎖」を意味し、問題解決のプロセスを一連のステップとして明示的に表現する手法です。この概 念は 2022年に Googleの研究者によって提案され、LLMの推論能力を大幅に向上させる画期的な 手法として注目されました。

### Chain-of-Thoughtの基本原理

 従来のプロンプティング手法では、モデルに問題を与えて直接答えを求めていました。例えば、「太 郎さんは 5個のリンゴを持っていて、3個を花子さんにあげました。太郎さんには何個のリンゴが残って いますか?」という問題に対して、モデルは直接「2個」と答えることが期待されます。

 これに対して Chain-of-Thoughtでは、答えに至るまでの推論過程を明示的に示すことを促しま す。同じ問題に対して、以下のような応答を生成させます。

**太郎さんは最初に 5個のリンゴを持っていました。 そこから3個を花子さんにあげたので、 5 - 3 = 2 したがって、太郎さんには 2個のリンゴが残っています。**

 このように、中間的な推論ステップを明示することで、モデルは複雑な問題をより小さなサブ問題に 分解し、段階的に解決できるようになります。特に、複数のステップを要する数学問題や論理パズルに おいて、CoTは劇的な性能向上をもたらしました。

#### Few-shot Chain-of-Thought

 Chain-of-Thoughtを実現する最も基本的な方法が Few-shot CoTです。この手法では、プロ ンプト内に推論過程を含む例(exemplar)をいくつか提示し、モデルに同様の形式で推論させます。 例えば、以下のようなプロンプトを構成します。

**問題: ある数の 2倍に 3を足すと11になります。その数は何ですか?**

**推論: ある数を xとします。**

**問題文より、2x + 3 = 11 という式が立てられます。**

**両辺から3を引くと、2x = 8 両辺を 2で割ると、x = 4**

**答え : 4**

**問題: 花子さんは太郎さんより3歳年上で、太郎さんは次郎さんの 2倍の年齢です。**

**次郎さんが 5歳なら、花子さんは何歳ですか?**

**推論:**

 このようなプロンプトを与えることで、モデルは最後の問題に対しても段階的な推論過程を生成する ようになります。Few-shot CoTの利点は、特別なモデルの再学習を必要とせず、プロンプトの工夫だ けで推論能力を引き出せる点です。

 研究によると、Few-shot CoTは特に大規模なモデル(パラメータ数が 1000億を超えるようなモデ ル)で顕著な効果を示します。小規模なモデルでは推論ステップを生成してもその内容が不正確にな りがちですが、大規模モデルでは各ステップの論理的整合性が保たれ、最終的な答えの精度が向上 します。

#### Zero-shot Chain-of-Thought

 Few-shot CoTの課題は、適切な例を作成する必要がある点です。特に新しいタイプの問題や専 門的な領域では、良質な例を用意することが困難な場合があります。この課題に対応するのが Zero-shot CoTです。

 Zero-shot CoTでは、例を与える代わりに、モデルに対して「段階的に考えてください」といった指 示を与えます。最も有名なのは "Let's think step by step"(段階的に考えましょう)というプロンプト で、これを問題の後に追加するだけで、モデルは自動的に推論過程を生成するようになります。

**問題: ある数の 2倍に 3を足すと11になります。その数は何ですか? Let's think step by step.**

このシンプルな指示だけで、モデルは以下のような推論を展開します。

第6章

**まず、求める数を xとおきます。 問題文から、2x + 3 = 11 という方程式が得られます。 この方程式を解くために、まず両辺から3を引きます。2x = 8 次に両辺を 2で割ります。x = 4 したがって、答えは 4です。**

 Zero-shot CoTの驚くべき点は、特定のタスクに関する例を一切与えていないにもかかわらず、モ デルが適切な推論構造を自発的に生成できることです。これは、大規模言語モデルが学習データの 中から「段階的推論」というパターン自体を抽出し、それを様々な問題に適用できることを示唆してい ます。

#### CoTの限界

 CoTは強力な手法ですが、いくつかの限界も存在します。第一に、生成される推論過程が必ずし も正しいとは限りません。モデルは流暢に推論ステップを記述できますが、その内容に論理的な誤りや 事実誤認が含まれることがあります。特に、知識が不足している領域では、もっともらしく見える誤った 推論を生成してしまう可能性があります。

 第二に、推論過程を明示的に生成することは、計算コストとレイテンシの増加を伴います。単に答え だけを生成する場合と比べて、推論ステップを含めると生成するトークン数が大幅に増加し、応答時 間が長くなります。実用的なアプリケーションでは、この時間コストと精度向上のトレードオフを考慮す る必要があります。

 第三に、すべての問題が CoTに適しているわけではありません。簡単な事実確認や定型的な質 問では、段階的推論を経ることがかえって冗長になり、誤りを導入するリスクが高まる場合もあります。

 これらの限界にもかかわらず、CoTは推論強化モデルの基盤技術として広く採用されています。 次節では、CoTをさらに発展させ、強化学習によって推論能力を向上させる手法について説明しま す。

## 6.3 強化学習による推論能力の学習

 プロンプトベースの CoTは、既存の LLMから推論能力を引き出す有効な手法ですが、モデル自 体の推論能力を根本的に強化するわけではありません。推論強化モデルは、強化学習を用いてモデ ルの推論プロセス自体を最適化することで、より高度な推論能力を獲得しています。ここでは、推論能 力の学習における強化学習の役割と、特に重要な概念である Process Reward Model(PRM) について説明します。

#### なぜ強化学習が必要なのか

 5.2節で説明したインストラクションチューニングでは、人間が作成した「指示と応答」のペアを教師 データとして学習します。しかし、複雑な推論を要する問題では、正解の応答だけを与えるアプローチ には限界があります。

 第一に、複雑な問題の正解に至る経路は一通りではありません。数学の証明問題を例に取ると、同 じ結論に到達するまでに複数の異なるアプローチが存在します。すべての可能な解法パターンを網 羅的に教師データとして準備することは現実的ではありません。

 第二に、推論においては「正しい答え」だけでなく「正しい推論プロセス」が重要です。たまたま正 解に到達したが推論過程に誤りがある応答と、論理的に正しいプロセスを経て正解に到達した応答 を、最終的な答えだけを見て区別はできません。インストラクションチューニングでは、この違いを学習 に反映させることが困難です。

 第三に、人間が明示的に書き下せる推論ステップには限界があります。専門家でも、自分の思考プ ロセスをすべて言語化できるわけではなく、直感や暗黙知に依存する部分があります。このような知 識を教師データとして提供することは極めて困難です。

 強化学習はこれらの課題に対処します。正解の経路を明示的に与える代わりに、モデルが生成し た推論プロセスを評価し、その評価に基づいてモデルを改善していきます。これにより、人間が明示的 に教えられない推論戦略もモデル自身が発見できる可能性が生まれます。

#### Outcome Reward Model: 結果による評価

 推論タスクにおける強化学習の最も単純なアプローチは、Outcome Reward Model(ORM)で す。これは 5.3節で説明した RLHFにおける報酬モデルと同様の考え方で、モデルが生成した最終

的な答えが正しいかどうかに基づいて報酬を与えます。

 数学問題を例にすると、モデルが問題を解いて答えを出力した際、その答えが正解と一致してい れば高い報酬(例えば +1)を、誤りであれば低い報酬(例えば 0または -1)を与えます。モデルはこの 報酬を最大化するように学習し、正解に到達する確率が高い推論戦略を獲得していきます。

 ORMの利点は、実装がシンプルで、正解が明確な問題(数学、コーディングなど)に適用しやすい 点です。最終的な答えの正誤を判定するだけでよいため、人間のアノテーションコストも比較的低く抑 えられます。

 しかし、ORMには重大な限界があります。最も問題なのは、推論プロセスの質を評価できないとい う点です。モデルが幸運にも正解に到達した場合と、論理的に正しい手順を踏んで正解に到達した 場合を区別できません。また、推論の途中でどこまで正しくて、どこから間違えたのかというフィードバッ クを提供できないため、学習効率が悪化します。

 さらに、ORMは「すべてか無か」の評価になりがちです。複雑な問題では、部分的に正しいアプ ローチを取っていても、最終的な答えに小さな計算ミスがあれば報酬は 0になってしまいます。これで は、有望な推論戦略を適切に評価し育てられません。

#### Process Reward Model: プロセスによる評価

 これらの課題を解決するのが Process Reward Model(PRM)です。PRMは、最終的な答え だけでなく、推論プロセスの各ステップを評価します。この考え方は、人間が難しい問題を教える際に 「答えだけでなく途中の考え方も評価する」というアプローチと一致しています。

PRMの動作原理を具体例で見てみましょう。以下のような数学問題を考えます。

**問題: x2 - 5x + 6 = 0 を解きなさい。**

モデルが以下のような推論プロセスを生成したとします。

**ステップ1: この方程式は二次方程式なので、因数分解を試みます。**

**ステップ2: x2 - 5x + 6 = (x - 2)(x - 3) と因数分解できます。**

**ステップ3: したがって、(x - 2)(x - 3) = 0**

**ステップ4: この方程式が成り立つのは、x - 2 = 0 または x - 3 = 0 のときです。**

**ステップ5: よって、x = 2 または x = 3**

 ORMでは最終的な答え「x = 2 または x = 3」が正しいかどうかだけを評価しますが、PRMは 各ステップを個別に評価します。

- ・ステップ1: 適切なアプローチの選択 → 高評価
- ・ステップ2: 因数分解の正しさ → 高評価
- ・ステップ3: 式の変形の正しさ → 高評価
- ・ステップ4: 論理的推論の正しさ → 高評価
- ・ステップ5: 最終的な答えの導出 → 高評価

 もし途中でモデルが誤りを犯した場合、PRMはその時点で低い報酬を与えます。例えば、ステップ 2で因数分解を間違えた場合。

**ステップ2: x2 - 5x + 6 = (x - 1)(x - 6) と因数分解できます。(誤り)**

 この時点で PRMは低い報酬を与え、以降のステップがいくら論理的に正しくても、誤った前提に 基づいている限り適切な評価は得られません。

このステップごとの評価により、モデルは以下のような重要なフィードバックを得られます。

- 1. どこで間違えたか : 推論プロセスのどの時点で誤りが発生したかを特定できます
- 2. 部分的な成功の評価: 最終的な答えが間違っていても、正しいステップは適切に評価さ れます
- 3. 推論戦略の改善: どのようなアプローチが有効かを、より詳細なレベルで学習できます

#### PRMの学習データ構築

 PRMを実装する上で最も困難なのは、推論プロセスの各ステップを評価するための学習データの 作成です。最終的な答えの正誤判定とは異なり、各推論ステップが論理的に正しいかを判断するに は、高度な専門知識が必要な場合があります。

- 一般的なアプローチとして、以下のような方法が用いられます。
  - 1. 人間アノテーターによる評価

 専門家が推論プロセスを読み、各ステップに対して「正しい/誤り/どちらとも言えない」 といったラベルを付与します。これは最も信頼性が高い方法ですが、コストが高く、大規模な データセット構築が困難です。

2. 自動検証が可能な領域の活用

数学やプログラミングなど、各ステップの正しさを形式的に検証できる領域では、自動的

に評価データを生成できます。例えば、数学の証明では各式変形が数学的に正当かを機械 的にチェックでき、プログラミングでは各行のコードが文法的に正しいか、期待される動作を するかをテストできます。

### 3. 結果から逆算するヒューリスティック

 最終的な答えが正しい推論プロセスの各ステップを「恐らく正しい」、答えが間違っている 場合は「どこかに誤りがある」と推定する方法です。精度は落ちますが、大量のデータを効率 的に作成できます。

#### 4. モデル自身による検証

 より強力な LLMや専門化されたモデルに各ステップの正しさを判定させる方法です。例 えば、GPT-4のような高性能モデルに「このステップは論理的に正しいか」を判断させるこ とができます。

実際の推論強化モデルの開発では、これらの方法を組み合わせて使用しています。

### PRMを用いた学習プロセス

PRMを用いた推論強化モデルの学習は、通常以下のような段階を経ます。

#### 段階1: 基盤モデルの準備

 まず、インストラクションチューニング済みのベースモデルを準備します。このモデルは基 本的な推論能力を持っていますが、複雑な問題では十分な性能を発揮できません。

#### 段階2: PRMの訓練

 大量の推論プロセスとそのステップごとの評価データを用いて、PRMを学習させます。 PRMは、推論の途中状態(問題文とこれまでのステップ)を入力として、次のステップが正 しい方向に進んでいるかを評価するスコアを出力します。

#### 段階3: 強化学習による最適化

 学 習され た P R M を報 酬 モ デ ルとして 使 用し、P P O( P r o x i m a l P o l i c y Optimization)などの強化学習アルゴリズムでベースモデルを改善します。モデルは問 題を解く際に、PRMから高い評価を得られるような推論ステップを生成するように学習され ます。

 重要なのは、この過程でモデルが単に「正解を暗記する」のではなく、「正しい推論プロセスを構築 する能力」を獲得する点です。同じ問題でも複数の解法があり、PRMはその論理的正しさを評価す るため、モデルは汎用的な推論戦略を学習します。

### Self-Consistency: 推論の信頼性向上

 PRMと組み合わせて用いられる重要な技術に Self-Consistencyがあります。これは、同じ問題 に対してモデルに複数回推論させ、最も頻繁に現れる答えを最終的な回答とする手法です。

 Self-Consistencyの背後にある考え方は、正しい推論プロセスは複数の異なるアプローチから同 じ結論に到達するが、誤った推論は偶然に依存するためバラバラな答えになりやすい、というもので す。

具体的なプロセスは以下の通りです。

- 1. 同じ問題に対して、温度パラメータを設定してモデルに複数回(例えば 10回)推論させ ます
- 2. 各推論プロセスから最終的な答えを抽出します
- 3. 最も頻繁に現れる答えを正解候補とします

 例えば、ある数学問題に対して 10回推論させた結果、7回が「x = 5」、2回が「x = 3」、1回が「x = 7」という答えだった場合、「x = 5」を最終回答とします。

 Self-Consistencyは特に、問題の難易度が高く、モデルが確実に正解できるわけではない場合に 有効です。計算コストは増加しますが(複数回の推論が必要)、精度は大幅に向上します。

 PRMとSelf-Consistencyを組み合わせることで、さらに洗練された評価が可能になります。複数 の推論過程それぞれを PRMで評価し、「最も論理的に正しい推論過程」と「最も頻繁に現れる答え」 の両方を考慮して最終的な回答を決定します。